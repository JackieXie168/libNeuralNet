<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
<title>OpenNN: gradient_descent.cpp Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css">
<link href="doxygen.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.5.9 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul>
      <li><a href="index.html"><span>Main&nbsp;Page</span></a></li>
      <li><a href="pages.html"><span>Related&nbsp;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div class="tabs">
    <ul>
      <li><a href="files.html"><span>File&nbsp;List</span></a></li>
    </ul>
  </div>
<h1>gradient_descent.cpp</h1><div class="fragment"><pre class="fragment"><a name="l00001"></a>00001 <span class="comment">/****************************************************************************************************************/</span>
<a name="l00002"></a>00002 <span class="comment">/*                                                                                                              */</span>
<a name="l00003"></a>00003 <span class="comment">/*   OpenNN: Open Neural Networks Library                                                                       */</span>
<a name="l00004"></a>00004 <span class="comment">/*   www.opennn.cimne.com                                                                                       */</span>
<a name="l00005"></a>00005 <span class="comment">/*                                                                                                              */</span>
<a name="l00006"></a>00006 <span class="comment">/*   G R A D I E N T   D E S C E N T   C L A S S                                                                */</span>
<a name="l00007"></a>00007 <span class="comment">/*                                                                                                              */</span>
<a name="l00008"></a>00008 <span class="comment">/*   Roberto Lopez                                                                                              */</span>
<a name="l00009"></a>00009 <span class="comment">/*   International Center for Numerical Methods in Engineering (CIMNE)                                          */</span>
<a name="l00010"></a>00010 <span class="comment">/*   Technical University of Catalonia (UPC)                                                                    */</span>
<a name="l00011"></a>00011 <span class="comment">/*   Barcelona, Spain                                                                                           */</span>
<a name="l00012"></a>00012 <span class="comment">/*   E-mail: rlopez@cimne.upc.edu                                                                               */</span>
<a name="l00013"></a>00013 <span class="comment">/*                                                                                                              */</span>
<a name="l00014"></a>00014 <span class="comment">/****************************************************************************************************************/</span>
<a name="l00015"></a>00015 
<a name="l00016"></a>00016 <span class="comment">// System includes</span>
<a name="l00017"></a>00017 
<a name="l00018"></a>00018 <span class="preprocessor">#include &lt;string&gt;</span>
<a name="l00019"></a>00019 <span class="preprocessor">#include &lt;sstream&gt;</span>
<a name="l00020"></a>00020 <span class="preprocessor">#include &lt;iostream&gt;</span>
<a name="l00021"></a>00021 <span class="preprocessor">#include &lt;fstream&gt;</span>
<a name="l00022"></a>00022 <span class="preprocessor">#include &lt;algorithm&gt;</span>
<a name="l00023"></a>00023 <span class="preprocessor">#include &lt;functional&gt;</span>
<a name="l00024"></a>00024 <span class="preprocessor">#include &lt;limits&gt;</span>
<a name="l00025"></a>00025 <span class="preprocessor">#include &lt;cmath&gt;</span>
<a name="l00026"></a>00026 <span class="preprocessor">#include &lt;ctime&gt;</span>
<a name="l00027"></a>00027 
<a name="l00028"></a>00028 <span class="comment">// Open NN includes</span>
<a name="l00029"></a>00029 
<a name="l00030"></a>00030 <span class="preprocessor">#include "gradient_descent.h"</span>
<a name="l00031"></a>00031 
<a name="l00032"></a>00032 <span class="keyword">namespace </span>OpenNN
<a name="l00033"></a>00033 {
<a name="l00034"></a>00034 
<a name="l00035"></a>00035 <span class="comment">// DEFAULT CONSTRUCTOR</span>
<a name="l00036"></a>00036 
<a name="l00040"></a>00040 
<a name="l00041"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#d4c53dcaa3371726975a35df1978629c">00041</a> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#d4c53dcaa3371726975a35df1978629c">GradientDescent::GradientDescent</a>(<span class="keywordtype">void</span>) 
<a name="l00042"></a>00042  : <a class="code" href="class_open_n_n_1_1_training_algorithm.html">TrainingAlgorithm</a>()
<a name="l00043"></a>00043 {
<a name="l00044"></a>00044    <a class="code" href="class_open_n_n_1_1_gradient_descent.html#de5a4d5ec6b048ca182ba2e17e182055" title="This method sets the members of the training algorithm object to their default values...">set_default</a>();
<a name="l00045"></a>00045 }
<a name="l00046"></a>00046 
<a name="l00047"></a>00047 
<a name="l00048"></a>00048 <span class="comment">// PERFORMANCE FUNCTIONAL CONSTRUCTOR </span>
<a name="l00049"></a>00049 
<a name="l00054"></a>00054 
<a name="l00055"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#dc19d4061896e1bef6c91afb5addd17d">00055</a> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#d4c53dcaa3371726975a35df1978629c">GradientDescent::GradientDescent</a>(<a class="code" href="class_open_n_n_1_1_performance_functional.html">PerformanceFunctional</a>* new_performance_functional_pointer)
<a name="l00056"></a>00056 : <a class="code" href="class_open_n_n_1_1_training_algorithm.html">TrainingAlgorithm</a>(new_performance_functional_pointer)
<a name="l00057"></a>00057 {
<a name="l00058"></a>00058    training_rate_algorithm.<a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html#b39e6f8f5624543629db1b2566c0ba94">set_performance_functional_pointer</a>(new_performance_functional_pointer);
<a name="l00059"></a>00059 
<a name="l00060"></a>00060    <a class="code" href="class_open_n_n_1_1_gradient_descent.html#de5a4d5ec6b048ca182ba2e17e182055" title="This method sets the members of the training algorithm object to their default values...">set_default</a>();
<a name="l00061"></a>00061 }
<a name="l00062"></a>00062 
<a name="l00063"></a>00063 
<a name="l00064"></a>00064 <span class="comment">// XML CONSTRUCTOR</span>
<a name="l00065"></a>00065 
<a name="l00070"></a>00070 
<a name="l00071"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#40f5385a356150abef61dfbe282a5e74">00071</a> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#d4c53dcaa3371726975a35df1978629c">GradientDescent::GradientDescent</a>(TiXmlElement* gradient_descent_element) : <a class="code" href="class_open_n_n_1_1_training_algorithm.html">TrainingAlgorithm</a>(gradient_descent_element)
<a name="l00072"></a>00072 {
<a name="l00073"></a>00073    <a class="code" href="class_open_n_n_1_1_gradient_descent.html#de5a4d5ec6b048ca182ba2e17e182055" title="This method sets the members of the training algorithm object to their default values...">set_default</a>();
<a name="l00074"></a>00074 
<a name="l00075"></a>00075    <a class="code" href="class_open_n_n_1_1_gradient_descent.html#ee5b6a70c80e41df05f583193ec52045">from_XML</a>(gradient_descent_element);
<a name="l00076"></a>00076 }
<a name="l00077"></a>00077 
<a name="l00078"></a>00078 
<a name="l00079"></a>00079 <span class="comment">// DESTRUCTOR</span>
<a name="l00080"></a>00080 
<a name="l00082"></a>00082 
<a name="l00083"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#681a0ffccb2b004a1406dfe500addd8a">00083</a> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#681a0ffccb2b004a1406dfe500addd8a" title="Destructor.">GradientDescent::~GradientDescent</a>(<span class="keywordtype">void</span>)
<a name="l00084"></a>00084 {
<a name="l00085"></a>00085 }
<a name="l00086"></a>00086 
<a name="l00087"></a>00087 
<a name="l00088"></a>00088 <span class="comment">// METHODS</span>
<a name="l00089"></a>00089 
<a name="l00090"></a>00090 <span class="comment">// const TrainingRateAlgorithm&amp; get_training_rate_algorithm(void) const method</span>
<a name="l00091"></a>00091 
<a name="l00093"></a>00093 
<a name="l00094"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#f56fbbae486b692bb84d4e0da4b0f0f8">00094</a> <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html">TrainingRateAlgorithm</a>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#f56fbbae486b692bb84d4e0da4b0f0f8" title="This method returns a constant reference to the training rate algorithm object inside...">GradientDescent::get_training_rate_algorithm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00095"></a>00095 <span class="keyword"></span>{
<a name="l00096"></a>00096    <span class="keywordflow">return</span>(training_rate_algorithm);
<a name="l00097"></a>00097 }
<a name="l00098"></a>00098 
<a name="l00099"></a>00099 
<a name="l00100"></a>00100 <span class="comment">// TrainingRateAlgorithm* get_training_rate_algorithm_pointer(void) method</span>
<a name="l00101"></a>00101 
<a name="l00103"></a>00103 
<a name="l00104"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#dc28271b363aad570eaa0a3ab33a4db0">00104</a> <a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html">TrainingRateAlgorithm</a>* <a class="code" href="class_open_n_n_1_1_gradient_descent.html#dc28271b363aad570eaa0a3ab33a4db0" title="This method returns a pointer to the training rate algorithm object inside the gradient...">GradientDescent::get_training_rate_algorithm_pointer</a>(<span class="keywordtype">void</span>)
<a name="l00105"></a>00105 {
<a name="l00106"></a>00106    <span class="keywordflow">return</span>(&amp;training_rate_algorithm);
<a name="l00107"></a>00107 }
<a name="l00108"></a>00108 
<a name="l00109"></a>00109 
<a name="l00110"></a>00110 <span class="comment">// const double&amp; get_warning_parameters_norm(void) const method</span>
<a name="l00111"></a>00111 
<a name="l00114"></a>00114 
<a name="l00115"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#8c79c02292682be10175dd88e49df847">00115</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#8c79c02292682be10175dd88e49df847">GradientDescent::get_warning_parameters_norm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00116"></a>00116 <span class="keyword"></span>{
<a name="l00117"></a>00117    <span class="keywordflow">return</span>(warning_parameters_norm);       
<a name="l00118"></a>00118 }
<a name="l00119"></a>00119 
<a name="l00120"></a>00120 
<a name="l00121"></a>00121 <span class="comment">// const double&amp; get_warning_gradient_norm(void) const method</span>
<a name="l00122"></a>00122 
<a name="l00125"></a>00125 
<a name="l00126"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#d670d10c866f2db246824998bc58b103">00126</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#d670d10c866f2db246824998bc58b103">GradientDescent::get_warning_gradient_norm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00127"></a>00127 <span class="keyword"></span>{
<a name="l00128"></a>00128    <span class="keywordflow">return</span>(warning_gradient_norm);       
<a name="l00129"></a>00129 }
<a name="l00130"></a>00130 
<a name="l00131"></a>00131 
<a name="l00132"></a>00132 <span class="comment">// const double&amp; get_warning_training_rate(void) const method</span>
<a name="l00133"></a>00133 
<a name="l00136"></a>00136 
<a name="l00137"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#cff823bf71a1fbc7df17939ddab9912b">00137</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#cff823bf71a1fbc7df17939ddab9912b">GradientDescent::get_warning_training_rate</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00138"></a>00138 <span class="keyword"></span>{
<a name="l00139"></a>00139    <span class="keywordflow">return</span>(warning_training_rate);
<a name="l00140"></a>00140 }
<a name="l00141"></a>00141 
<a name="l00142"></a>00142 
<a name="l00143"></a>00143 <span class="comment">// const double&amp; get_error_parameters_norm(void) const method</span>
<a name="l00144"></a>00144 
<a name="l00147"></a>00147 
<a name="l00148"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#abf083d521b38d878afc58b7cf744ad9">00148</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#abf083d521b38d878afc58b7cf744ad9">GradientDescent::get_error_parameters_norm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00149"></a>00149 <span class="keyword"></span>{
<a name="l00150"></a>00150    <span class="keywordflow">return</span>(error_parameters_norm);
<a name="l00151"></a>00151 }
<a name="l00152"></a>00152 
<a name="l00153"></a>00153 
<a name="l00154"></a>00154 <span class="comment">// const double&amp; get_error_gradient_norm(void) const method</span>
<a name="l00155"></a>00155 
<a name="l00158"></a>00158 
<a name="l00159"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#5d28f85181e21d4c074bb1871206fb5c">00159</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#5d28f85181e21d4c074bb1871206fb5c">GradientDescent::get_error_gradient_norm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00160"></a>00160 <span class="keyword"></span>{
<a name="l00161"></a>00161    <span class="keywordflow">return</span>(error_gradient_norm);
<a name="l00162"></a>00162 }
<a name="l00163"></a>00163 
<a name="l00164"></a>00164 
<a name="l00165"></a>00165 <span class="comment">// const double&amp; get_error_training_rate(void) const method</span>
<a name="l00166"></a>00166 
<a name="l00169"></a>00169 
<a name="l00170"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#43c43e8fa25ece6586133dbbaf5403c4">00170</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#43c43e8fa25ece6586133dbbaf5403c4">GradientDescent::get_error_training_rate</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00171"></a>00171 <span class="keyword"></span>{
<a name="l00172"></a>00172    <span class="keywordflow">return</span>(error_training_rate);
<a name="l00173"></a>00173 }
<a name="l00174"></a>00174 
<a name="l00175"></a>00175 
<a name="l00176"></a>00176 <span class="comment">// const double&amp; get_minimum_parameters_increment_norm(void) const method</span>
<a name="l00177"></a>00177 
<a name="l00179"></a>00179 
<a name="l00180"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#977e8321cd4b1027a2d47bd10bcf47ca">00180</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#977e8321cd4b1027a2d47bd10bcf47ca" title="This method returns the minimum norm of the parameter increment vector used as a...">GradientDescent::get_minimum_parameters_increment_norm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00181"></a>00181 <span class="keyword"></span>{
<a name="l00182"></a>00182    <span class="keywordflow">return</span>(minimum_parameters_increment_norm);
<a name="l00183"></a>00183 }
<a name="l00184"></a>00184 
<a name="l00185"></a>00185 
<a name="l00186"></a>00186 <span class="comment">// const double&amp; get_minimum_performance_increase(void) const method</span>
<a name="l00187"></a>00187 
<a name="l00189"></a>00189 
<a name="l00190"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#460c313eb05a471addf301f8691827fb">00190</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#460c313eb05a471addf301f8691827fb" title="This method returns the minimum performance improvement during training.">GradientDescent::get_minimum_performance_increase</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00191"></a>00191 <span class="keyword"></span>{
<a name="l00192"></a>00192    <span class="keywordflow">return</span>(minimum_performance_increase);
<a name="l00193"></a>00193 }
<a name="l00194"></a>00194 
<a name="l00195"></a>00195 
<a name="l00196"></a>00196 <span class="comment">// const double&amp; get_performance_goal(void) const method</span>
<a name="l00197"></a>00197 
<a name="l00200"></a>00200 
<a name="l00201"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#b2a6b927e38a1dfe173bb866f27d2b96">00201</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#b2a6b927e38a1dfe173bb866f27d2b96">GradientDescent::get_performance_goal</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00202"></a>00202 <span class="keyword"></span>{
<a name="l00203"></a>00203    <span class="keywordflow">return</span>(performance_goal);
<a name="l00204"></a>00204 }
<a name="l00205"></a>00205 
<a name="l00206"></a>00206 
<a name="l00207"></a>00207 <span class="comment">// const double&amp; get_gradient_norm_goal(void) const method</span>
<a name="l00208"></a>00208 
<a name="l00211"></a>00211 
<a name="l00212"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#096b7ba798dd03e82670ea5263e194b5">00212</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#096b7ba798dd03e82670ea5263e194b5">GradientDescent::get_gradient_norm_goal</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00213"></a>00213 <span class="keyword"></span>{
<a name="l00214"></a>00214    <span class="keywordflow">return</span>(gradient_norm_goal);
<a name="l00215"></a>00215 }
<a name="l00216"></a>00216 
<a name="l00217"></a>00217 
<a name="l00218"></a>00218 <span class="comment">// const unsigned int&amp; get_maximum_generalization_evaluation_decreases(void) const method</span>
<a name="l00219"></a>00219 
<a name="l00221"></a>00221 
<a name="l00222"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#ef2c9be7d3ae74c843117414c4588be7">00222</a> <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#ef2c9be7d3ae74c843117414c4588be7" title="This method returns the maximum number of generalization failures during the training...">GradientDescent::get_maximum_generalization_evaluation_decreases</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00223"></a>00223 <span class="keyword"></span>{
<a name="l00224"></a>00224    <span class="keywordflow">return</span>(maximum_generalization_evaluation_decreases);
<a name="l00225"></a>00225 }
<a name="l00226"></a>00226 
<a name="l00227"></a>00227 
<a name="l00228"></a>00228 <span class="comment">// const unsigned int&amp; get_maximum_epochs_number(void) const method</span>
<a name="l00229"></a>00229 
<a name="l00231"></a>00231 
<a name="l00232"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#7d2670b88ec8ba50df21b9a0763b5d04">00232</a> <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#7d2670b88ec8ba50df21b9a0763b5d04" title="This method returns the maximum number of epochs for training.">GradientDescent::get_maximum_epochs_number</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00233"></a>00233 <span class="keyword"></span>{
<a name="l00234"></a>00234    <span class="keywordflow">return</span>(maximum_epochs_number);
<a name="l00235"></a>00235 }
<a name="l00236"></a>00236 
<a name="l00237"></a>00237 
<a name="l00238"></a>00238 <span class="comment">// const double&amp; get_maximum_time(void) const method</span>
<a name="l00239"></a>00239 
<a name="l00241"></a>00241 
<a name="l00242"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#e8fe1dd09d144f2ece993cf9338f5830">00242</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#e8fe1dd09d144f2ece993cf9338f5830" title="This method returns the maximum training time.">GradientDescent::get_maximum_time</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00243"></a>00243 <span class="keyword"></span>{
<a name="l00244"></a>00244    <span class="keywordflow">return</span>(maximum_time);
<a name="l00245"></a>00245 }
<a name="l00246"></a>00246 
<a name="l00247"></a>00247 
<a name="l00248"></a>00248 <span class="comment">// const bool&amp; get_reserve_parameters_history(void) const method</span>
<a name="l00249"></a>00249 
<a name="l00251"></a>00251 
<a name="l00252"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#0adf17968a23452099c3a3af7788de07">00252</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#0adf17968a23452099c3a3af7788de07" title="This method returns true if the parameters history matrix is to be reserved, and...">GradientDescent::get_reserve_parameters_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00253"></a>00253 <span class="keyword"></span>{
<a name="l00254"></a>00254    <span class="keywordflow">return</span>(reserve_parameters_history);     
<a name="l00255"></a>00255 }
<a name="l00256"></a>00256 
<a name="l00257"></a>00257 
<a name="l00258"></a>00258 <span class="comment">// const bool&amp; get_reserve_parameters_norm_history(void) const method </span>
<a name="l00259"></a>00259 
<a name="l00261"></a>00261 
<a name="l00262"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#a34afdc9f24b77546a0417c2136fa005">00262</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#a34afdc9f24b77546a0417c2136fa005" title="This method returns true if the parameters norm history vector is to be reserved...">GradientDescent::get_reserve_parameters_norm_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00263"></a>00263 <span class="keyword"></span>{
<a name="l00264"></a>00264    <span class="keywordflow">return</span>(reserve_parameters_norm_history);     
<a name="l00265"></a>00265 }
<a name="l00266"></a>00266 
<a name="l00267"></a>00267 
<a name="l00268"></a>00268 <span class="comment">// const bool&amp; get_reserve_evaluation_history(void) const method</span>
<a name="l00269"></a>00269 
<a name="l00271"></a>00271 
<a name="l00272"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#4a117b0b0bb4eb10d78d397aaa2dc2ff">00272</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#4a117b0b0bb4eb10d78d397aaa2dc2ff" title="This method returns true if the evaluation history vector is to be reserved, and...">GradientDescent::get_reserve_evaluation_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00273"></a>00273 <span class="keyword"></span>{
<a name="l00274"></a>00274    <span class="keywordflow">return</span>(reserve_evaluation_history);     
<a name="l00275"></a>00275 }
<a name="l00276"></a>00276 
<a name="l00277"></a>00277 
<a name="l00278"></a>00278 <span class="comment">// const bool&amp; get_reserve_gradient_history(void) const method</span>
<a name="l00279"></a>00279 
<a name="l00281"></a>00281 
<a name="l00282"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#6c78744fa192aee34b9bbe2fd7419ed1">00282</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#6c78744fa192aee34b9bbe2fd7419ed1" title="This method returns true if the gradient history vector of vectors is to be reserved...">GradientDescent::get_reserve_gradient_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00283"></a>00283 <span class="keyword"></span>{
<a name="l00284"></a>00284    <span class="keywordflow">return</span>(reserve_gradient_history);     
<a name="l00285"></a>00285 }
<a name="l00286"></a>00286 
<a name="l00287"></a>00287 
<a name="l00288"></a>00288 <span class="comment">// const bool&amp; get_reserve_gradient_norm_history(void) const method</span>
<a name="l00289"></a>00289 
<a name="l00291"></a>00291 
<a name="l00292"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#b0eef1b50c1003bf779b3f7658e0ac8f">00292</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#b0eef1b50c1003bf779b3f7658e0ac8f" title="This method returns true if the gradient norm history vector is to be reserved, and...">GradientDescent::get_reserve_gradient_norm_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00293"></a>00293 <span class="keyword"></span>{
<a name="l00294"></a>00294    <span class="keywordflow">return</span>(reserve_gradient_norm_history);     
<a name="l00295"></a>00295 }
<a name="l00296"></a>00296 
<a name="l00297"></a>00297 
<a name="l00298"></a>00298 <span class="comment">// const bool&amp; get_reserve_training_direction_history(void) const method</span>
<a name="l00299"></a>00299 
<a name="l00301"></a>00301 
<a name="l00302"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#003543fe0d6f7a5d1bfae52837b19d8a">00302</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#003543fe0d6f7a5d1bfae52837b19d8a" title="This method returns true if the training direction history matrix is to be reserved...">GradientDescent::get_reserve_training_direction_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00303"></a>00303 <span class="keyword"></span>{
<a name="l00304"></a>00304    <span class="keywordflow">return</span>(reserve_training_direction_history);     
<a name="l00305"></a>00305 }
<a name="l00306"></a>00306 
<a name="l00307"></a>00307 
<a name="l00308"></a>00308 <span class="comment">// const bool&amp; get_reserve_training_rate_history(void) const method</span>
<a name="l00309"></a>00309 
<a name="l00311"></a>00311 
<a name="l00312"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#038114122cf181642076b7e76e494abc">00312</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#038114122cf181642076b7e76e494abc" title="This method returns true if the training rate history vector is to be reserved, and...">GradientDescent::get_reserve_training_rate_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00313"></a>00313 <span class="keyword"></span>{
<a name="l00314"></a>00314    <span class="keywordflow">return</span>(reserve_training_rate_history);     
<a name="l00315"></a>00315 }
<a name="l00316"></a>00316 
<a name="l00317"></a>00317 
<a name="l00318"></a>00318 <span class="comment">// const bool&amp; get_reserve_elapsed_time_history(void) const method</span>
<a name="l00319"></a>00319 
<a name="l00321"></a>00321 
<a name="l00322"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#b1721d85c7aab1ee42ee55ec24e296c5">00322</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#b1721d85c7aab1ee42ee55ec24e296c5" title="This method returns true if the elapsed time history vector is to be reserved, and...">GradientDescent::get_reserve_elapsed_time_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00323"></a>00323 <span class="keyword"></span>{
<a name="l00324"></a>00324    <span class="keywordflow">return</span>(reserve_elapsed_time_history);     
<a name="l00325"></a>00325 }
<a name="l00326"></a>00326 
<a name="l00327"></a>00327 
<a name="l00328"></a>00328 <span class="comment">// const bool&amp; get_reserve_generalization_evaluation_history(void) const method</span>
<a name="l00329"></a>00329 
<a name="l00331"></a>00331 
<a name="l00332"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#e8eda1d13c5f4fd1aca52e9971336730">00332</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#e8eda1d13c5f4fd1aca52e9971336730" title="This method returns true if the Generalization evaluation history vector is to be...">GradientDescent::get_reserve_generalization_evaluation_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00333"></a>00333 <span class="keyword"></span>{
<a name="l00334"></a>00334    <span class="keywordflow">return</span>(reserve_generalization_evaluation_history);
<a name="l00335"></a>00335 }
<a name="l00336"></a>00336 
<a name="l00337"></a>00337 
<a name="l00338"></a>00338 <span class="comment">// const unsigned int&amp; get_display_period(void) const method</span>
<a name="l00339"></a>00339 
<a name="l00341"></a>00341 
<a name="l00342"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#0598d1a344e0b4a0589998c3402ff22a">00342</a> <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; <a class="code" href="class_open_n_n_1_1_gradient_descent.html#0598d1a344e0b4a0589998c3402ff22a" title="This method returns the number of epochs between the training showing progress.">GradientDescent::get_display_period</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00343"></a>00343 <span class="keyword"></span>{
<a name="l00344"></a>00344    <span class="keywordflow">return</span>(display_period);
<a name="l00345"></a>00345 }
<a name="l00346"></a>00346 
<a name="l00347"></a>00347 
<a name="l00348"></a>00348 <span class="comment">// void set_training_rate_algorithm(const TrainingRateAlgorithm&amp;) method</span>
<a name="l00349"></a>00349 
<a name="l00352"></a>00352 
<a name="l00353"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#3e29b2699a9b3b4eee80d17e493a3c6d">00353</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#3e29b2699a9b3b4eee80d17e493a3c6d">GradientDescent::set_training_rate_algorithm</a>(<span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html">TrainingRateAlgorithm</a>&amp; new_training_rate_algorithm)
<a name="l00354"></a>00354 {
<a name="l00355"></a>00355    training_rate_algorithm = new_training_rate_algorithm;
<a name="l00356"></a>00356 }
<a name="l00357"></a>00357 
<a name="l00358"></a>00358 
<a name="l00359"></a>00359 <span class="comment">// void set_default(void) method</span>
<a name="l00360"></a>00360 
<a name="l00361"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#de5a4d5ec6b048ca182ba2e17e182055">00361</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#de5a4d5ec6b048ca182ba2e17e182055" title="This method sets the members of the training algorithm object to their default values...">GradientDescent::set_default</a>(<span class="keywordtype">void</span>)
<a name="l00362"></a>00362 {
<a name="l00363"></a>00363    <span class="comment">// TRAINING PARAMETERS</span>
<a name="l00364"></a>00364 
<a name="l00365"></a>00365    warning_parameters_norm = 1.0e6;
<a name="l00366"></a>00366    warning_gradient_norm = 1.0e6;   
<a name="l00367"></a>00367    warning_training_rate = 1.0e6;
<a name="l00368"></a>00368 
<a name="l00369"></a>00369    error_parameters_norm = 1.0e9;
<a name="l00370"></a>00370    error_gradient_norm = 1.0e9;
<a name="l00371"></a>00371    error_training_rate = 1.0e9;
<a name="l00372"></a>00372 
<a name="l00373"></a>00373    <span class="comment">// STOPPING CRITERIA</span>
<a name="l00374"></a>00374 
<a name="l00375"></a>00375    minimum_parameters_increment_norm = 0.0;
<a name="l00376"></a>00376 
<a name="l00377"></a>00377    minimum_performance_increase = 0.0;
<a name="l00378"></a>00378    performance_goal = -1.0e99;
<a name="l00379"></a>00379    gradient_norm_goal = 0.0;
<a name="l00380"></a>00380    maximum_generalization_evaluation_decreases = 1000000;
<a name="l00381"></a>00381 
<a name="l00382"></a>00382    maximum_epochs_number = 1000;
<a name="l00383"></a>00383    maximum_time = 1000.0;
<a name="l00384"></a>00384 
<a name="l00385"></a>00385    <span class="comment">// TRAINING HISTORY</span>
<a name="l00386"></a>00386 
<a name="l00387"></a>00387    reserve_parameters_history = <span class="keyword">false</span>;
<a name="l00388"></a>00388    reserve_parameters_norm_history = <span class="keyword">false</span>;
<a name="l00389"></a>00389 
<a name="l00390"></a>00390    reserve_evaluation_history = <span class="keyword">true</span>;
<a name="l00391"></a>00391    reserve_gradient_history = <span class="keyword">false</span>;
<a name="l00392"></a>00392    reserve_gradient_norm_history = <span class="keyword">false</span>;
<a name="l00393"></a>00393    reserve_generalization_evaluation_history = <span class="keyword">false</span>;
<a name="l00394"></a>00394 
<a name="l00395"></a>00395    reserve_training_direction_history = <span class="keyword">false</span>;
<a name="l00396"></a>00396    reserve_training_rate_history = <span class="keyword">false</span>;
<a name="l00397"></a>00397    reserve_elapsed_time_history = <span class="keyword">false</span>;
<a name="l00398"></a>00398 
<a name="l00399"></a>00399    <span class="comment">// UTILITIES</span>
<a name="l00400"></a>00400 
<a name="l00401"></a>00401    <a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a> = <span class="keyword">true</span>;
<a name="l00402"></a>00402    display_period = 100;
<a name="l00403"></a>00403 }
<a name="l00404"></a>00404 
<a name="l00405"></a>00405 
<a name="l00406"></a>00406 <span class="comment">// void set_reserve_all_training_history(bool) method</span>
<a name="l00407"></a>00407 
<a name="l00421"></a>00421 
<a name="l00422"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#8b7860afbf36d0aac837091f35a63e84">00422</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#8b7860afbf36d0aac837091f35a63e84">GradientDescent::set_reserve_all_training_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_all_training_history)
<a name="l00423"></a>00423 {
<a name="l00424"></a>00424    <span class="comment">// Multilayer perceptron</span>
<a name="l00425"></a>00425 
<a name="l00426"></a>00426    reserve_parameters_history = new_reserve_all_training_history;
<a name="l00427"></a>00427    reserve_parameters_norm_history = new_reserve_all_training_history;
<a name="l00428"></a>00428    
<a name="l00429"></a>00429    <span class="comment">// Performance functional</span>
<a name="l00430"></a>00430 
<a name="l00431"></a>00431    reserve_evaluation_history = new_reserve_all_training_history;
<a name="l00432"></a>00432    reserve_gradient_history = new_reserve_all_training_history;
<a name="l00433"></a>00433    reserve_gradient_norm_history = new_reserve_all_training_history;
<a name="l00434"></a>00434 
<a name="l00435"></a>00435    reserve_generalization_evaluation_history = new_reserve_all_training_history;
<a name="l00436"></a>00436 
<a name="l00437"></a>00437    <span class="comment">// Training algorithm</span>
<a name="l00438"></a>00438 
<a name="l00439"></a>00439    reserve_training_direction_history = new_reserve_all_training_history;
<a name="l00440"></a>00440    reserve_training_rate_history = new_reserve_all_training_history;
<a name="l00441"></a>00441 
<a name="l00442"></a>00442    reserve_elapsed_time_history = new_reserve_all_training_history;
<a name="l00443"></a>00443 }
<a name="l00444"></a>00444 
<a name="l00445"></a>00445 
<a name="l00446"></a>00446 <span class="comment">// void set_warning_parameters_norm(const double&amp;) method</span>
<a name="l00447"></a>00447 
<a name="l00451"></a>00451 
<a name="l00452"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#c525a49652ff909ec76613b721e33739">00452</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#c525a49652ff909ec76613b721e33739">GradientDescent::set_warning_parameters_norm</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_warning_parameters_norm)
<a name="l00453"></a>00453 {
<a name="l00454"></a>00454    <span class="comment">// Control sentence (if debug)</span>
<a name="l00455"></a>00455 
<a name="l00456"></a>00456 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00457"></a>00457 <span class="preprocessor"></span>
<a name="l00458"></a>00458    <span class="keywordflow">if</span>(new_warning_parameters_norm &lt; 0.0)
<a name="l00459"></a>00459    {
<a name="l00460"></a>00460       std::ostringstream buffer;
<a name="l00461"></a>00461 
<a name="l00462"></a>00462       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00463"></a>00463              &lt;&lt; <span class="stringliteral">"void set_warning_parameters_norm(const double&amp;) method.\n"</span>
<a name="l00464"></a>00464              &lt;&lt; <span class="stringliteral">"Warning parameters norm must be equal or greater than 0.\n"</span>;
<a name="l00465"></a>00465 
<a name="l00466"></a>00466       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00467"></a>00467    }
<a name="l00468"></a>00468 
<a name="l00469"></a>00469 <span class="preprocessor">   #endif</span>
<a name="l00470"></a>00470 <span class="preprocessor"></span>
<a name="l00471"></a>00471    <span class="comment">// Set warning parameters norm</span>
<a name="l00472"></a>00472 
<a name="l00473"></a>00473    warning_parameters_norm = new_warning_parameters_norm;     
<a name="l00474"></a>00474 }
<a name="l00475"></a>00475 
<a name="l00476"></a>00476 
<a name="l00477"></a>00477 <span class="comment">// void set_warning_gradient_norm(const double&amp;) method</span>
<a name="l00478"></a>00478 
<a name="l00482"></a>00482 
<a name="l00483"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#c7f1c7a84ab5067aa2550b3e36537c21">00483</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#c7f1c7a84ab5067aa2550b3e36537c21">GradientDescent::set_warning_gradient_norm</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_warning_gradient_norm)
<a name="l00484"></a>00484 {
<a name="l00485"></a>00485    <span class="comment">// Control sentence (if debug)</span>
<a name="l00486"></a>00486 
<a name="l00487"></a>00487 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00488"></a>00488 <span class="preprocessor"></span>
<a name="l00489"></a>00489    <span class="keywordflow">if</span>(new_warning_gradient_norm &lt; 0.0)
<a name="l00490"></a>00490    {
<a name="l00491"></a>00491       std::ostringstream buffer;
<a name="l00492"></a>00492 
<a name="l00493"></a>00493       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00494"></a>00494              &lt;&lt; <span class="stringliteral">"void set_warning_gradient_norm(const double&amp;) method.\n"</span>
<a name="l00495"></a>00495              &lt;&lt; <span class="stringliteral">"Warning gradient norm must be equal or greater than 0.\n"</span>;
<a name="l00496"></a>00496 
<a name="l00497"></a>00497       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00498"></a>00498    }
<a name="l00499"></a>00499 
<a name="l00500"></a>00500 <span class="preprocessor">   #endif</span>
<a name="l00501"></a>00501 <span class="preprocessor"></span>
<a name="l00502"></a>00502    <span class="comment">// Set warning gradient norm</span>
<a name="l00503"></a>00503 
<a name="l00504"></a>00504    warning_gradient_norm = new_warning_gradient_norm;     
<a name="l00505"></a>00505 }
<a name="l00506"></a>00506 
<a name="l00507"></a>00507 
<a name="l00508"></a>00508 <span class="comment">// void set_warning_training_rate(const double&amp;) method</span>
<a name="l00509"></a>00509 
<a name="l00513"></a>00513 
<a name="l00514"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#9359ff2faa9eab75167a5867381a6b34">00514</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#9359ff2faa9eab75167a5867381a6b34">GradientDescent::set_warning_training_rate</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_warning_training_rate)
<a name="l00515"></a>00515 {
<a name="l00516"></a>00516    <span class="comment">// Control sentence (if debug)</span>
<a name="l00517"></a>00517 
<a name="l00518"></a>00518 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00519"></a>00519 <span class="preprocessor"></span>
<a name="l00520"></a>00520    <span class="keywordflow">if</span>(new_warning_training_rate &lt; 0.0)
<a name="l00521"></a>00521    {
<a name="l00522"></a>00522       std::ostringstream buffer;
<a name="l00523"></a>00523 
<a name="l00524"></a>00524       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span> 
<a name="l00525"></a>00525              &lt;&lt; <span class="stringliteral">"void set_warning_training_rate(const double&amp;) method.\n"</span>
<a name="l00526"></a>00526              &lt;&lt; <span class="stringliteral">"Warning training rate must be equal or greater than 0.\n"</span>;
<a name="l00527"></a>00527 
<a name="l00528"></a>00528       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00529"></a>00529    }
<a name="l00530"></a>00530 
<a name="l00531"></a>00531 <span class="preprocessor">   #endif</span>
<a name="l00532"></a>00532 <span class="preprocessor"></span>
<a name="l00533"></a>00533    warning_training_rate = new_warning_training_rate;
<a name="l00534"></a>00534 }
<a name="l00535"></a>00535 
<a name="l00536"></a>00536 
<a name="l00537"></a>00537 <span class="comment">// void set_error_parameters_norm(const double&amp;) method</span>
<a name="l00538"></a>00538 
<a name="l00542"></a>00542 
<a name="l00543"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#2625e879f726b1501ce33ab5710bbb88">00543</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#2625e879f726b1501ce33ab5710bbb88">GradientDescent::set_error_parameters_norm</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_error_parameters_norm)
<a name="l00544"></a>00544 {
<a name="l00545"></a>00545    <span class="comment">// Control sentence (if debug)</span>
<a name="l00546"></a>00546 
<a name="l00547"></a>00547 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00548"></a>00548 <span class="preprocessor"></span>
<a name="l00549"></a>00549    <span class="keywordflow">if</span>(new_error_parameters_norm &lt; 0.0)
<a name="l00550"></a>00550    {
<a name="l00551"></a>00551       std::ostringstream buffer;
<a name="l00552"></a>00552 
<a name="l00553"></a>00553       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00554"></a>00554              &lt;&lt; <span class="stringliteral">"void set_error_parameters_norm(const double&amp;) method.\n"</span>
<a name="l00555"></a>00555              &lt;&lt; <span class="stringliteral">"Error parameters norm must be equal or greater than 0.\n"</span>;
<a name="l00556"></a>00556 
<a name="l00557"></a>00557       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00558"></a>00558    }
<a name="l00559"></a>00559 
<a name="l00560"></a>00560 <span class="preprocessor">   #endif</span>
<a name="l00561"></a>00561 <span class="preprocessor"></span>
<a name="l00562"></a>00562    <span class="comment">// Set error parameters norm</span>
<a name="l00563"></a>00563 
<a name="l00564"></a>00564    error_parameters_norm = new_error_parameters_norm;
<a name="l00565"></a>00565 }
<a name="l00566"></a>00566 
<a name="l00567"></a>00567 
<a name="l00568"></a>00568 <span class="comment">// void set_error_gradient_norm(const double&amp;) method</span>
<a name="l00569"></a>00569 
<a name="l00573"></a>00573 
<a name="l00574"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#e8d1a6991a7a735195ed65b15315d233">00574</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#e8d1a6991a7a735195ed65b15315d233">GradientDescent::set_error_gradient_norm</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_error_gradient_norm)
<a name="l00575"></a>00575 {
<a name="l00576"></a>00576    <span class="comment">// Control sentence (if debug)</span>
<a name="l00577"></a>00577 
<a name="l00578"></a>00578 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00579"></a>00579 <span class="preprocessor"></span>
<a name="l00580"></a>00580    <span class="keywordflow">if</span>(new_error_gradient_norm &lt; 0.0)
<a name="l00581"></a>00581    {
<a name="l00582"></a>00582       std::ostringstream buffer;
<a name="l00583"></a>00583 
<a name="l00584"></a>00584       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00585"></a>00585              &lt;&lt; <span class="stringliteral">"void set_error_gradient_norm(const double&amp;) method.\n"</span>
<a name="l00586"></a>00586              &lt;&lt; <span class="stringliteral">"Error gradient norm must be equal or greater than 0.\n"</span>;
<a name="l00587"></a>00587 
<a name="l00588"></a>00588       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00589"></a>00589    }
<a name="l00590"></a>00590 
<a name="l00591"></a>00591 <span class="preprocessor">   #endif</span>
<a name="l00592"></a>00592 <span class="preprocessor"></span>
<a name="l00593"></a>00593    <span class="comment">// Set error gradient norm</span>
<a name="l00594"></a>00594 
<a name="l00595"></a>00595    error_gradient_norm = new_error_gradient_norm;
<a name="l00596"></a>00596 }
<a name="l00597"></a>00597 
<a name="l00598"></a>00598 
<a name="l00599"></a>00599 <span class="comment">// void set_error_training_rate(const double&amp;) method</span>
<a name="l00600"></a>00600 
<a name="l00604"></a>00604 
<a name="l00605"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#1d3b83e0e926af808f2d6a3b2bde050c">00605</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#1d3b83e0e926af808f2d6a3b2bde050c">GradientDescent::set_error_training_rate</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_error_training_rate)
<a name="l00606"></a>00606 {
<a name="l00607"></a>00607    <span class="comment">// Control sentence (if debug)</span>
<a name="l00608"></a>00608 
<a name="l00609"></a>00609 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00610"></a>00610 <span class="preprocessor"></span>
<a name="l00611"></a>00611    <span class="keywordflow">if</span>(new_error_training_rate &lt; 0.0)
<a name="l00612"></a>00612    {
<a name="l00613"></a>00613       std::ostringstream buffer;
<a name="l00614"></a>00614 
<a name="l00615"></a>00615       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00616"></a>00616              &lt;&lt; <span class="stringliteral">"void set_error_training_rate(const double&amp;) method.\n"</span>
<a name="l00617"></a>00617              &lt;&lt; <span class="stringliteral">"Error training rate must be equal or greater than 0.\n"</span>;
<a name="l00618"></a>00618 
<a name="l00619"></a>00619       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00620"></a>00620    }
<a name="l00621"></a>00621 
<a name="l00622"></a>00622 <span class="preprocessor">   #endif</span>
<a name="l00623"></a>00623 <span class="preprocessor"></span>
<a name="l00624"></a>00624    <span class="comment">// Set error training rate</span>
<a name="l00625"></a>00625 
<a name="l00626"></a>00626    error_training_rate = new_error_training_rate;
<a name="l00627"></a>00627 }
<a name="l00628"></a>00628 
<a name="l00629"></a>00629 
<a name="l00630"></a>00630 <span class="comment">// void set_minimum_parameters_increment_norm(const double&amp;) method</span>
<a name="l00631"></a>00631 
<a name="l00634"></a>00634 
<a name="l00635"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#6ebdcab74360e712408d2b5f734cc2d8">00635</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#6ebdcab74360e712408d2b5f734cc2d8">GradientDescent::set_minimum_parameters_increment_norm</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_minimum_parameters_increment_norm)
<a name="l00636"></a>00636 {
<a name="l00637"></a>00637    <span class="comment">// Control sentence (if debug)</span>
<a name="l00638"></a>00638 
<a name="l00639"></a>00639 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00640"></a>00640 <span class="preprocessor"></span>
<a name="l00641"></a>00641    <span class="keywordflow">if</span>(new_minimum_parameters_increment_norm &lt; 0.0)
<a name="l00642"></a>00642    {
<a name="l00643"></a>00643       std::ostringstream buffer;
<a name="l00644"></a>00644 
<a name="l00645"></a>00645       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00646"></a>00646              &lt;&lt; <span class="stringliteral">"void new_minimum_parameters_increment_norm(const double&amp;) method.\n"</span>
<a name="l00647"></a>00647              &lt;&lt; <span class="stringliteral">"Minimum parameters increment norm must be equal or greater than 0.\n"</span>;
<a name="l00648"></a>00648 
<a name="l00649"></a>00649       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00650"></a>00650    }
<a name="l00651"></a>00651 
<a name="l00652"></a>00652 <span class="preprocessor">   #endif</span>
<a name="l00653"></a>00653 <span class="preprocessor"></span>
<a name="l00654"></a>00654    <span class="comment">// Set error training rate</span>
<a name="l00655"></a>00655 
<a name="l00656"></a>00656    minimum_parameters_increment_norm = new_minimum_parameters_increment_norm;
<a name="l00657"></a>00657 }
<a name="l00658"></a>00658 
<a name="l00659"></a>00659 
<a name="l00660"></a>00660 <span class="comment">// void set_minimum_performance_increase(const double&amp;) method</span>
<a name="l00661"></a>00661 
<a name="l00664"></a>00664 
<a name="l00665"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#a1a12c07c1db1420bb59f58c3a17fea4">00665</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#a1a12c07c1db1420bb59f58c3a17fea4">GradientDescent::set_minimum_performance_increase</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_minimum_performance_increase)
<a name="l00666"></a>00666 {
<a name="l00667"></a>00667    <span class="comment">// Control sentence (if debug)</span>
<a name="l00668"></a>00668 
<a name="l00669"></a>00669 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00670"></a>00670 <span class="preprocessor"></span>
<a name="l00671"></a>00671    <span class="keywordflow">if</span>(new_minimum_performance_increase &lt; 0.0)
<a name="l00672"></a>00672    {
<a name="l00673"></a>00673       std::ostringstream buffer;
<a name="l00674"></a>00674 
<a name="l00675"></a>00675       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00676"></a>00676              &lt;&lt; <span class="stringliteral">"void set_minimum_performance_increase(const double&amp;) method.\n"</span>
<a name="l00677"></a>00677              &lt;&lt; <span class="stringliteral">"Minimum performance improvement must be equal or greater than 0.\n"</span>;
<a name="l00678"></a>00678 
<a name="l00679"></a>00679       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00680"></a>00680    }
<a name="l00681"></a>00681 
<a name="l00682"></a>00682 <span class="preprocessor">   #endif</span>
<a name="l00683"></a>00683 <span class="preprocessor"></span>
<a name="l00684"></a>00684    <span class="comment">// Set minimum performance improvement</span>
<a name="l00685"></a>00685 
<a name="l00686"></a>00686    minimum_performance_increase = new_minimum_performance_increase;
<a name="l00687"></a>00687 }
<a name="l00688"></a>00688 
<a name="l00689"></a>00689 
<a name="l00690"></a>00690 <span class="comment">// void set_performance_goal(const double&amp;) method</span>
<a name="l00691"></a>00691 
<a name="l00695"></a>00695 
<a name="l00696"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#8e10381a376476d92e3c96a56a7cf257">00696</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#8e10381a376476d92e3c96a56a7cf257">GradientDescent::set_performance_goal</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_performance_goal)
<a name="l00697"></a>00697 {
<a name="l00698"></a>00698    performance_goal = new_performance_goal;
<a name="l00699"></a>00699 }
<a name="l00700"></a>00700 
<a name="l00701"></a>00701 
<a name="l00702"></a>00702 <span class="comment">// void set_gradient_norm_goal(const double&amp;) method</span>
<a name="l00703"></a>00703 
<a name="l00707"></a>00707 
<a name="l00708"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#1ad68485a9faa5a39c134b4177551c48">00708</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#1ad68485a9faa5a39c134b4177551c48">GradientDescent::set_gradient_norm_goal</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_gradient_norm_goal)
<a name="l00709"></a>00709 {
<a name="l00710"></a>00710    <span class="comment">// Control sentence (if debug)</span>
<a name="l00711"></a>00711 
<a name="l00712"></a>00712 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00713"></a>00713 <span class="preprocessor"></span>
<a name="l00714"></a>00714    <span class="keywordflow">if</span>(new_gradient_norm_goal &lt; 0.0)
<a name="l00715"></a>00715    {
<a name="l00716"></a>00716       std::ostringstream buffer;
<a name="l00717"></a>00717 
<a name="l00718"></a>00718       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: GradientDescent class.\n"</span>
<a name="l00719"></a>00719              &lt;&lt; <span class="stringliteral">"void set_gradient_norm_goal(const double&amp;) method.\n"</span>
<a name="l00720"></a>00720              &lt;&lt; <span class="stringliteral">"Gradient norm goal must be equal or greater than 0.\n"</span>;
<a name="l00721"></a>00721 
<a name="l00722"></a>00722       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00723"></a>00723    }
<a name="l00724"></a>00724 
<a name="l00725"></a>00725 <span class="preprocessor">   #endif</span>
<a name="l00726"></a>00726 <span class="preprocessor"></span>
<a name="l00727"></a>00727    <span class="comment">// Set gradient norm goal</span>
<a name="l00728"></a>00728 
<a name="l00729"></a>00729    gradient_norm_goal = new_gradient_norm_goal;
<a name="l00730"></a>00730 }
<a name="l00731"></a>00731 
<a name="l00732"></a>00732 
<a name="l00733"></a>00733 <span class="comment">// void set_maximum_generalization_evaluation_decreases(const unsigned int&amp;) method</span>
<a name="l00734"></a>00734 
<a name="l00737"></a>00737 
<a name="l00738"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#de9212832bd5d3a23f751d830f153e8a">00738</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#de9212832bd5d3a23f751d830f153e8a">GradientDescent::set_maximum_generalization_evaluation_decreases</a>(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; new_maximum_generalization_evaluation_decreases)
<a name="l00739"></a>00739 {
<a name="l00740"></a>00740    <span class="comment">// Control sentence (if debug)</span>
<a name="l00741"></a>00741 
<a name="l00742"></a>00742 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00743"></a>00743 <span class="preprocessor"></span>
<a name="l00744"></a>00744    <span class="keywordflow">if</span>(new_maximum_generalization_evaluation_decreases &lt; 0)
<a name="l00745"></a>00745    {
<a name="l00746"></a>00746       std::ostringstream buffer;
<a name="l00747"></a>00747 
<a name="l00748"></a>00748       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00749"></a>00749              &lt;&lt; <span class="stringliteral">"void set_maximum_generalization_evaluation_decreases(const unsigned int&amp;) method.\n"</span>
<a name="l00750"></a>00750              &lt;&lt; <span class="stringliteral">"Number of generalization performance decreases must be equal or greater than 0.\n"</span>;
<a name="l00751"></a>00751 
<a name="l00752"></a>00752       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00753"></a>00753    }
<a name="l00754"></a>00754 
<a name="l00755"></a>00755 <span class="preprocessor">   #endif</span>
<a name="l00756"></a>00756 <span class="preprocessor"></span>
<a name="l00757"></a>00757    <span class="comment">// Set maximum generalization performace decrases</span>
<a name="l00758"></a>00758 
<a name="l00759"></a>00759    maximum_generalization_evaluation_decreases = new_maximum_generalization_evaluation_decreases;
<a name="l00760"></a>00760 }
<a name="l00761"></a>00761 
<a name="l00762"></a>00762 
<a name="l00763"></a>00763 <span class="comment">// void set_maximum_epochs_number(unsigned int) method</span>
<a name="l00764"></a>00764 
<a name="l00767"></a>00767 
<a name="l00768"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#f66661bb709e3b68c748a4307e8bee45">00768</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#f66661bb709e3b68c748a4307e8bee45">GradientDescent::set_maximum_epochs_number</a>(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; new_maximum_epochs_number)
<a name="l00769"></a>00769 {
<a name="l00770"></a>00770    <span class="comment">// Control sentence (if debug)</span>
<a name="l00771"></a>00771 
<a name="l00772"></a>00772 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00773"></a>00773 <span class="preprocessor"></span>
<a name="l00774"></a>00774    <span class="keywordflow">if</span>(new_maximum_epochs_number &lt; 0)
<a name="l00775"></a>00775    {
<a name="l00776"></a>00776       std::ostringstream buffer;
<a name="l00777"></a>00777 
<a name="l00778"></a>00778       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00779"></a>00779              &lt;&lt; <span class="stringliteral">"void set_maximum_epochs_number(unsigned int) method.\n"</span>
<a name="l00780"></a>00780              &lt;&lt; <span class="stringliteral">"Number of epochs must be equal or greater than 0.\n"</span>;
<a name="l00781"></a>00781 
<a name="l00782"></a>00782       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00783"></a>00783    }
<a name="l00784"></a>00784 
<a name="l00785"></a>00785 <span class="preprocessor">   #endif</span>
<a name="l00786"></a>00786 <span class="preprocessor"></span>
<a name="l00787"></a>00787    <span class="comment">// Set maximum epochs number</span>
<a name="l00788"></a>00788 
<a name="l00789"></a>00789    maximum_epochs_number = new_maximum_epochs_number;
<a name="l00790"></a>00790 }
<a name="l00791"></a>00791 
<a name="l00792"></a>00792 
<a name="l00793"></a>00793 <span class="comment">// void set_maximum_time(const double&amp;) method</span>
<a name="l00794"></a>00794 
<a name="l00797"></a>00797 
<a name="l00798"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#79b6c48c3f392baeaabff840d22cb5b3">00798</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#79b6c48c3f392baeaabff840d22cb5b3">GradientDescent::set_maximum_time</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_maximum_time)
<a name="l00799"></a>00799 {
<a name="l00800"></a>00800    <span class="comment">// Control sentence (if debug)</span>
<a name="l00801"></a>00801 
<a name="l00802"></a>00802 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00803"></a>00803 <span class="preprocessor"></span>
<a name="l00804"></a>00804    <span class="keywordflow">if</span>(new_maximum_time &lt; 0.0)
<a name="l00805"></a>00805    {
<a name="l00806"></a>00806       std::ostringstream buffer;
<a name="l00807"></a>00807 
<a name="l00808"></a>00808       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00809"></a>00809              &lt;&lt; <span class="stringliteral">"void set_maximum_time(const double&amp;) method.\n"</span>
<a name="l00810"></a>00810              &lt;&lt; <span class="stringliteral">"Maximum time must be equal or greater than 0.\n"</span>;
<a name="l00811"></a>00811 
<a name="l00812"></a>00812       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00813"></a>00813    }
<a name="l00814"></a>00814    
<a name="l00815"></a>00815 <span class="preprocessor">   #endif</span>
<a name="l00816"></a>00816 <span class="preprocessor"></span>
<a name="l00817"></a>00817    <span class="comment">// Set maximum time</span>
<a name="l00818"></a>00818 
<a name="l00819"></a>00819    maximum_time = new_maximum_time;
<a name="l00820"></a>00820 }
<a name="l00821"></a>00821 
<a name="l00822"></a>00822 
<a name="l00823"></a>00823 <span class="comment">// void set_reserve_parameters_history(bool) method</span>
<a name="l00824"></a>00824 
<a name="l00827"></a>00827 
<a name="l00828"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#3d828a36bf0df6069f04d5c7ac2eeb51">00828</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#3d828a36bf0df6069f04d5c7ac2eeb51">GradientDescent::set_reserve_parameters_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_parameters_history)
<a name="l00829"></a>00829 {
<a name="l00830"></a>00830    reserve_parameters_history = new_reserve_parameters_history;     
<a name="l00831"></a>00831 }
<a name="l00832"></a>00832 
<a name="l00833"></a>00833 
<a name="l00834"></a>00834 <span class="comment">// void set_reserve_parameters_norm_history(bool) method</span>
<a name="l00835"></a>00835 
<a name="l00838"></a>00838 
<a name="l00839"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#daf7e1540cc81138bd541bee36269464">00839</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#daf7e1540cc81138bd541bee36269464">GradientDescent::set_reserve_parameters_norm_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_parameters_norm_history)
<a name="l00840"></a>00840 {
<a name="l00841"></a>00841    reserve_parameters_norm_history = new_reserve_parameters_norm_history;     
<a name="l00842"></a>00842 }
<a name="l00843"></a>00843 
<a name="l00844"></a>00844 
<a name="l00845"></a>00845 <span class="comment">// void set_reserve_evaluation_history(bool) method</span>
<a name="l00846"></a>00846 
<a name="l00849"></a>00849 
<a name="l00850"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#aa21d82bbe0fd3bb31295e97da9593c9">00850</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#aa21d82bbe0fd3bb31295e97da9593c9">GradientDescent::set_reserve_evaluation_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_evaluation_history)
<a name="l00851"></a>00851 {
<a name="l00852"></a>00852    reserve_evaluation_history = new_reserve_evaluation_history;     
<a name="l00853"></a>00853 }
<a name="l00854"></a>00854 
<a name="l00855"></a>00855 
<a name="l00856"></a>00856 <span class="comment">// void set_reserve_gradient_history(bool) method</span>
<a name="l00857"></a>00857 
<a name="l00860"></a>00860 
<a name="l00861"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#8ec7e29c6611ca1d0e3d03d543d62ce7">00861</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#8ec7e29c6611ca1d0e3d03d543d62ce7">GradientDescent::set_reserve_gradient_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_gradient_history)
<a name="l00862"></a>00862 {
<a name="l00863"></a>00863    reserve_gradient_history = new_reserve_gradient_history;    
<a name="l00864"></a>00864 }
<a name="l00865"></a>00865 
<a name="l00866"></a>00866 
<a name="l00867"></a>00867 <span class="comment">// void set_reserve_gradient_norm_history(bool) method</span>
<a name="l00868"></a>00868 
<a name="l00872"></a>00872 
<a name="l00873"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#c35bd72977a353a1edbc01178c1965e1">00873</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#c35bd72977a353a1edbc01178c1965e1">GradientDescent::set_reserve_gradient_norm_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_gradient_norm_history)
<a name="l00874"></a>00874 {
<a name="l00875"></a>00875    reserve_gradient_norm_history = new_reserve_gradient_norm_history;     
<a name="l00876"></a>00876 }
<a name="l00877"></a>00877 
<a name="l00878"></a>00878 
<a name="l00879"></a>00879 <span class="comment">// void set_reserve_training_direction_history(bool) method</span>
<a name="l00880"></a>00880 
<a name="l00884"></a>00884 
<a name="l00885"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#9c2553d655440601e779a3d093de593c">00885</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#9c2553d655440601e779a3d093de593c">GradientDescent::set_reserve_training_direction_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_training_direction_history)
<a name="l00886"></a>00886 {
<a name="l00887"></a>00887    reserve_training_direction_history = new_reserve_training_direction_history;          
<a name="l00888"></a>00888 }
<a name="l00889"></a>00889 
<a name="l00890"></a>00890 
<a name="l00891"></a>00891 <span class="comment">// void set_reserve_training_rate_history(bool) method</span>
<a name="l00892"></a>00892 
<a name="l00896"></a>00896 
<a name="l00897"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#7adc9d5149d9599aa3df9d254974d503">00897</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#7adc9d5149d9599aa3df9d254974d503">GradientDescent::set_reserve_training_rate_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_training_rate_history)
<a name="l00898"></a>00898 {
<a name="l00899"></a>00899    reserve_training_rate_history = new_reserve_training_rate_history;          
<a name="l00900"></a>00900 }
<a name="l00901"></a>00901 
<a name="l00902"></a>00902 
<a name="l00903"></a>00903 <span class="comment">// void set_reserve_elapsed_time_history(bool) method</span>
<a name="l00904"></a>00904 
<a name="l00908"></a>00908 
<a name="l00909"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#8e98003d637460b1a6f57cc5063b77d8">00909</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#8e98003d637460b1a6f57cc5063b77d8">GradientDescent::set_reserve_elapsed_time_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_elapsed_time_history)
<a name="l00910"></a>00910 {
<a name="l00911"></a>00911    reserve_elapsed_time_history = new_reserve_elapsed_time_history;     
<a name="l00912"></a>00912 }
<a name="l00913"></a>00913 
<a name="l00914"></a>00914 
<a name="l00915"></a>00915 <span class="comment">// void set_reserve_generalization_evaluation_history(bool) method</span>
<a name="l00916"></a>00916 
<a name="l00920"></a>00920 
<a name="l00921"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#05e2aedc5889583d5cfcdd3e69ea24ce">00921</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#05e2aedc5889583d5cfcdd3e69ea24ce">GradientDescent::set_reserve_generalization_evaluation_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_generalization_evaluation_history)  
<a name="l00922"></a>00922 {
<a name="l00923"></a>00923    reserve_generalization_evaluation_history = new_reserve_generalization_evaluation_history;
<a name="l00924"></a>00924 }
<a name="l00925"></a>00925 
<a name="l00926"></a>00926 
<a name="l00927"></a>00927 <span class="comment">// void set_display_period(unsigned int) method</span>
<a name="l00928"></a>00928 
<a name="l00932"></a>00932 
<a name="l00933"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#a296ed469c45fd0db6a301f120c3b257">00933</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#a296ed469c45fd0db6a301f120c3b257">GradientDescent::set_display_period</a>(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; new_display_period)
<a name="l00934"></a>00934 {
<a name="l00935"></a>00935    <span class="comment">// Control sentence (if debug)</span>
<a name="l00936"></a>00936 
<a name="l00937"></a>00937 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00938"></a>00938 <span class="preprocessor"></span>     
<a name="l00939"></a>00939    <span class="keywordflow">if</span>(new_display_period &lt;= 0)
<a name="l00940"></a>00940    {
<a name="l00941"></a>00941       std::ostringstream buffer;
<a name="l00942"></a>00942 
<a name="l00943"></a>00943       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00944"></a>00944              &lt;&lt; <span class="stringliteral">"void set_display_period(const double&amp;) method.\n"</span>
<a name="l00945"></a>00945              &lt;&lt; <span class="stringliteral">"First training rate must be greater than 0.\n"</span>;
<a name="l00946"></a>00946 
<a name="l00947"></a>00947       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00948"></a>00948    }
<a name="l00949"></a>00949 
<a name="l00950"></a>00950 <span class="preprocessor">   #endif</span>
<a name="l00951"></a>00951 <span class="preprocessor"></span>
<a name="l00952"></a>00952    display_period = new_display_period;
<a name="l00953"></a>00953 }
<a name="l00954"></a>00954 
<a name="l00955"></a>00955 
<a name="l00956"></a>00956 
<a name="l00957"></a>00957 <span class="comment">// Vector&lt;double&gt; calculate_training_direction(const Vector&lt;double&gt;&amp;) const method</span>
<a name="l00958"></a>00958 
<a name="l00960"></a>00960 <span class="comment">//@todo</span>
<a name="l00961"></a>00961 
<a name="l00962"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#c77548469088ea0f6256a531aff79cd1">00962</a> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#c77548469088ea0f6256a531aff79cd1" title="This method returns the gradient descent training direction, which is the negative...">GradientDescent::calculate_training_direction</a>(<span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; gradient)<span class="keyword"> const</span>
<a name="l00963"></a>00963 <span class="keyword"></span>{
<a name="l00964"></a>00964    <span class="keyword">const</span> <span class="keywordtype">double</span> gradient_norm = gradient.<a class="code" href="class_open_n_n_1_1_vector.html#17c63b7346189c7337602b0aac5f529f" title="This element returns the vector norm.">calculate_norm</a>();
<a name="l00965"></a>00965 
<a name="l00966"></a>00966    <span class="keywordflow">return</span>(gradient*(-1.0)/gradient_norm);
<a name="l00967"></a>00967 }
<a name="l00968"></a>00968 
<a name="l00969"></a>00969 
<a name="l00970"></a>00970 <span class="comment">// std::string GradientDescentResults::to_string(void) const method</span>
<a name="l00971"></a>00971 
<a name="l00972"></a><a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#2dd38073cfa1e2a36947cd51c52a082f">00972</a> std::string <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#2dd38073cfa1e2a36947cd51c52a082f" title="This method returns a string representation of the results structure.">GradientDescent::GradientDescentResults::to_string</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00973"></a>00973 <span class="keyword"></span>{
<a name="l00974"></a>00974    std::ostringstream buffer;
<a name="l00975"></a>00975 
<a name="l00976"></a>00976    <span class="comment">// Parameters history</span>
<a name="l00977"></a>00977 
<a name="l00978"></a>00978    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#601013254de93dee73b9f278ca4f87e5" title="History of the neural network parameters over the training epochs.">parameters_history</a>.empty())
<a name="l00979"></a>00979    {
<a name="l00980"></a>00980       <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#601013254de93dee73b9f278ca4f87e5" title="History of the neural network parameters over the training epochs.">parameters_history</a>[0].empty())
<a name="l00981"></a>00981       {
<a name="l00982"></a>00982           buffer &lt;&lt; <span class="stringliteral">"% Parameters history:\n"</span>
<a name="l00983"></a>00983                  &lt;&lt; <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#601013254de93dee73b9f278ca4f87e5" title="History of the neural network parameters over the training epochs.">parameters_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l00984"></a>00984           }
<a name="l00985"></a>00985    }
<a name="l00986"></a>00986 
<a name="l00987"></a>00987    <span class="comment">// Parameters norm history</span>
<a name="l00988"></a>00988 
<a name="l00989"></a>00989    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#a59d49e7b828f3afc265f7936ad0d482" title="History of the parameters norm over the training epochs.">parameters_norm_history</a>.empty())
<a name="l00990"></a>00990    {
<a name="l00991"></a>00991        buffer &lt;&lt; <span class="stringliteral">"% Parameters norm history:\n"</span>
<a name="l00992"></a>00992               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#a59d49e7b828f3afc265f7936ad0d482" title="History of the parameters norm over the training epochs.">parameters_norm_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l00993"></a>00993    }
<a name="l00994"></a>00994 
<a name="l00995"></a>00995    <span class="comment">// Performance history   </span>
<a name="l00996"></a>00996 
<a name="l00997"></a>00997    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#dec274a4e28543f27a56756555b7e696" title="History of the performance function evaluation over the training epochs.">evaluation_history</a>.empty())
<a name="l00998"></a>00998    {
<a name="l00999"></a>00999        buffer &lt;&lt; <span class="stringliteral">"% Performance history:\n"</span>
<a name="l01000"></a>01000               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#dec274a4e28543f27a56756555b7e696" title="History of the performance function evaluation over the training epochs.">evaluation_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01001"></a>01001    }
<a name="l01002"></a>01002 
<a name="l01003"></a>01003    <span class="comment">// Generalization evaluation history</span>
<a name="l01004"></a>01004 
<a name="l01005"></a>01005    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#d662b30acce160b20ae6d33be506e74b" title="History of the generalization evaluation over the training epochs.">generalization_evaluation_history</a>.empty())
<a name="l01006"></a>01006    {
<a name="l01007"></a>01007        buffer &lt;&lt; <span class="stringliteral">"% Generalization evaluation history:\n"</span>
<a name="l01008"></a>01008               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#d662b30acce160b20ae6d33be506e74b" title="History of the generalization evaluation over the training epochs.">generalization_evaluation_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01009"></a>01009    }
<a name="l01010"></a>01010 
<a name="l01011"></a>01011    <span class="comment">// Gradient history </span>
<a name="l01012"></a>01012 
<a name="l01013"></a>01013    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#fd665a438eea61c97e9e1cef34be499c" title="History of the performance function gradient over the training epochs.">gradient_history</a>.empty())
<a name="l01014"></a>01014    {
<a name="l01015"></a>01015       <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#fd665a438eea61c97e9e1cef34be499c" title="History of the performance function gradient over the training epochs.">gradient_history</a>[0].empty())
<a name="l01016"></a>01016       {
<a name="l01017"></a>01017           buffer &lt;&lt; <span class="stringliteral">"% Gradient history:\n"</span>
<a name="l01018"></a>01018                  &lt;&lt; <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#fd665a438eea61c97e9e1cef34be499c" title="History of the performance function gradient over the training epochs.">gradient_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01019"></a>01019           }
<a name="l01020"></a>01020    }
<a name="l01021"></a>01021 
<a name="l01022"></a>01022    <span class="comment">// Gradient norm history</span>
<a name="l01023"></a>01023 
<a name="l01024"></a>01024    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#16479a6cf522e556aa03ba696a092348" title="History of the gradient norm over the training epochs.">gradient_norm_history</a>.empty())
<a name="l01025"></a>01025    {
<a name="l01026"></a>01026        buffer &lt;&lt; <span class="stringliteral">"% Gradient norm history:\n"</span>
<a name="l01027"></a>01027               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#16479a6cf522e556aa03ba696a092348" title="History of the gradient norm over the training epochs.">gradient_norm_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01028"></a>01028    }
<a name="l01029"></a>01029 
<a name="l01030"></a>01030    <span class="comment">// Training direction history</span>
<a name="l01031"></a>01031 
<a name="l01032"></a>01032    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#18aa4be2d6ad1b117e42999b6917b302" title="History of the random search training direction over the training epochs.">training_direction_history</a>.empty())
<a name="l01033"></a>01033    {
<a name="l01034"></a>01034       <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#18aa4be2d6ad1b117e42999b6917b302" title="History of the random search training direction over the training epochs.">training_direction_history</a>[0].empty())
<a name="l01035"></a>01035       {
<a name="l01036"></a>01036          buffer &lt;&lt; <span class="stringliteral">"% Training direction history:\n"</span>
<a name="l01037"></a>01037                 &lt;&lt; <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#18aa4be2d6ad1b117e42999b6917b302" title="History of the random search training direction over the training epochs.">training_direction_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01038"></a>01038           }
<a name="l01039"></a>01039    }
<a name="l01040"></a>01040 
<a name="l01041"></a>01041    <span class="comment">// Training rate history   </span>
<a name="l01042"></a>01042 
<a name="l01043"></a>01043    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#bc17ab2b4182b9433bb1727361e5577a" title="History of the random search training rate over the training epochs.">training_rate_history</a>.empty())
<a name="l01044"></a>01044    {
<a name="l01045"></a>01045        buffer &lt;&lt; <span class="stringliteral">"% Training rate history:\n"</span>
<a name="l01046"></a>01046               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#bc17ab2b4182b9433bb1727361e5577a" title="History of the random search training rate over the training epochs.">training_rate_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01047"></a>01047    }
<a name="l01048"></a>01048 
<a name="l01049"></a>01049    <span class="comment">// Elapsed time history</span>
<a name="l01050"></a>01050 
<a name="l01051"></a>01051    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#06298caa84b2778d0cca72a5a0a46e99" title="History of the elapsed time over the training epochs.">elapsed_time_history</a>.empty())
<a name="l01052"></a>01052    {
<a name="l01053"></a>01053        buffer &lt;&lt; <span class="stringliteral">"% Elapsed time history:\n"</span>
<a name="l01054"></a>01054               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#06298caa84b2778d0cca72a5a0a46e99" title="History of the elapsed time over the training epochs.">elapsed_time_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01055"></a>01055    }
<a name="l01056"></a>01056 
<a name="l01057"></a>01057    <span class="keywordflow">return</span>(buffer.str());
<a name="l01058"></a>01058 }
<a name="l01059"></a>01059 
<a name="l01060"></a>01060 
<a name="l01061"></a>01061 <span class="comment">// void GradientDescentResults::resize_training_history(const unsigned int&amp; new_size) method</span>
<a name="l01062"></a>01062 
<a name="l01065"></a>01065 
<a name="l01066"></a><a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#899325ad55f53de29083af88a0f6e6a3">01066</a> <span class="keywordtype">void</span> <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#899325ad55f53de29083af88a0f6e6a3">GradientDescent::GradientDescentResults::resize_training_history</a>(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; new_size)
<a name="l01067"></a>01067 {
<a name="l01068"></a>01068    <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#601013254de93dee73b9f278ca4f87e5" title="History of the neural network parameters over the training epochs.">parameters_history</a>.resize(new_size);
<a name="l01069"></a>01069    <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#a59d49e7b828f3afc265f7936ad0d482" title="History of the parameters norm over the training epochs.">parameters_norm_history</a>.resize(new_size);
<a name="l01070"></a>01070 
<a name="l01071"></a>01071    <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#dec274a4e28543f27a56756555b7e696" title="History of the performance function evaluation over the training epochs.">evaluation_history</a>.resize(new_size);
<a name="l01072"></a>01072    <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#d662b30acce160b20ae6d33be506e74b" title="History of the generalization evaluation over the training epochs.">generalization_evaluation_history</a>.resize(new_size);
<a name="l01073"></a>01073    <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#fd665a438eea61c97e9e1cef34be499c" title="History of the performance function gradient over the training epochs.">gradient_history</a>.resize(new_size);
<a name="l01074"></a>01074    <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#16479a6cf522e556aa03ba696a092348" title="History of the gradient norm over the training epochs.">gradient_norm_history</a>.resize(new_size);
<a name="l01075"></a>01075    
<a name="l01076"></a>01076    <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#18aa4be2d6ad1b117e42999b6917b302" title="History of the random search training direction over the training epochs.">training_direction_history</a>.resize(new_size);
<a name="l01077"></a>01077    <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#bc17ab2b4182b9433bb1727361e5577a" title="History of the random search training rate over the training epochs.">training_rate_history</a>.resize(new_size);
<a name="l01078"></a>01078    <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#06298caa84b2778d0cca72a5a0a46e99" title="History of the elapsed time over the training epochs.">elapsed_time_history</a>.resize(new_size);
<a name="l01079"></a>01079 }
<a name="l01080"></a>01080 
<a name="l01081"></a>01081 
<a name="l01082"></a>01082 <span class="comment">// GradientDescentResults* perform_training(void) method</span>
<a name="l01083"></a>01083 
<a name="l01085"></a>01085 
<a name="l01086"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#35748714df5dcc4f03c642084b5d16cb">01086</a> <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html">GradientDescent::GradientDescentResults</a>* <a class="code" href="class_open_n_n_1_1_gradient_descent.html#35748714df5dcc4f03c642084b5d16cb">GradientDescent::perform_training</a>(<span class="keywordtype">void</span>)
<a name="l01087"></a>01087 {
<a name="l01088"></a>01088    <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html">GradientDescentResults</a>* training_results_pointer = <span class="keyword">new</span> <a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html">GradientDescentResults</a>; 
<a name="l01089"></a>01089 
<a name="l01090"></a>01090    <span class="comment">// Control sentence (if debug)</span>
<a name="l01091"></a>01091 
<a name="l01092"></a>01092 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l01093"></a>01093 <span class="preprocessor"></span>
<a name="l01094"></a>01094    <a class="code" href="class_open_n_n_1_1_training_algorithm.html#7bb1b776eaf2742280096376389bd37e">check</a>();
<a name="l01095"></a>01095 
<a name="l01096"></a>01096 <span class="preprocessor">   #endif</span>
<a name="l01097"></a>01097 <span class="preprocessor"></span>
<a name="l01098"></a>01098    <span class="comment">// Start training </span>
<a name="l01099"></a>01099 
<a name="l01100"></a>01100    <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a>)
<a name="l01101"></a>01101    {
<a name="l01102"></a>01102       std::cout &lt;&lt; <span class="stringliteral">"Training with gradient descent...\n"</span>;
<a name="l01103"></a>01103    }
<a name="l01104"></a>01104 
<a name="l01105"></a>01105    <span class="comment">// Neural network stuff</span>
<a name="l01106"></a>01106 
<a name="l01107"></a>01107    <a class="code" href="class_open_n_n_1_1_neural_network.html">NeuralNetwork</a>* neural_network_pointer = <a class="code" href="class_open_n_n_1_1_training_algorithm.html#90ba564d00e69786d58ed94672e6e8f4" title="Pointer to a performance functional for a multilayer perceptron object.">performance_functional_pointer</a>-&gt;<a class="code" href="class_open_n_n_1_1_performance_functional.html#d4d992ddddb4281f1da8c078b2db2610" title="This method returns a pointer to the neural network associated to the performance...">get_neural_network_pointer</a>();
<a name="l01108"></a>01108 
<a name="l01109"></a>01109    <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> parameters_number = neural_network_pointer-&gt;<a class="code" href="class_open_n_n_1_1_neural_network.html#20de593bf5517b7c2ca50feb689b20cc">count_parameters_number</a>();
<a name="l01110"></a>01110 
<a name="l01111"></a>01111    <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> parameters(parameters_number);
<a name="l01112"></a>01112    <span class="keywordtype">double</span> parameters_norm;
<a name="l01113"></a>01113 
<a name="l01114"></a>01114    <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> parameters_increment(parameters_number);
<a name="l01115"></a>01115    <span class="keywordtype">double</span> parameters_increment_norm;
<a name="l01116"></a>01116 
<a name="l01117"></a>01117    <span class="comment">// Performance functional stuff</span>
<a name="l01118"></a>01118 
<a name="l01119"></a>01119    <span class="keywordtype">double</span> generalization_evaluation = 0.0; 
<a name="l01120"></a>01120    <span class="keywordtype">double</span> old_generalization_evaluation = 0.0;
<a name="l01121"></a>01121    <span class="keywordtype">double</span> generalization_evaluation_increment = 0.0;
<a name="l01122"></a>01122       
<a name="l01123"></a>01123    <span class="keywordtype">double</span> performance = 0.0;
<a name="l01124"></a>01124    <span class="keywordtype">double</span> old_performance = 0.0;
<a name="l01125"></a>01125    <span class="keywordtype">double</span> performance_increase = 0.0;
<a name="l01126"></a>01126 
<a name="l01127"></a>01127    <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> gradient(parameters_number);
<a name="l01128"></a>01128    <span class="keywordtype">double</span> gradient_norm;
<a name="l01129"></a>01129 
<a name="l01130"></a>01130    <a class="code" href="struct_open_n_n_1_1_performance_functional_1_1_first_order_evaluation.html">PerformanceFunctional::FirstOrderEvaluation</a> first_order_evaluation;
<a name="l01131"></a>01131 
<a name="l01132"></a>01132    <span class="comment">// Training algorithm stuff </span>
<a name="l01133"></a>01133 
<a name="l01134"></a>01134    <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> generalization_evaluation_decreases_count = 0;
<a name="l01135"></a>01135 
<a name="l01136"></a>01136    <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> training_direction(parameters_number);
<a name="l01137"></a>01137 
<a name="l01138"></a>01138    <span class="keywordtype">double</span> initial_training_rate = 0.0;
<a name="l01139"></a>01139    <span class="keywordtype">double</span> training_rate = 0.0;
<a name="l01140"></a>01140    <span class="keywordtype">double</span> old_training_rate = 0.0;
<a name="l01141"></a>01141 
<a name="l01142"></a>01142    <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> directional_point(2);
<a name="l01143"></a>01143    directional_point[0] = 0.0;
<a name="l01144"></a>01144    directional_point[1] = 0.0;
<a name="l01145"></a>01145 
<a name="l01146"></a>01146    <span class="keywordtype">bool</span> stop_training = <span class="keyword">false</span>;
<a name="l01147"></a>01147 
<a name="l01148"></a>01148    time_t beginning_time, current_time;
<a name="l01149"></a>01149    time(&amp;beginning_time);
<a name="l01150"></a>01150    <span class="keywordtype">double</span> elapsed_time;
<a name="l01151"></a>01151 
<a name="l01152"></a>01152    training_results_pointer-&gt;<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#899325ad55f53de29083af88a0f6e6a3">resize_training_history</a>(maximum_epochs_number+1);    
<a name="l01153"></a>01153    
<a name="l01154"></a>01154    <span class="comment">// Main loop</span>
<a name="l01155"></a>01155 
<a name="l01156"></a>01156    <span class="keywordflow">for</span>(<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> epoch = 0; epoch &lt;= maximum_epochs_number; epoch++)
<a name="l01157"></a>01157    {
<a name="l01158"></a>01158       <span class="comment">// Neural network stuff</span>
<a name="l01159"></a>01159 
<a name="l01160"></a>01160       parameters = neural_network_pointer-&gt;<a class="code" href="class_open_n_n_1_1_neural_network.html#c84ff3f3ab91ec99cb157c80623ee63d">arrange_parameters</a>();
<a name="l01161"></a>01161 
<a name="l01162"></a>01162       parameters_norm = parameters.<a class="code" href="class_open_n_n_1_1_vector.html#17c63b7346189c7337602b0aac5f529f" title="This element returns the vector norm.">calculate_norm</a>();
<a name="l01163"></a>01163 
<a name="l01164"></a>01164       <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a> &amp;&amp; parameters_norm &gt;= warning_parameters_norm)
<a name="l01165"></a>01165       {
<a name="l01166"></a>01166          std::cout &lt;&lt; <span class="stringliteral">"OpenNN Warning: Parameters norm is "</span> &lt;&lt; parameters_norm &lt;&lt; <span class="stringliteral">".\n"</span>;          
<a name="l01167"></a>01167       }
<a name="l01168"></a>01168 
<a name="l01169"></a>01169       <span class="comment">// Performance functional stuff</span>
<a name="l01170"></a>01170       
<a name="l01171"></a>01171       <span class="keywordflow">if</span>(epoch == 0)
<a name="l01172"></a>01172       {      
<a name="l01173"></a>01173          performance = <a class="code" href="class_open_n_n_1_1_training_algorithm.html#90ba564d00e69786d58ed94672e6e8f4" title="Pointer to a performance functional for a multilayer perceptron object.">performance_functional_pointer</a>-&gt;<a class="code" href="class_open_n_n_1_1_performance_functional.html#7e5b8970b7bc383e9ca3722f413413ef">calculate_evaluation</a>();
<a name="l01174"></a>01174          performance_increase = 0.0; 
<a name="l01175"></a>01175       }
<a name="l01176"></a>01176       <span class="keywordflow">else</span>
<a name="l01177"></a>01177       {
<a name="l01178"></a>01178          performance = directional_point[1];
<a name="l01179"></a>01179          performance_increase = old_performance - performance; 
<a name="l01180"></a>01180       }
<a name="l01181"></a>01181 
<a name="l01182"></a>01182       gradient = <a class="code" href="class_open_n_n_1_1_training_algorithm.html#90ba564d00e69786d58ed94672e6e8f4" title="Pointer to a performance functional for a multilayer perceptron object.">performance_functional_pointer</a>-&gt;<a class="code" href="class_open_n_n_1_1_performance_functional.html#e737dd930d87aeba1e160fd4b557c672" title="This method returns the objective function gradient, as the sum of the objective...">calculate_gradient</a>();
<a name="l01183"></a>01183 
<a name="l01184"></a>01184       gradient_norm = gradient.<a class="code" href="class_open_n_n_1_1_vector.html#17c63b7346189c7337602b0aac5f529f" title="This element returns the vector norm.">calculate_norm</a>();
<a name="l01185"></a>01185 
<a name="l01186"></a>01186       <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a> &amp;&amp; gradient_norm &gt;= warning_gradient_norm)
<a name="l01187"></a>01187       {
<a name="l01188"></a>01188          std::cout &lt;&lt; <span class="stringliteral">"OpenNN Warning: Gradient norm is "</span> &lt;&lt; gradient_norm &lt;&lt; <span class="stringliteral">".\n"</span>;          
<a name="l01189"></a>01189       }
<a name="l01190"></a>01190 
<a name="l01191"></a>01191       generalization_evaluation = <a class="code" href="class_open_n_n_1_1_training_algorithm.html#90ba564d00e69786d58ed94672e6e8f4" title="Pointer to a performance functional for a multilayer perceptron object.">performance_functional_pointer</a>-&gt;<a class="code" href="class_open_n_n_1_1_performance_functional.html#3c2b99fabf16216c13d02ada73498982">calculate_generalization_evaluation</a>();
<a name="l01192"></a>01192 
<a name="l01193"></a>01193           <span class="keywordflow">if</span>(epoch != 0 &amp;&amp; generalization_evaluation &gt; old_generalization_evaluation)
<a name="l01194"></a>01194           {
<a name="l01195"></a>01195              generalization_evaluation_decreases_count++;         
<a name="l01196"></a>01196           }
<a name="l01197"></a>01197 
<a name="l01198"></a>01198       <span class="comment">// Training algorithm </span>
<a name="l01199"></a>01199 
<a name="l01200"></a>01200       training_direction = <a class="code" href="class_open_n_n_1_1_gradient_descent.html#c77548469088ea0f6256a531aff79cd1" title="This method returns the gradient descent training direction, which is the negative...">calculate_training_direction</a>(gradient);
<a name="l01201"></a>01201                         
<a name="l01202"></a>01202       <span class="keywordflow">if</span>(epoch == 0)
<a name="l01203"></a>01203       {
<a name="l01204"></a>01204          initial_training_rate = training_rate_algorithm.<a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html#8ffaea7532e210e0eca361b77f6960d7" title="This method returns the initial training rate value in line minimization.">get_first_training_rate</a>();
<a name="l01205"></a>01205       }
<a name="l01206"></a>01206       <span class="keywordflow">else</span>
<a name="l01207"></a>01207       {
<a name="l01208"></a>01208          initial_training_rate = old_training_rate;
<a name="l01209"></a>01209       }    
<a name="l01210"></a>01210       
<a name="l01211"></a>01211           directional_point = training_rate_algorithm.<a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html#b055b2baa3c91adbd522d5a41e5f5183">calculate_directional_point</a>(performance, training_direction, initial_training_rate);
<a name="l01212"></a>01212 
<a name="l01213"></a>01213       training_rate = directional_point[0];
<a name="l01214"></a>01214 
<a name="l01215"></a>01215       parameters_increment = training_direction*training_rate;
<a name="l01216"></a>01216       parameters_increment_norm = parameters_increment.<a class="code" href="class_open_n_n_1_1_vector.html#17c63b7346189c7337602b0aac5f529f" title="This element returns the vector norm.">calculate_norm</a>();
<a name="l01217"></a>01217       
<a name="l01218"></a>01218       <span class="comment">// Elapsed time</span>
<a name="l01219"></a>01219 
<a name="l01220"></a>01220       time(&amp;current_time);
<a name="l01221"></a>01221       elapsed_time = difftime(current_time, beginning_time);
<a name="l01222"></a>01222 
<a name="l01223"></a>01223       <span class="comment">// Training history multilayer perceptron </span>
<a name="l01224"></a>01224 
<a name="l01225"></a>01225       <span class="keywordflow">if</span>(reserve_parameters_history)
<a name="l01226"></a>01226       {
<a name="l01227"></a>01227          training_results_pointer-&gt;<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#601013254de93dee73b9f278ca4f87e5" title="History of the neural network parameters over the training epochs.">parameters_history</a>[epoch] = parameters;
<a name="l01228"></a>01228       }
<a name="l01229"></a>01229 
<a name="l01230"></a>01230       <span class="keywordflow">if</span>(reserve_parameters_norm_history)
<a name="l01231"></a>01231       {
<a name="l01232"></a>01232          training_results_pointer-&gt;<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#a59d49e7b828f3afc265f7936ad0d482" title="History of the parameters norm over the training epochs.">parameters_norm_history</a>[epoch] = parameters_norm; 
<a name="l01233"></a>01233       }
<a name="l01234"></a>01234 
<a name="l01235"></a>01235       <span class="comment">// Training history performance functional</span>
<a name="l01236"></a>01236 
<a name="l01237"></a>01237       <span class="keywordflow">if</span>(reserve_evaluation_history)
<a name="l01238"></a>01238       {
<a name="l01239"></a>01239          training_results_pointer-&gt;<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#dec274a4e28543f27a56756555b7e696" title="History of the performance function evaluation over the training epochs.">evaluation_history</a>[epoch] = performance;
<a name="l01240"></a>01240       }
<a name="l01241"></a>01241 
<a name="l01242"></a>01242       <span class="keywordflow">if</span>(reserve_gradient_history)
<a name="l01243"></a>01243       {
<a name="l01244"></a>01244          training_results_pointer-&gt;<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#fd665a438eea61c97e9e1cef34be499c" title="History of the performance function gradient over the training epochs.">gradient_history</a>[epoch] = gradient;                                
<a name="l01245"></a>01245       }
<a name="l01246"></a>01246 
<a name="l01247"></a>01247       <span class="keywordflow">if</span>(reserve_gradient_norm_history)
<a name="l01248"></a>01248       {
<a name="l01249"></a>01249          training_results_pointer-&gt;<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#16479a6cf522e556aa03ba696a092348" title="History of the gradient norm over the training epochs.">gradient_norm_history</a>[epoch] = gradient_norm;
<a name="l01250"></a>01250       }
<a name="l01251"></a>01251 
<a name="l01252"></a>01252       <span class="keywordflow">if</span>(reserve_generalization_evaluation_history)
<a name="l01253"></a>01253       {
<a name="l01254"></a>01254          training_results_pointer-&gt;<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#d662b30acce160b20ae6d33be506e74b" title="History of the generalization evaluation over the training epochs.">generalization_evaluation_history</a>[epoch] = generalization_evaluation;
<a name="l01255"></a>01255       }
<a name="l01256"></a>01256 
<a name="l01257"></a>01257       <span class="comment">// Training history training algorithm</span>
<a name="l01258"></a>01258 
<a name="l01259"></a>01259       <span class="keywordflow">if</span>(reserve_training_direction_history)
<a name="l01260"></a>01260       {
<a name="l01261"></a>01261          training_results_pointer-&gt;<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#18aa4be2d6ad1b117e42999b6917b302" title="History of the random search training direction over the training epochs.">training_direction_history</a>[epoch] = training_direction;                                
<a name="l01262"></a>01262       }
<a name="l01263"></a>01263 
<a name="l01264"></a>01264       <span class="keywordflow">if</span>(reserve_training_rate_history)
<a name="l01265"></a>01265       {
<a name="l01266"></a>01266          training_results_pointer-&gt;<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#bc17ab2b4182b9433bb1727361e5577a" title="History of the random search training rate over the training epochs.">training_rate_history</a>[epoch] = training_rate;
<a name="l01267"></a>01267       }
<a name="l01268"></a>01268 
<a name="l01269"></a>01269       <span class="keywordflow">if</span>(reserve_elapsed_time_history)
<a name="l01270"></a>01270       {
<a name="l01271"></a>01271          training_results_pointer-&gt;<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#06298caa84b2778d0cca72a5a0a46e99" title="History of the elapsed time over the training epochs.">elapsed_time_history</a>[epoch] = elapsed_time;
<a name="l01272"></a>01272       }
<a name="l01273"></a>01273 
<a name="l01274"></a>01274       <span class="comment">// Stopping Criteria</span>
<a name="l01275"></a>01275 
<a name="l01276"></a>01276       <span class="keywordflow">if</span>(parameters_increment_norm &lt;= minimum_parameters_increment_norm)
<a name="l01277"></a>01277       {
<a name="l01278"></a>01278          <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a>)
<a name="l01279"></a>01279          {
<a name="l01280"></a>01280             std::cout &lt;&lt; <span class="stringliteral">"Epoch "</span> &lt;&lt; epoch &lt;&lt; <span class="stringliteral">": Minimum parameters increment norm reached.\n"</span>;
<a name="l01281"></a>01281             std::cout &lt;&lt; <span class="stringliteral">"Parameters increment norm: "</span> &lt;&lt; parameters_increment_norm &lt;&lt; std::endl;
<a name="l01282"></a>01282          }
<a name="l01283"></a>01283 
<a name="l01284"></a>01284          stop_training = <span class="keyword">true</span>;
<a name="l01285"></a>01285       }
<a name="l01286"></a>01286 
<a name="l01287"></a>01287       <span class="keywordflow">else</span> <span class="keywordflow">if</span>(epoch != 0 &amp;&amp; performance_increase &lt;= minimum_performance_increase)
<a name="l01288"></a>01288       {
<a name="l01289"></a>01289          <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a>)
<a name="l01290"></a>01290          {
<a name="l01291"></a>01291             std::cout &lt;&lt; <span class="stringliteral">"Epoch "</span> &lt;&lt; epoch &lt;&lt; <span class="stringliteral">": Minimum performance increase reached.\n"</span>
<a name="l01292"></a>01292                       &lt;&lt; <span class="stringliteral">"Performance increase: "</span> &lt;&lt; performance_increase &lt;&lt; std::endl;
<a name="l01293"></a>01293          }
<a name="l01294"></a>01294 
<a name="l01295"></a>01295          stop_training = <span class="keyword">true</span>;
<a name="l01296"></a>01296       }
<a name="l01297"></a>01297 
<a name="l01298"></a>01298       <span class="keywordflow">else</span> <span class="keywordflow">if</span>(performance &lt;= performance_goal)
<a name="l01299"></a>01299       {
<a name="l01300"></a>01300          <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a>)
<a name="l01301"></a>01301          {
<a name="l01302"></a>01302             std::cout &lt;&lt; <span class="stringliteral">"Epoch "</span> &lt;&lt; epoch &lt;&lt; <span class="stringliteral">": Performance goal reached.\n"</span>;
<a name="l01303"></a>01303          }
<a name="l01304"></a>01304 
<a name="l01305"></a>01305          stop_training = <span class="keyword">true</span>;
<a name="l01306"></a>01306       }
<a name="l01307"></a>01307 
<a name="l01308"></a>01308       <span class="keywordflow">else</span> <span class="keywordflow">if</span>(epoch != 0 &amp;&amp; generalization_evaluation_increment &gt; 0.0)
<a name="l01309"></a>01309       {
<a name="l01310"></a>01310          <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a>)
<a name="l01311"></a>01311          {
<a name="l01312"></a>01312             std::cout &lt;&lt; <span class="stringliteral">"Epoch "</span> &lt;&lt; epoch &lt;&lt; <span class="stringliteral">": Validation performance stopped improving.\n"</span>;
<a name="l01313"></a>01313             std::cout &lt;&lt; <span class="stringliteral">"Validation performance increment: "</span> &lt;&lt; generalization_evaluation_increment &lt;&lt; std::endl;
<a name="l01314"></a>01314          }
<a name="l01315"></a>01315 
<a name="l01316"></a>01316          stop_training = <span class="keyword">true</span>;
<a name="l01317"></a>01317       }
<a name="l01318"></a>01318 
<a name="l01319"></a>01319       <span class="keywordflow">else</span> <span class="keywordflow">if</span>(gradient_norm &lt;= gradient_norm_goal)
<a name="l01320"></a>01320       {
<a name="l01321"></a>01321          <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a>)
<a name="l01322"></a>01322          {
<a name="l01323"></a>01323             std::cout &lt;&lt; <span class="stringliteral">"Epoch "</span> &lt;&lt; epoch &lt;&lt; <span class="stringliteral">": Gradient norm goal reached.\n"</span>;  
<a name="l01324"></a>01324          }
<a name="l01325"></a>01325 
<a name="l01326"></a>01326          stop_training = <span class="keyword">true</span>;
<a name="l01327"></a>01327       }
<a name="l01328"></a>01328 
<a name="l01329"></a>01329       <span class="keywordflow">else</span> <span class="keywordflow">if</span>(epoch == maximum_epochs_number)
<a name="l01330"></a>01330       {
<a name="l01331"></a>01331          <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a>)
<a name="l01332"></a>01332          {
<a name="l01333"></a>01333             std::cout &lt;&lt; <span class="stringliteral">"Epoch "</span> &lt;&lt; epoch &lt;&lt; <span class="stringliteral">": Maximum number of epochs reached.\n"</span>;
<a name="l01334"></a>01334          }
<a name="l01335"></a>01335 
<a name="l01336"></a>01336          stop_training = <span class="keyword">true</span>;
<a name="l01337"></a>01337       }
<a name="l01338"></a>01338 
<a name="l01339"></a>01339       <span class="keywordflow">else</span> <span class="keywordflow">if</span>(elapsed_time &gt;= maximum_time)
<a name="l01340"></a>01340       {
<a name="l01341"></a>01341          <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a>)
<a name="l01342"></a>01342          {
<a name="l01343"></a>01343             std::cout &lt;&lt; <span class="stringliteral">"Epoch "</span> &lt;&lt; epoch &lt;&lt; <span class="stringliteral">": Maximum training time reached.\n"</span>;
<a name="l01344"></a>01344          }
<a name="l01345"></a>01345 
<a name="l01346"></a>01346          stop_training = <span class="keyword">true</span>;
<a name="l01347"></a>01347       }
<a name="l01348"></a>01348 
<a name="l01349"></a>01349       <span class="keywordflow">if</span>(stop_training)
<a name="l01350"></a>01350       {
<a name="l01351"></a>01351          <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a>)
<a name="l01352"></a>01352                  {
<a name="l01353"></a>01353             std::cout &lt;&lt; <span class="stringliteral">"Parameters norm: "</span> &lt;&lt; parameters_norm &lt;&lt; <span class="stringliteral">"\n"</span>
<a name="l01354"></a>01354                       &lt;&lt; <span class="stringliteral">"Performance: "</span> &lt;&lt; performance &lt;&lt; <span class="stringliteral">"\n"</span>
<a name="l01355"></a>01355                       &lt;&lt; <span class="stringliteral">"Gradient norm: "</span> &lt;&lt; gradient_norm &lt;&lt; <span class="stringliteral">"\n"</span>
<a name="l01356"></a>01356                                   &lt;&lt; <a class="code" href="class_open_n_n_1_1_training_algorithm.html#90ba564d00e69786d58ed94672e6e8f4" title="Pointer to a performance functional for a multilayer perceptron object.">performance_functional_pointer</a>-&gt;<a class="code" href="class_open_n_n_1_1_performance_functional.html#4e6c610158bf2e4bcce7b3e7d33fbaf4">write_information</a>() 
<a name="l01357"></a>01357                       &lt;&lt; <span class="stringliteral">"Training rate: "</span> &lt;&lt; training_rate &lt;&lt; <span class="stringliteral">"\n"</span>
<a name="l01358"></a>01358                       &lt;&lt; <span class="stringliteral">"Elapsed time: "</span> &lt;&lt; elapsed_time &lt;&lt; std::endl; 
<a name="l01359"></a>01359 
<a name="l01360"></a>01360             <span class="keywordflow">if</span>(generalization_evaluation != 0)
<a name="l01361"></a>01361             {
<a name="l01362"></a>01362                std::cout &lt;&lt; <span class="stringliteral">"Validation performance: "</span> &lt;&lt; generalization_evaluation &lt;&lt; std::endl;
<a name="l01363"></a>01363             }
<a name="l01364"></a>01364 
<a name="l01365"></a>01365                  }
<a name="l01366"></a>01366     
<a name="l01367"></a>01367          training_results_pointer-&gt;<a class="code" href="struct_open_n_n_1_1_gradient_descent_1_1_gradient_descent_results.html#899325ad55f53de29083af88a0f6e6a3">resize_training_history</a>(1+epoch);
<a name="l01368"></a>01368 
<a name="l01369"></a>01369          <span class="keywordflow">break</span>;
<a name="l01370"></a>01370       }
<a name="l01371"></a>01371       <span class="keywordflow">else</span> <span class="keywordflow">if</span>(<a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a> &amp;&amp; epoch % display_period == 0)
<a name="l01372"></a>01372       {
<a name="l01373"></a>01373          std::cout &lt;&lt; <span class="stringliteral">"Epoch "</span> &lt;&lt; epoch &lt;&lt; <span class="stringliteral">";\n"</span>
<a name="l01374"></a>01374                    &lt;&lt; <span class="stringliteral">"Parameters norm: "</span> &lt;&lt; parameters_norm &lt;&lt; <span class="stringliteral">"\n"</span>
<a name="l01375"></a>01375                    &lt;&lt; <span class="stringliteral">"Performance: "</span> &lt;&lt; performance &lt;&lt; <span class="stringliteral">"\n"</span>
<a name="l01376"></a>01376                    &lt;&lt; <span class="stringliteral">"Gradient norm: "</span> &lt;&lt; gradient_norm &lt;&lt; <span class="stringliteral">"\n"</span>
<a name="l01377"></a>01377                    &lt;&lt; <a class="code" href="class_open_n_n_1_1_training_algorithm.html#90ba564d00e69786d58ed94672e6e8f4" title="Pointer to a performance functional for a multilayer perceptron object.">performance_functional_pointer</a>-&gt;<a class="code" href="class_open_n_n_1_1_performance_functional.html#4e6c610158bf2e4bcce7b3e7d33fbaf4">write_information</a>()
<a name="l01378"></a>01378                    &lt;&lt; <span class="stringliteral">"Training rate: "</span> &lt;&lt; training_rate &lt;&lt; <span class="stringliteral">"\n"</span>
<a name="l01379"></a>01379                    &lt;&lt; <span class="stringliteral">"Elapsed time: "</span> &lt;&lt; elapsed_time &lt;&lt; std::endl;
<a name="l01380"></a>01380 
<a name="l01381"></a>01381          <span class="keywordflow">if</span>(generalization_evaluation != 0)
<a name="l01382"></a>01382          {
<a name="l01383"></a>01383             std::cout &lt;&lt; <span class="stringliteral">"Validation performance: "</span> &lt;&lt; generalization_evaluation &lt;&lt; std::endl;
<a name="l01384"></a>01384          }
<a name="l01385"></a>01385 
<a name="l01386"></a>01386       }
<a name="l01387"></a>01387 
<a name="l01388"></a>01388       <span class="comment">// Set new parameters</span>
<a name="l01389"></a>01389 
<a name="l01390"></a>01390       parameters += parameters_increment;
<a name="l01391"></a>01391 
<a name="l01392"></a>01392       neural_network_pointer-&gt;<a class="code" href="class_open_n_n_1_1_neural_network.html#3928d67ced4452ace75c38869d039a98">set_parameters</a>(parameters);
<a name="l01393"></a>01393 
<a name="l01394"></a>01394       <span class="comment">// Update stuff</span>
<a name="l01395"></a>01395 
<a name="l01396"></a>01396       old_performance = performance;
<a name="l01397"></a>01397       old_generalization_evaluation = generalization_evaluation;
<a name="l01398"></a>01398    
<a name="l01399"></a>01399       old_training_rate = training_rate;
<a name="l01400"></a>01400    } 
<a name="l01401"></a>01401 
<a name="l01402"></a>01402    <span class="keywordflow">return</span>(training_results_pointer);
<a name="l01403"></a>01403 }
<a name="l01404"></a>01404 
<a name="l01405"></a>01405 
<a name="l01406"></a>01406 <span class="comment">// std::string write_training_algorithm_type(void) const method</span>
<a name="l01407"></a>01407 
<a name="l01408"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#9feb1b4939513b37febe25ed0adbba2f">01408</a> std::string <a class="code" href="class_open_n_n_1_1_gradient_descent.html#9feb1b4939513b37febe25ed0adbba2f" title="This method writes a string with the type of training algoritm.">GradientDescent::write_training_algorithm_type</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l01409"></a>01409 <span class="keyword"></span>{
<a name="l01410"></a>01410    <span class="keywordflow">return</span>(<span class="stringliteral">"GRADIENT_DESCENT"</span>);
<a name="l01411"></a>01411 }
<a name="l01412"></a>01412 
<a name="l01413"></a>01413 
<a name="l01414"></a>01414 <span class="comment">// TiXmlElement* to_XML(void) const method</span>
<a name="l01415"></a>01415 
<a name="l01418"></a>01418 
<a name="l01419"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#2d790ddb6884db0ec6d0a7fe7ebd6768">01419</a> TiXmlElement* <a class="code" href="class_open_n_n_1_1_gradient_descent.html#2d790ddb6884db0ec6d0a7fe7ebd6768">GradientDescent::to_XML</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l01420"></a>01420 <span class="keyword"></span>{
<a name="l01421"></a>01421    std::ostringstream buffer;
<a name="l01422"></a>01422 
<a name="l01423"></a>01423    <span class="comment">// Training algorithm</span>
<a name="l01424"></a>01424 
<a name="l01425"></a>01425    TiXmlElement* gradient_descent_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"GradientDescent"</span>);
<a name="l01426"></a>01426    gradient_descent_element-&gt;SetAttribute(<span class="stringliteral">"Version"</span>, 4); 
<a name="l01427"></a>01427 
<a name="l01428"></a>01428 
<a name="l01429"></a>01429    <span class="comment">// Warning parameters norm</span>
<a name="l01430"></a>01430 
<a name="l01431"></a>01431    TiXmlElement* warning_parameters_norm_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"WarningParametersNorm"</span>);
<a name="l01432"></a>01432    gradient_descent_element-&gt;LinkEndChild(warning_parameters_norm_element);
<a name="l01433"></a>01433 
<a name="l01434"></a>01434    buffer.str(<span class="stringliteral">""</span>);
<a name="l01435"></a>01435    buffer &lt;&lt; warning_parameters_norm;
<a name="l01436"></a>01436 
<a name="l01437"></a>01437    TiXmlText* warning_parameters_norm_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01438"></a>01438    warning_parameters_norm_element-&gt;LinkEndChild(warning_parameters_norm_text);
<a name="l01439"></a>01439 
<a name="l01440"></a>01440    <span class="comment">// Warning gradient norm </span>
<a name="l01441"></a>01441 
<a name="l01442"></a>01442    TiXmlElement* warning_gradient_norm_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"WarningGradientNorm"</span>);
<a name="l01443"></a>01443    gradient_descent_element-&gt;LinkEndChild(warning_gradient_norm_element);
<a name="l01444"></a>01444 
<a name="l01445"></a>01445    buffer.str(<span class="stringliteral">""</span>);
<a name="l01446"></a>01446    buffer &lt;&lt; warning_gradient_norm;
<a name="l01447"></a>01447 
<a name="l01448"></a>01448    TiXmlText* warning_gradient_norm_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01449"></a>01449    warning_gradient_norm_element-&gt;LinkEndChild(warning_gradient_norm_text);
<a name="l01450"></a>01450 
<a name="l01451"></a>01451    <span class="comment">// Warning training rate </span>
<a name="l01452"></a>01452 
<a name="l01453"></a>01453    TiXmlElement* warning_training_rate_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"WarningTrainingRate"</span>);
<a name="l01454"></a>01454    gradient_descent_element-&gt;LinkEndChild(warning_training_rate_element);
<a name="l01455"></a>01455 
<a name="l01456"></a>01456    buffer.str(<span class="stringliteral">""</span>);
<a name="l01457"></a>01457    buffer &lt;&lt; warning_training_rate;
<a name="l01458"></a>01458 
<a name="l01459"></a>01459    TiXmlText* warning_training_rate_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01460"></a>01460    warning_training_rate_element-&gt;LinkEndChild(warning_training_rate_text);
<a name="l01461"></a>01461 
<a name="l01462"></a>01462    <span class="comment">// Error parameters norm</span>
<a name="l01463"></a>01463 
<a name="l01464"></a>01464    TiXmlElement* error_parameters_norm_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ErrorParametersNorm"</span>);
<a name="l01465"></a>01465    gradient_descent_element-&gt;LinkEndChild(error_parameters_norm_element);
<a name="l01466"></a>01466 
<a name="l01467"></a>01467    buffer.str(<span class="stringliteral">""</span>);
<a name="l01468"></a>01468    buffer &lt;&lt; error_parameters_norm;
<a name="l01469"></a>01469 
<a name="l01470"></a>01470    TiXmlText* error_parameters_norm_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01471"></a>01471    error_parameters_norm_element-&gt;LinkEndChild(error_parameters_norm_text);
<a name="l01472"></a>01472 
<a name="l01473"></a>01473    <span class="comment">// Error gradient norm </span>
<a name="l01474"></a>01474 
<a name="l01475"></a>01475    TiXmlElement* error_gradient_norm_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ErrorGradientNorm"</span>);
<a name="l01476"></a>01476    gradient_descent_element-&gt;LinkEndChild(error_gradient_norm_element);
<a name="l01477"></a>01477 
<a name="l01478"></a>01478    buffer.str(<span class="stringliteral">""</span>);
<a name="l01479"></a>01479    buffer &lt;&lt; error_gradient_norm;
<a name="l01480"></a>01480 
<a name="l01481"></a>01481    TiXmlText* error_gradient_norm_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01482"></a>01482    error_gradient_norm_element-&gt;LinkEndChild(error_gradient_norm_text);
<a name="l01483"></a>01483 
<a name="l01484"></a>01484    <span class="comment">// Error training rate</span>
<a name="l01485"></a>01485 
<a name="l01486"></a>01486    TiXmlElement* error_training_rate_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ErrorTrainingRate"</span>);
<a name="l01487"></a>01487    gradient_descent_element-&gt;LinkEndChild(error_training_rate_element);
<a name="l01488"></a>01488 
<a name="l01489"></a>01489    buffer.str(<span class="stringliteral">""</span>);
<a name="l01490"></a>01490    buffer &lt;&lt; error_training_rate;
<a name="l01491"></a>01491 
<a name="l01492"></a>01492    TiXmlText* error_training_rate_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01493"></a>01493    error_training_rate_element-&gt;LinkEndChild(error_training_rate_text);
<a name="l01494"></a>01494 
<a name="l01495"></a>01495    <span class="comment">// Minimum parameters increment norm</span>
<a name="l01496"></a>01496 
<a name="l01497"></a>01497    TiXmlElement* minimum_parameters_increment_norm_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"MinimumParametersIncrement"</span>);
<a name="l01498"></a>01498    gradient_descent_element-&gt;LinkEndChild(minimum_parameters_increment_norm_element);
<a name="l01499"></a>01499 
<a name="l01500"></a>01500    buffer.str(<span class="stringliteral">""</span>);
<a name="l01501"></a>01501    buffer &lt;&lt; minimum_parameters_increment_norm;
<a name="l01502"></a>01502 
<a name="l01503"></a>01503    TiXmlText* minimum_parameters_increment_norm_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01504"></a>01504    minimum_parameters_increment_norm_element-&gt;LinkEndChild(minimum_parameters_increment_norm_text);
<a name="l01505"></a>01505 
<a name="l01506"></a>01506    <span class="comment">// Minimum performance increase </span>
<a name="l01507"></a>01507 
<a name="l01508"></a>01508    TiXmlElement* minimum_performance_increase_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"MinimumPerformanceIncrease"</span>);
<a name="l01509"></a>01509    gradient_descent_element-&gt;LinkEndChild(minimum_performance_increase_element);
<a name="l01510"></a>01510 
<a name="l01511"></a>01511    buffer.str(<span class="stringliteral">""</span>);
<a name="l01512"></a>01512    buffer &lt;&lt; minimum_performance_increase;
<a name="l01513"></a>01513 
<a name="l01514"></a>01514    TiXmlText* minimum_performance_increase_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01515"></a>01515    minimum_performance_increase_element-&gt;LinkEndChild(minimum_performance_increase_text);
<a name="l01516"></a>01516 
<a name="l01517"></a>01517    <span class="comment">// Performance goal </span>
<a name="l01518"></a>01518 
<a name="l01519"></a>01519    TiXmlElement* performance_goal_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"PerformanceGoal"</span>);
<a name="l01520"></a>01520    gradient_descent_element-&gt;LinkEndChild(performance_goal_element);
<a name="l01521"></a>01521 
<a name="l01522"></a>01522    buffer.str(<span class="stringliteral">""</span>);
<a name="l01523"></a>01523    buffer &lt;&lt; performance_goal;
<a name="l01524"></a>01524 
<a name="l01525"></a>01525    TiXmlText* performance_goal_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01526"></a>01526    performance_goal_element-&gt;LinkEndChild(performance_goal_text);
<a name="l01527"></a>01527 
<a name="l01528"></a>01528    <span class="comment">// Gradient norm goal </span>
<a name="l01529"></a>01529 
<a name="l01530"></a>01530    TiXmlElement* gradient_norm_goal_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"GradientNormGoal"</span>);
<a name="l01531"></a>01531    gradient_descent_element-&gt;LinkEndChild(gradient_norm_goal_element);
<a name="l01532"></a>01532 
<a name="l01533"></a>01533    buffer.str(<span class="stringliteral">""</span>);
<a name="l01534"></a>01534    buffer &lt;&lt; gradient_norm_goal;
<a name="l01535"></a>01535 
<a name="l01536"></a>01536    TiXmlText* gradient_norm_goal_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01537"></a>01537    gradient_norm_goal_element-&gt;LinkEndChild(gradient_norm_goal_text);
<a name="l01538"></a>01538 
<a name="l01539"></a>01539    <span class="comment">// Maximum generalization performance decreases</span>
<a name="l01540"></a>01540 
<a name="l01541"></a>01541    TiXmlElement* maximum_generalization_evaluation_decreases_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"MaximumGeneralizationEvaluationDecreases"</span>);
<a name="l01542"></a>01542    gradient_descent_element-&gt;LinkEndChild(maximum_generalization_evaluation_decreases_element);
<a name="l01543"></a>01543 
<a name="l01544"></a>01544    buffer.str(<span class="stringliteral">""</span>);
<a name="l01545"></a>01545    buffer &lt;&lt; maximum_generalization_evaluation_decreases;
<a name="l01546"></a>01546 
<a name="l01547"></a>01547    TiXmlText* maximum_generalization_evaluation_decreases_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01548"></a>01548    maximum_generalization_evaluation_decreases_element-&gt;LinkEndChild(maximum_generalization_evaluation_decreases_text);
<a name="l01549"></a>01549 
<a name="l01550"></a>01550    <span class="comment">// Maximum epochs number </span>
<a name="l01551"></a>01551 
<a name="l01552"></a>01552    TiXmlElement* maximum_epochs_number_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"MaximumEpochsNumber"</span>);
<a name="l01553"></a>01553    gradient_descent_element-&gt;LinkEndChild(maximum_epochs_number_element);
<a name="l01554"></a>01554 
<a name="l01555"></a>01555    buffer.str(<span class="stringliteral">""</span>);
<a name="l01556"></a>01556    buffer &lt;&lt; maximum_epochs_number;
<a name="l01557"></a>01557 
<a name="l01558"></a>01558    TiXmlText* maximum_epochs_number_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01559"></a>01559    maximum_epochs_number_element-&gt;LinkEndChild(maximum_epochs_number_text);
<a name="l01560"></a>01560 
<a name="l01561"></a>01561    <span class="comment">// Maximum time </span>
<a name="l01562"></a>01562 
<a name="l01563"></a>01563    TiXmlElement* maximum_time_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"MaximumTime"</span>);
<a name="l01564"></a>01564    gradient_descent_element-&gt;LinkEndChild(maximum_time_element);
<a name="l01565"></a>01565 
<a name="l01566"></a>01566    buffer.str(<span class="stringliteral">""</span>);
<a name="l01567"></a>01567    buffer &lt;&lt; maximum_time;
<a name="l01568"></a>01568 
<a name="l01569"></a>01569    TiXmlText* maximum_time_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01570"></a>01570    maximum_time_element-&gt;LinkEndChild(maximum_time_text);
<a name="l01571"></a>01571 
<a name="l01572"></a>01572    <span class="comment">// Reserve parameters history </span>
<a name="l01573"></a>01573 
<a name="l01574"></a>01574    TiXmlElement* reserve_parameters_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveParametersHistory"</span>);
<a name="l01575"></a>01575    gradient_descent_element-&gt;LinkEndChild(reserve_parameters_history_element);
<a name="l01576"></a>01576 
<a name="l01577"></a>01577    buffer.str(<span class="stringliteral">""</span>);
<a name="l01578"></a>01578    buffer &lt;&lt; reserve_parameters_history;
<a name="l01579"></a>01579 
<a name="l01580"></a>01580    TiXmlText* reserve_parameters_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01581"></a>01581    reserve_parameters_history_element-&gt;LinkEndChild(reserve_parameters_history_text);
<a name="l01582"></a>01582 
<a name="l01583"></a>01583    <span class="comment">// Reserve parameters norm history </span>
<a name="l01584"></a>01584 
<a name="l01585"></a>01585    TiXmlElement* reserve_parameters_norm_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveParametersNormHistory"</span>);
<a name="l01586"></a>01586    gradient_descent_element-&gt;LinkEndChild(reserve_parameters_norm_history_element);
<a name="l01587"></a>01587 
<a name="l01588"></a>01588    buffer.str(<span class="stringliteral">""</span>);
<a name="l01589"></a>01589    buffer &lt;&lt; reserve_parameters_norm_history;
<a name="l01590"></a>01590 
<a name="l01591"></a>01591    TiXmlText* reserve_parameters_norm_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01592"></a>01592    reserve_parameters_norm_history_element-&gt;LinkEndChild(reserve_parameters_norm_history_text);
<a name="l01593"></a>01593 
<a name="l01594"></a>01594    <span class="comment">// Reserve evaluation history </span>
<a name="l01595"></a>01595 
<a name="l01596"></a>01596    TiXmlElement* reserve_evaluation_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReservePerformanceHistory"</span>);
<a name="l01597"></a>01597    gradient_descent_element-&gt;LinkEndChild(reserve_evaluation_history_element);
<a name="l01598"></a>01598 
<a name="l01599"></a>01599    buffer.str(<span class="stringliteral">""</span>);
<a name="l01600"></a>01600    buffer &lt;&lt; reserve_evaluation_history;
<a name="l01601"></a>01601 
<a name="l01602"></a>01602    TiXmlText* reserve_evaluation_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01603"></a>01603    reserve_evaluation_history_element-&gt;LinkEndChild(reserve_evaluation_history_text);
<a name="l01604"></a>01604 
<a name="l01605"></a>01605    <span class="comment">// Reserve gradient history </span>
<a name="l01606"></a>01606 
<a name="l01607"></a>01607    TiXmlElement* reserve_gradient_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveGradientHistory"</span>);
<a name="l01608"></a>01608    gradient_descent_element-&gt;LinkEndChild(reserve_gradient_history_element);
<a name="l01609"></a>01609 
<a name="l01610"></a>01610    buffer.str(<span class="stringliteral">""</span>);
<a name="l01611"></a>01611    buffer &lt;&lt; reserve_gradient_history;
<a name="l01612"></a>01612 
<a name="l01613"></a>01613    TiXmlText* reserve_gradient_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01614"></a>01614    reserve_gradient_history_element-&gt;LinkEndChild(reserve_gradient_history_text);
<a name="l01615"></a>01615 
<a name="l01616"></a>01616    <span class="comment">// Reserve gradient norm history </span>
<a name="l01617"></a>01617 
<a name="l01618"></a>01618    TiXmlElement* reserve_gradient_norm_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveGradientNormHistory"</span>);
<a name="l01619"></a>01619    gradient_descent_element-&gt;LinkEndChild(reserve_gradient_norm_history_element);
<a name="l01620"></a>01620 
<a name="l01621"></a>01621    buffer.str(<span class="stringliteral">""</span>);
<a name="l01622"></a>01622    buffer &lt;&lt; reserve_gradient_norm_history;
<a name="l01623"></a>01623 
<a name="l01624"></a>01624    TiXmlText* reserve_gradient_norm_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01625"></a>01625    reserve_gradient_norm_history_element-&gt;LinkEndChild(reserve_gradient_norm_history_text);
<a name="l01626"></a>01626 
<a name="l01627"></a>01627    <span class="comment">// Reserve training direction history </span>
<a name="l01628"></a>01628 
<a name="l01629"></a>01629    TiXmlElement* reserve_training_direction_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveTrainingDirectionHistory"</span>);
<a name="l01630"></a>01630    gradient_descent_element-&gt;LinkEndChild(reserve_training_direction_history_element);
<a name="l01631"></a>01631 
<a name="l01632"></a>01632    buffer.str(<span class="stringliteral">""</span>);
<a name="l01633"></a>01633    buffer &lt;&lt; reserve_training_direction_history;
<a name="l01634"></a>01634 
<a name="l01635"></a>01635    TiXmlText* reserve_training_direction_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01636"></a>01636    reserve_training_direction_history_element-&gt;LinkEndChild(reserve_training_direction_history_text);
<a name="l01637"></a>01637 
<a name="l01638"></a>01638    <span class="comment">// Reserve training rate history </span>
<a name="l01639"></a>01639 
<a name="l01640"></a>01640    TiXmlElement* reserve_training_rate_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveTrainingRateHistory"</span>);
<a name="l01641"></a>01641    gradient_descent_element-&gt;LinkEndChild(reserve_training_rate_history_element);
<a name="l01642"></a>01642 
<a name="l01643"></a>01643    buffer.str(<span class="stringliteral">""</span>);
<a name="l01644"></a>01644    buffer &lt;&lt; reserve_training_rate_history;
<a name="l01645"></a>01645 
<a name="l01646"></a>01646    TiXmlText* reserve_training_rate_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01647"></a>01647    reserve_training_rate_history_element-&gt;LinkEndChild(reserve_training_rate_history_text);
<a name="l01648"></a>01648 
<a name="l01649"></a>01649    <span class="comment">// Reserve elapsed time history </span>
<a name="l01650"></a>01650 
<a name="l01651"></a>01651    TiXmlElement* reserve_elapsed_time_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveElapsedTimeHistory"</span>);
<a name="l01652"></a>01652    gradient_descent_element-&gt;LinkEndChild(reserve_elapsed_time_history_element);
<a name="l01653"></a>01653 
<a name="l01654"></a>01654    buffer.str(<span class="stringliteral">""</span>);
<a name="l01655"></a>01655    buffer &lt;&lt; reserve_elapsed_time_history;
<a name="l01656"></a>01656 
<a name="l01657"></a>01657    TiXmlText* reserve_elapsed_time_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01658"></a>01658    reserve_elapsed_time_history_element-&gt;LinkEndChild(reserve_elapsed_time_history_text);
<a name="l01659"></a>01659 
<a name="l01660"></a>01660    <span class="comment">// Reserve generalization evaluation history </span>
<a name="l01661"></a>01661 
<a name="l01662"></a>01662    TiXmlElement* reserve_generalization_evaluation_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveGeneralizationPerformanceHistory"</span>);
<a name="l01663"></a>01663    gradient_descent_element-&gt;LinkEndChild(reserve_generalization_evaluation_history_element);
<a name="l01664"></a>01664 
<a name="l01665"></a>01665    buffer.str(<span class="stringliteral">""</span>);
<a name="l01666"></a>01666    buffer &lt;&lt; reserve_generalization_evaluation_history;
<a name="l01667"></a>01667 
<a name="l01668"></a>01668    TiXmlText* reserve_generalization_evaluation_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01669"></a>01669    reserve_generalization_evaluation_history_element-&gt;LinkEndChild(reserve_generalization_evaluation_history_text);
<a name="l01670"></a>01670 
<a name="l01671"></a>01671    <span class="comment">// Display period</span>
<a name="l01672"></a>01672 
<a name="l01673"></a>01673    TiXmlElement* display_period_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"DisplayPeriod"</span>);
<a name="l01674"></a>01674    gradient_descent_element-&gt;LinkEndChild(display_period_element);
<a name="l01675"></a>01675 
<a name="l01676"></a>01676    buffer.str(<span class="stringliteral">""</span>);
<a name="l01677"></a>01677    buffer &lt;&lt; display_period;
<a name="l01678"></a>01678 
<a name="l01679"></a>01679    TiXmlText* display_period_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01680"></a>01680    display_period_element-&gt;LinkEndChild(display_period_text);
<a name="l01681"></a>01681 
<a name="l01682"></a>01682    <span class="comment">// Display warnings </span>
<a name="l01683"></a>01683 
<a name="l01684"></a>01684    TiXmlElement* display_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"Display"</span>);
<a name="l01685"></a>01685    gradient_descent_element-&gt;LinkEndChild(display_element);
<a name="l01686"></a>01686 
<a name="l01687"></a>01687    buffer.str(<span class="stringliteral">""</span>);
<a name="l01688"></a>01688    buffer &lt;&lt; <a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a>;
<a name="l01689"></a>01689 
<a name="l01690"></a>01690    TiXmlText* display_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01691"></a>01691    display_element-&gt;LinkEndChild(display_text);
<a name="l01692"></a>01692 
<a name="l01693"></a>01693    <span class="keywordflow">return</span>(gradient_descent_element);
<a name="l01694"></a>01694 }
<a name="l01695"></a>01695 
<a name="l01696"></a>01696 
<a name="l01697"></a>01697 <span class="comment">// void from_XML(TiXmlElement*) method</span>
<a name="l01698"></a>01698 
<a name="l01699"></a><a class="code" href="class_open_n_n_1_1_gradient_descent.html#ee5b6a70c80e41df05f583193ec52045">01699</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_gradient_descent.html#ee5b6a70c80e41df05f583193ec52045">GradientDescent::from_XML</a>(TiXmlElement* gradient_descent_element)
<a name="l01700"></a>01700 {
<a name="l01701"></a>01701    <span class="comment">// Training rate algorithm</span>
<a name="l01702"></a>01702 
<a name="l01703"></a>01703    TiXmlElement* training_rate_algorithm_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"TrainingRateAlgorithm"</span>);
<a name="l01704"></a>01704    
<a name="l01705"></a>01705    <span class="keywordflow">if</span>(training_rate_algorithm_element)
<a name="l01706"></a>01706    {
<a name="l01707"></a>01707       <span class="keywordflow">try</span>
<a name="l01708"></a>01708       {
<a name="l01709"></a>01709          training_rate_algorithm.<a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html#368026ed823890e2267c985d611112ce">from_XML</a>(training_rate_algorithm_element);
<a name="l01710"></a>01710       }
<a name="l01711"></a>01711       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01712"></a>01712       {
<a name="l01713"></a>01713          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01714"></a>01714       }
<a name="l01715"></a>01715    }
<a name="l01716"></a>01716 
<a name="l01717"></a>01717    <span class="comment">// Warning parameters norm</span>
<a name="l01718"></a>01718 
<a name="l01719"></a>01719    TiXmlElement* warning_parameters_norm_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"WarningParametersNorm"</span>);
<a name="l01720"></a>01720 
<a name="l01721"></a>01721    <span class="keywordflow">if</span>(warning_parameters_norm_element)
<a name="l01722"></a>01722    {
<a name="l01723"></a>01723       <span class="keywordtype">double</span> new_warning_parameters_norm = atof(warning_parameters_norm_element-&gt;GetText()); 
<a name="l01724"></a>01724 
<a name="l01725"></a>01725       <span class="keywordflow">try</span>
<a name="l01726"></a>01726       {
<a name="l01727"></a>01727          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#c525a49652ff909ec76613b721e33739">set_warning_parameters_norm</a>(new_warning_parameters_norm);
<a name="l01728"></a>01728       }
<a name="l01729"></a>01729       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01730"></a>01730       {
<a name="l01731"></a>01731          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01732"></a>01732       }
<a name="l01733"></a>01733    }
<a name="l01734"></a>01734 
<a name="l01735"></a>01735    <span class="comment">// Warning gradient norm </span>
<a name="l01736"></a>01736 
<a name="l01737"></a>01737    TiXmlElement* warning_gradient_norm_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"WarningGradientNorm"</span>);
<a name="l01738"></a>01738 
<a name="l01739"></a>01739    <span class="keywordflow">if</span>(warning_gradient_norm_element)
<a name="l01740"></a>01740    {
<a name="l01741"></a>01741       <span class="keywordtype">double</span> new_warning_gradient_norm = atof(warning_gradient_norm_element-&gt;GetText()); 
<a name="l01742"></a>01742 
<a name="l01743"></a>01743       <span class="keywordflow">try</span>
<a name="l01744"></a>01744       {
<a name="l01745"></a>01745          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#c7f1c7a84ab5067aa2550b3e36537c21">set_warning_gradient_norm</a>(new_warning_gradient_norm);
<a name="l01746"></a>01746       }
<a name="l01747"></a>01747       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01748"></a>01748       {
<a name="l01749"></a>01749          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01750"></a>01750       }
<a name="l01751"></a>01751    }
<a name="l01752"></a>01752 
<a name="l01753"></a>01753    <span class="comment">// Warning training rate </span>
<a name="l01754"></a>01754 
<a name="l01755"></a>01755    TiXmlElement* warning_training_rate_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"WarningTrainingRate"</span>);
<a name="l01756"></a>01756 
<a name="l01757"></a>01757    <span class="keywordflow">if</span>(warning_training_rate_element)
<a name="l01758"></a>01758    {
<a name="l01759"></a>01759       <span class="keywordtype">double</span> new_warning_training_rate = atof(warning_training_rate_element-&gt;GetText()); 
<a name="l01760"></a>01760 
<a name="l01761"></a>01761       <span class="keywordflow">try</span>
<a name="l01762"></a>01762       {
<a name="l01763"></a>01763          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#9359ff2faa9eab75167a5867381a6b34">set_warning_training_rate</a>(new_warning_training_rate);
<a name="l01764"></a>01764       }
<a name="l01765"></a>01765       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01766"></a>01766       {
<a name="l01767"></a>01767          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01768"></a>01768       }
<a name="l01769"></a>01769    }
<a name="l01770"></a>01770 
<a name="l01771"></a>01771    <span class="comment">// Error parameters norm</span>
<a name="l01772"></a>01772 
<a name="l01773"></a>01773    TiXmlElement* error_parameters_norm_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ErrorParametersNorm"</span>);
<a name="l01774"></a>01774 
<a name="l01775"></a>01775    <span class="keywordflow">if</span>(error_parameters_norm_element)
<a name="l01776"></a>01776    {
<a name="l01777"></a>01777       <span class="keywordtype">double</span> new_error_parameters_norm = atof(error_parameters_norm_element-&gt;GetText()); 
<a name="l01778"></a>01778 
<a name="l01779"></a>01779       <span class="keywordflow">try</span>
<a name="l01780"></a>01780       {
<a name="l01781"></a>01781           <a class="code" href="class_open_n_n_1_1_gradient_descent.html#2625e879f726b1501ce33ab5710bbb88">set_error_parameters_norm</a>(new_error_parameters_norm);
<a name="l01782"></a>01782       }
<a name="l01783"></a>01783       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01784"></a>01784       {
<a name="l01785"></a>01785          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01786"></a>01786       }
<a name="l01787"></a>01787    }
<a name="l01788"></a>01788 
<a name="l01789"></a>01789    <span class="comment">// Error gradient norm </span>
<a name="l01790"></a>01790 
<a name="l01791"></a>01791    TiXmlElement* error_gradient_norm_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ErrorGradientNorm"</span>);
<a name="l01792"></a>01792 
<a name="l01793"></a>01793    <span class="keywordflow">if</span>(error_gradient_norm_element)
<a name="l01794"></a>01794    {
<a name="l01795"></a>01795       <span class="keywordtype">double</span> new_error_gradient_norm = atof(error_gradient_norm_element-&gt;GetText()); 
<a name="l01796"></a>01796 
<a name="l01797"></a>01797       <span class="keywordflow">try</span>
<a name="l01798"></a>01798       {
<a name="l01799"></a>01799          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#e8d1a6991a7a735195ed65b15315d233">set_error_gradient_norm</a>(new_error_gradient_norm);
<a name="l01800"></a>01800       }
<a name="l01801"></a>01801       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01802"></a>01802       {
<a name="l01803"></a>01803          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01804"></a>01804       }
<a name="l01805"></a>01805    }
<a name="l01806"></a>01806 
<a name="l01807"></a>01807    <span class="comment">// Error training rate</span>
<a name="l01808"></a>01808 
<a name="l01809"></a>01809    TiXmlElement* error_training_rate_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ErrorTrainingRate"</span>);
<a name="l01810"></a>01810 
<a name="l01811"></a>01811    <span class="keywordflow">if</span>(error_training_rate_element)
<a name="l01812"></a>01812    {
<a name="l01813"></a>01813       <span class="keywordtype">double</span> new_error_training_rate = atof(error_training_rate_element-&gt;GetText()); 
<a name="l01814"></a>01814 
<a name="l01815"></a>01815       <span class="keywordflow">try</span>
<a name="l01816"></a>01816       {
<a name="l01817"></a>01817          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#1d3b83e0e926af808f2d6a3b2bde050c">set_error_training_rate</a>(new_error_training_rate);
<a name="l01818"></a>01818       }
<a name="l01819"></a>01819       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01820"></a>01820       {
<a name="l01821"></a>01821          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01822"></a>01822       }
<a name="l01823"></a>01823    }
<a name="l01824"></a>01824 
<a name="l01825"></a>01825    <span class="comment">// Minimum parameters increment norm</span>
<a name="l01826"></a>01826 
<a name="l01827"></a>01827    TiXmlElement* minimum_parameters_increment_norm_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"MinimumParametersIncrementNorm"</span>);
<a name="l01828"></a>01828 
<a name="l01829"></a>01829    <span class="keywordflow">if</span>(minimum_parameters_increment_norm_element)
<a name="l01830"></a>01830    {
<a name="l01831"></a>01831       <span class="keywordtype">double</span> new_minimum_parameters_increment_norm = atof(minimum_parameters_increment_norm_element-&gt;GetText()); 
<a name="l01832"></a>01832 
<a name="l01833"></a>01833       <span class="keywordflow">try</span>
<a name="l01834"></a>01834       {
<a name="l01835"></a>01835          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#6ebdcab74360e712408d2b5f734cc2d8">set_minimum_parameters_increment_norm</a>(new_minimum_parameters_increment_norm);
<a name="l01836"></a>01836       }
<a name="l01837"></a>01837       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01838"></a>01838       {
<a name="l01839"></a>01839          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01840"></a>01840       }
<a name="l01841"></a>01841    }
<a name="l01842"></a>01842 
<a name="l01843"></a>01843    <span class="comment">// Minimum performance increase </span>
<a name="l01844"></a>01844 
<a name="l01845"></a>01845    TiXmlElement* minimum_performance_increase_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"MinimumPerformanceIncrease"</span>);
<a name="l01846"></a>01846 
<a name="l01847"></a>01847    <span class="keywordflow">if</span>(minimum_performance_increase_element)
<a name="l01848"></a>01848    {
<a name="l01849"></a>01849       <span class="keywordtype">double</span> new_minimum_performance_increase = atof(minimum_performance_increase_element-&gt;GetText()); 
<a name="l01850"></a>01850 
<a name="l01851"></a>01851       <span class="keywordflow">try</span>
<a name="l01852"></a>01852       {
<a name="l01853"></a>01853          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#a1a12c07c1db1420bb59f58c3a17fea4">set_minimum_performance_increase</a>(new_minimum_performance_increase);
<a name="l01854"></a>01854       }
<a name="l01855"></a>01855       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01856"></a>01856       {
<a name="l01857"></a>01857          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01858"></a>01858       }
<a name="l01859"></a>01859    }
<a name="l01860"></a>01860 
<a name="l01861"></a>01861    <span class="comment">// Performance goal </span>
<a name="l01862"></a>01862 
<a name="l01863"></a>01863    TiXmlElement* performance_goal_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"PerformanceGoal"</span>);
<a name="l01864"></a>01864 
<a name="l01865"></a>01865    <span class="keywordflow">if</span>(performance_goal_element)
<a name="l01866"></a>01866    {
<a name="l01867"></a>01867       <span class="keywordtype">double</span> new_performance_goal = atof(performance_goal_element-&gt;GetText()); 
<a name="l01868"></a>01868 
<a name="l01869"></a>01869       <span class="keywordflow">try</span>
<a name="l01870"></a>01870       {
<a name="l01871"></a>01871          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#8e10381a376476d92e3c96a56a7cf257">set_performance_goal</a>(new_performance_goal);
<a name="l01872"></a>01872       }
<a name="l01873"></a>01873       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01874"></a>01874       {
<a name="l01875"></a>01875          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01876"></a>01876       }
<a name="l01877"></a>01877    }
<a name="l01878"></a>01878 
<a name="l01879"></a>01879    <span class="comment">// Gradient norm goal </span>
<a name="l01880"></a>01880 
<a name="l01881"></a>01881    TiXmlElement* gradient_norm_goal_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"GradientNormGoal"</span>);
<a name="l01882"></a>01882 
<a name="l01883"></a>01883    <span class="keywordflow">if</span>(gradient_norm_goal_element)
<a name="l01884"></a>01884    {
<a name="l01885"></a>01885       <span class="keywordtype">double</span> new_gradient_norm_goal = atof(gradient_norm_goal_element-&gt;GetText()); 
<a name="l01886"></a>01886 
<a name="l01887"></a>01887       <span class="keywordflow">try</span>
<a name="l01888"></a>01888       {
<a name="l01889"></a>01889          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#1ad68485a9faa5a39c134b4177551c48">set_gradient_norm_goal</a>(new_gradient_norm_goal);
<a name="l01890"></a>01890       }
<a name="l01891"></a>01891       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01892"></a>01892       {
<a name="l01893"></a>01893          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01894"></a>01894       }
<a name="l01895"></a>01895    }
<a name="l01896"></a>01896 
<a name="l01897"></a>01897    <span class="comment">// Maximum generalization performance decreases</span>
<a name="l01898"></a>01898 
<a name="l01899"></a>01899    TiXmlElement* maximum_generalization_evaluation_decreases_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"MaximumGeneralizationEvaluationDecreases"</span>);
<a name="l01900"></a>01900 
<a name="l01901"></a>01901    <span class="keywordflow">if</span>(maximum_generalization_evaluation_decreases_element)
<a name="l01902"></a>01902    {
<a name="l01903"></a>01903       <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> new_maximum_generalization_evaluation_decreases = atoi(maximum_generalization_evaluation_decreases_element-&gt;GetText()); 
<a name="l01904"></a>01904 
<a name="l01905"></a>01905       <span class="keywordflow">try</span>
<a name="l01906"></a>01906       {
<a name="l01907"></a>01907          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#de9212832bd5d3a23f751d830f153e8a">set_maximum_generalization_evaluation_decreases</a>(new_maximum_generalization_evaluation_decreases);
<a name="l01908"></a>01908       }
<a name="l01909"></a>01909       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01910"></a>01910       {
<a name="l01911"></a>01911          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01912"></a>01912       }
<a name="l01913"></a>01913    }
<a name="l01914"></a>01914 
<a name="l01915"></a>01915    <span class="comment">// Maximum epochs number </span>
<a name="l01916"></a>01916 
<a name="l01917"></a>01917    TiXmlElement* maximum_epochs_number_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"MaximumEpochsNumber"</span>);
<a name="l01918"></a>01918 
<a name="l01919"></a>01919    <span class="keywordflow">if</span>(maximum_epochs_number_element)
<a name="l01920"></a>01920    {
<a name="l01921"></a>01921        <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> new_maximum_epochs_number = atoi(maximum_epochs_number_element-&gt;GetText()); 
<a name="l01922"></a>01922 
<a name="l01923"></a>01923       <span class="keywordflow">try</span>
<a name="l01924"></a>01924       {
<a name="l01925"></a>01925          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#f66661bb709e3b68c748a4307e8bee45">set_maximum_epochs_number</a>(new_maximum_epochs_number);
<a name="l01926"></a>01926       }
<a name="l01927"></a>01927       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01928"></a>01928       {
<a name="l01929"></a>01929          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01930"></a>01930       }
<a name="l01931"></a>01931    }
<a name="l01932"></a>01932 
<a name="l01933"></a>01933    <span class="comment">// Maximum time </span>
<a name="l01934"></a>01934 
<a name="l01935"></a>01935    TiXmlElement* maximum_time_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"MaximumTime"</span>);
<a name="l01936"></a>01936 
<a name="l01937"></a>01937    <span class="keywordflow">if</span>(maximum_time_element)
<a name="l01938"></a>01938    {
<a name="l01939"></a>01939       <span class="keywordtype">double</span> new_maximum_time = atof(maximum_time_element-&gt;GetText()); 
<a name="l01940"></a>01940 
<a name="l01941"></a>01941       <span class="keywordflow">try</span>
<a name="l01942"></a>01942       {
<a name="l01943"></a>01943          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#79b6c48c3f392baeaabff840d22cb5b3">set_maximum_time</a>(new_maximum_time);
<a name="l01944"></a>01944       }
<a name="l01945"></a>01945       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01946"></a>01946       {
<a name="l01947"></a>01947          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01948"></a>01948       }
<a name="l01949"></a>01949    }
<a name="l01950"></a>01950 
<a name="l01951"></a>01951    <span class="comment">// Reserve parameters history </span>
<a name="l01952"></a>01952 
<a name="l01953"></a>01953    TiXmlElement* reserve_parameters_history_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveParametersHistory"</span>);
<a name="l01954"></a>01954 
<a name="l01955"></a>01955    <span class="keywordflow">if</span>(reserve_parameters_history_element)
<a name="l01956"></a>01956    {
<a name="l01957"></a>01957       std::string new_reserve_parameters_history = reserve_parameters_history_element-&gt;GetText(); 
<a name="l01958"></a>01958 
<a name="l01959"></a>01959       <span class="keywordflow">try</span>
<a name="l01960"></a>01960       {
<a name="l01961"></a>01961          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#3d828a36bf0df6069f04d5c7ac2eeb51">set_reserve_parameters_history</a>(new_reserve_parameters_history != <span class="stringliteral">"0"</span>);
<a name="l01962"></a>01962       }
<a name="l01963"></a>01963       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01964"></a>01964       {
<a name="l01965"></a>01965          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01966"></a>01966       }
<a name="l01967"></a>01967    }
<a name="l01968"></a>01968 
<a name="l01969"></a>01969    <span class="comment">// Reserve parameters norm history </span>
<a name="l01970"></a>01970 
<a name="l01971"></a>01971    TiXmlElement* reserve_parameters_norm_history_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveParametersNormHistory"</span>);
<a name="l01972"></a>01972 
<a name="l01973"></a>01973    <span class="keywordflow">if</span>(reserve_parameters_norm_history_element)
<a name="l01974"></a>01974    {
<a name="l01975"></a>01975       std::string new_reserve_parameters_norm_history = reserve_parameters_norm_history_element-&gt;GetText(); 
<a name="l01976"></a>01976 
<a name="l01977"></a>01977       <span class="keywordflow">try</span>
<a name="l01978"></a>01978       {
<a name="l01979"></a>01979          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#daf7e1540cc81138bd541bee36269464">set_reserve_parameters_norm_history</a>(new_reserve_parameters_norm_history != <span class="stringliteral">"0"</span>);
<a name="l01980"></a>01980       }
<a name="l01981"></a>01981       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l01982"></a>01982       {
<a name="l01983"></a>01983          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l01984"></a>01984       }
<a name="l01985"></a>01985    }
<a name="l01986"></a>01986 
<a name="l01987"></a>01987    <span class="comment">// Reserve evaluation history </span>
<a name="l01988"></a>01988 
<a name="l01989"></a>01989    TiXmlElement* reserve_evaluation_history_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ReservePerformanceHistory"</span>);
<a name="l01990"></a>01990 
<a name="l01991"></a>01991    <span class="keywordflow">if</span>(reserve_evaluation_history_element)
<a name="l01992"></a>01992    {
<a name="l01993"></a>01993       std::string new_reserve_evaluation_history = reserve_evaluation_history_element-&gt;GetText(); 
<a name="l01994"></a>01994 
<a name="l01995"></a>01995       <span class="keywordflow">try</span>
<a name="l01996"></a>01996       {
<a name="l01997"></a>01997          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#aa21d82bbe0fd3bb31295e97da9593c9">set_reserve_evaluation_history</a>(new_reserve_evaluation_history != <span class="stringliteral">"0"</span>);
<a name="l01998"></a>01998       }
<a name="l01999"></a>01999       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02000"></a>02000       {
<a name="l02001"></a>02001          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l02002"></a>02002       }
<a name="l02003"></a>02003    }
<a name="l02004"></a>02004 
<a name="l02005"></a>02005    <span class="comment">// Reserve gradient history </span>
<a name="l02006"></a>02006 
<a name="l02007"></a>02007    TiXmlElement* reserve_gradient_history_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveGradientHistory"</span>);
<a name="l02008"></a>02008 
<a name="l02009"></a>02009    <span class="keywordflow">if</span>(reserve_gradient_history_element)
<a name="l02010"></a>02010    {
<a name="l02011"></a>02011       std::string new_reserve_gradient_history = reserve_gradient_history_element-&gt;GetText(); 
<a name="l02012"></a>02012 
<a name="l02013"></a>02013       <span class="keywordflow">try</span>
<a name="l02014"></a>02014       {
<a name="l02015"></a>02015          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#8ec7e29c6611ca1d0e3d03d543d62ce7">set_reserve_gradient_history</a>(new_reserve_gradient_history != <span class="stringliteral">"0"</span>);
<a name="l02016"></a>02016       }
<a name="l02017"></a>02017       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02018"></a>02018       {
<a name="l02019"></a>02019          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l02020"></a>02020       }
<a name="l02021"></a>02021    }
<a name="l02022"></a>02022 
<a name="l02023"></a>02023    <span class="comment">// Reserve gradient norm history </span>
<a name="l02024"></a>02024 
<a name="l02025"></a>02025    TiXmlElement* reserve_gradient_norm_history_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveGradientNormHistory"</span>);
<a name="l02026"></a>02026 
<a name="l02027"></a>02027    <span class="keywordflow">if</span>(reserve_gradient_norm_history_element)
<a name="l02028"></a>02028    {
<a name="l02029"></a>02029       std::string new_reserve_gradient_norm_history = reserve_gradient_norm_history_element-&gt;GetText(); 
<a name="l02030"></a>02030 
<a name="l02031"></a>02031       <span class="keywordflow">try</span>
<a name="l02032"></a>02032       {
<a name="l02033"></a>02033          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#c35bd72977a353a1edbc01178c1965e1">set_reserve_gradient_norm_history</a>(new_reserve_gradient_norm_history != <span class="stringliteral">"0"</span>);
<a name="l02034"></a>02034       }
<a name="l02035"></a>02035       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02036"></a>02036       {
<a name="l02037"></a>02037          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l02038"></a>02038       }
<a name="l02039"></a>02039    }
<a name="l02040"></a>02040 
<a name="l02041"></a>02041    <span class="comment">// Reserve training direction history </span>
<a name="l02042"></a>02042 
<a name="l02043"></a>02043    TiXmlElement* reserve_training_direction_history_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveTrainingDirectionHistory"</span>);
<a name="l02044"></a>02044 
<a name="l02045"></a>02045    <span class="keywordflow">if</span>(reserve_training_direction_history_element)
<a name="l02046"></a>02046    {
<a name="l02047"></a>02047       std::string new_reserve_training_direction_history = reserve_training_direction_history_element-&gt;GetText(); 
<a name="l02048"></a>02048 
<a name="l02049"></a>02049       <span class="keywordflow">try</span>
<a name="l02050"></a>02050       {
<a name="l02051"></a>02051          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#9c2553d655440601e779a3d093de593c">set_reserve_training_direction_history</a>(new_reserve_training_direction_history != <span class="stringliteral">"0"</span>);
<a name="l02052"></a>02052       }
<a name="l02053"></a>02053       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02054"></a>02054       {
<a name="l02055"></a>02055          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l02056"></a>02056       }
<a name="l02057"></a>02057    }
<a name="l02058"></a>02058 
<a name="l02059"></a>02059    <span class="comment">// Reserve training rate history </span>
<a name="l02060"></a>02060 
<a name="l02061"></a>02061    TiXmlElement* reserve_training_rate_history_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveTrainingRateHistory"</span>);
<a name="l02062"></a>02062 
<a name="l02063"></a>02063    <span class="keywordflow">if</span>(reserve_training_rate_history_element)
<a name="l02064"></a>02064    {
<a name="l02065"></a>02065       std::string new_reserve_training_rate_history = reserve_training_rate_history_element-&gt;GetText(); 
<a name="l02066"></a>02066 
<a name="l02067"></a>02067       <span class="keywordflow">try</span>
<a name="l02068"></a>02068       {
<a name="l02069"></a>02069          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#7adc9d5149d9599aa3df9d254974d503">set_reserve_training_rate_history</a>(new_reserve_training_rate_history != <span class="stringliteral">"0"</span>);
<a name="l02070"></a>02070       }
<a name="l02071"></a>02071       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02072"></a>02072       {
<a name="l02073"></a>02073          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l02074"></a>02074       }
<a name="l02075"></a>02075    }
<a name="l02076"></a>02076 
<a name="l02077"></a>02077    <span class="comment">// Reserve elapsed time history </span>
<a name="l02078"></a>02078 
<a name="l02079"></a>02079    TiXmlElement* reserve_elapsed_time_history_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveElapsedTimeHistory"</span>);
<a name="l02080"></a>02080 
<a name="l02081"></a>02081    <span class="keywordflow">if</span>(reserve_elapsed_time_history_element)
<a name="l02082"></a>02082    {
<a name="l02083"></a>02083       std::string new_reserve_elapsed_time_history = reserve_elapsed_time_history_element-&gt;GetText(); 
<a name="l02084"></a>02084 
<a name="l02085"></a>02085       <span class="keywordflow">try</span>
<a name="l02086"></a>02086       {
<a name="l02087"></a>02087          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#8e98003d637460b1a6f57cc5063b77d8">set_reserve_elapsed_time_history</a>(new_reserve_elapsed_time_history != <span class="stringliteral">"0"</span>);
<a name="l02088"></a>02088       }
<a name="l02089"></a>02089       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02090"></a>02090       {
<a name="l02091"></a>02091          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l02092"></a>02092       }
<a name="l02093"></a>02093    }
<a name="l02094"></a>02094 
<a name="l02095"></a>02095    <span class="comment">// Reserve generalization evaluation history </span>
<a name="l02096"></a>02096 
<a name="l02097"></a>02097    TiXmlElement* reserve_generalization_evaluation_history_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveGeneralizationPerformanceHistory"</span>);
<a name="l02098"></a>02098 
<a name="l02099"></a>02099    <span class="keywordflow">if</span>(reserve_generalization_evaluation_history_element)
<a name="l02100"></a>02100    {
<a name="l02101"></a>02101       std::string new_reserve_generalization_evaluation_history = reserve_generalization_evaluation_history_element-&gt;GetText(); 
<a name="l02102"></a>02102 
<a name="l02103"></a>02103       <span class="keywordflow">try</span>
<a name="l02104"></a>02104       {
<a name="l02105"></a>02105          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#05e2aedc5889583d5cfcdd3e69ea24ce">set_reserve_generalization_evaluation_history</a>(new_reserve_generalization_evaluation_history != <span class="stringliteral">"0"</span>);
<a name="l02106"></a>02106       }
<a name="l02107"></a>02107       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02108"></a>02108       {
<a name="l02109"></a>02109          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l02110"></a>02110       }
<a name="l02111"></a>02111    }
<a name="l02112"></a>02112 
<a name="l02113"></a>02113    <span class="comment">// Display period</span>
<a name="l02114"></a>02114 
<a name="l02115"></a>02115    TiXmlElement* display_period_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"DisplayPeriod"</span>);
<a name="l02116"></a>02116 
<a name="l02117"></a>02117    <span class="keywordflow">if</span>(display_period_element)
<a name="l02118"></a>02118    {
<a name="l02119"></a>02119       <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> new_display_period = atoi(display_period_element-&gt;GetText()); 
<a name="l02120"></a>02120 
<a name="l02121"></a>02121       <span class="keywordflow">try</span>
<a name="l02122"></a>02122       {
<a name="l02123"></a>02123          <a class="code" href="class_open_n_n_1_1_gradient_descent.html#a296ed469c45fd0db6a301f120c3b257">set_display_period</a>(new_display_period);
<a name="l02124"></a>02124       }
<a name="l02125"></a>02125       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02126"></a>02126       {
<a name="l02127"></a>02127          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l02128"></a>02128       }
<a name="l02129"></a>02129    }
<a name="l02130"></a>02130 
<a name="l02131"></a>02131    <span class="comment">// Display</span>
<a name="l02132"></a>02132 
<a name="l02133"></a>02133    TiXmlElement* display_element = gradient_descent_element-&gt;FirstChildElement(<span class="stringliteral">"Display"</span>);
<a name="l02134"></a>02134 
<a name="l02135"></a>02135    <span class="keywordflow">if</span>(display_element)
<a name="l02136"></a>02136    {
<a name="l02137"></a>02137       std::string new_display = display_element-&gt;GetText(); 
<a name="l02138"></a>02138 
<a name="l02139"></a>02139       <span class="keywordflow">try</span>
<a name="l02140"></a>02140       {
<a name="l02141"></a>02141          <a class="code" href="class_open_n_n_1_1_training_algorithm.html#8c9546bea276fe018a2e4269a28d0665">set_display</a>(new_display != <span class="stringliteral">"0"</span>);
<a name="l02142"></a>02142       }
<a name="l02143"></a>02143       <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02144"></a>02144       {
<a name="l02145"></a>02145          std::cout &lt;&lt; e.what() &lt;&lt; std::endl;             
<a name="l02146"></a>02146       }
<a name="l02147"></a>02147    }
<a name="l02148"></a>02148 }
<a name="l02149"></a>02149 
<a name="l02150"></a>02150 }
<a name="l02151"></a>02151 
<a name="l02152"></a>02152 
<a name="l02153"></a>02153 <span class="comment">// OpenNN: Open Neural Networks Library.</span>
<a name="l02154"></a>02154 <span class="comment">// Copyright (C) 2005-2012 Roberto Lopez </span>
<a name="l02155"></a>02155 <span class="comment">//</span>
<a name="l02156"></a>02156 <span class="comment">// This library is free software; you can redistribute it and/or</span>
<a name="l02157"></a>02157 <span class="comment">// modify it under the terms of the GNU Lesser General Public</span>
<a name="l02158"></a>02158 <span class="comment">// License as published by the Free Software Foundation; either</span>
<a name="l02159"></a>02159 <span class="comment">// version 2.1 of the License, or any later version.</span>
<a name="l02160"></a>02160 <span class="comment">//</span>
<a name="l02161"></a>02161 <span class="comment">// This library is distributed in the hope that it will be useful,</span>
<a name="l02162"></a>02162 <span class="comment">// but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<a name="l02163"></a>02163 <span class="comment">// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU</span>
<a name="l02164"></a>02164 <span class="comment">// Lesser General Public License for more details.</span>
<a name="l02165"></a>02165 
<a name="l02166"></a>02166 <span class="comment">// You should have received a copy of the GNU Lesser General Public</span>
<a name="l02167"></a>02167 <span class="comment">// License along with this library; if not, write to the Free Software</span>
<a name="l02168"></a>02168 <span class="comment">// Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA</span>
<a name="l02169"></a>02169 
</pre></div></div>
<hr size="1"><address style="text-align: right;"><small>Generated on Sun Aug 26 11:58:10 2012 for OpenNN by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.5.9 </small></address>
</body>
</html>
