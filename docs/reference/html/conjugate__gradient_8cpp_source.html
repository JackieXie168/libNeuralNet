<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
<title>OpenNN: conjugate_gradient.cpp Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css">
<link href="doxygen.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.5.9 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul>
      <li><a href="index.html"><span>Main&nbsp;Page</span></a></li>
      <li><a href="pages.html"><span>Related&nbsp;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div class="tabs">
    <ul>
      <li><a href="files.html"><span>File&nbsp;List</span></a></li>
    </ul>
  </div>
<h1>conjugate_gradient.cpp</h1><div class="fragment"><pre class="fragment"><a name="l00001"></a>00001 <span class="comment">/****************************************************************************************************************/</span>
<a name="l00002"></a>00002 <span class="comment">/*                                                                                                              */</span>
<a name="l00003"></a>00003 <span class="comment">/*   OpenNN: Open Neural Networks Library                                                                       */</span>
<a name="l00004"></a>00004 <span class="comment">/*   www.opennn.cimne.com                                                                                       */</span>
<a name="l00005"></a>00005 <span class="comment">/*                                                                                                              */</span>
<a name="l00006"></a>00006 <span class="comment">/*   C O N J U G A T E   G R A D I E N T   C L A S S                                                            */</span>
<a name="l00007"></a>00007 <span class="comment">/*                                                                                                              */</span>
<a name="l00008"></a>00008 <span class="comment">/*   Roberto Lopez                                                                                              */</span>
<a name="l00009"></a>00009 <span class="comment">/*   International Center for Numerical Methods in Engineering (CIMNE)                                          */</span>
<a name="l00010"></a>00010 <span class="comment">/*   Technical University of Catalonia (UPC)                                                                    */</span>
<a name="l00011"></a>00011 <span class="comment">/*   Barcelona, Spain                                                                                           */</span>
<a name="l00012"></a>00012 <span class="comment">/*   E-mail: rlopez@cimne.upc.edu                                                                               */</span>
<a name="l00013"></a>00013 <span class="comment">/*                                                                                                              */</span>
<a name="l00014"></a>00014 <span class="comment">/****************************************************************************************************************/</span>
<a name="l00015"></a>00015 
<a name="l00016"></a>00016 <span class="comment">// System inlcludes</span>
<a name="l00017"></a>00017 
<a name="l00018"></a>00018 <span class="preprocessor">#include &lt;string&gt;</span>
<a name="l00019"></a>00019 <span class="preprocessor">#include &lt;sstream&gt;</span>
<a name="l00020"></a>00020 <span class="preprocessor">#include &lt;iostream&gt;</span>
<a name="l00021"></a>00021 <span class="preprocessor">#include &lt;fstream&gt;</span>
<a name="l00022"></a>00022 <span class="preprocessor">#include &lt;algorithm&gt;</span>
<a name="l00023"></a>00023 <span class="preprocessor">#include &lt;functional&gt;</span>
<a name="l00024"></a>00024 <span class="preprocessor">#include &lt;climits&gt;</span>
<a name="l00025"></a>00025 <span class="preprocessor">#include &lt;cmath&gt;</span>
<a name="l00026"></a>00026 <span class="preprocessor">#include &lt;ctime&gt;</span>
<a name="l00027"></a>00027 
<a name="l00028"></a>00028 <span class="comment">// OpenNN includes</span>
<a name="l00029"></a>00029 
<a name="l00030"></a>00030 <span class="preprocessor">#include "conjugate_gradient.h"</span>
<a name="l00031"></a>00031 
<a name="l00032"></a>00032 <span class="comment">// TinyXml includes</span>
<a name="l00033"></a>00033 
<a name="l00034"></a>00034 <span class="preprocessor">#include "../../parsers/tinyxml/tinyxml.h"</span>
<a name="l00035"></a>00035 
<a name="l00036"></a>00036 
<a name="l00037"></a>00037 <span class="keyword">namespace </span>OpenNN
<a name="l00038"></a>00038 {
<a name="l00039"></a>00039 
<a name="l00040"></a>00040 <span class="comment">// DEFAULT CONSTRUCTOR</span>
<a name="l00041"></a>00041 
<a name="l00045"></a>00045 
<a name="l00046"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#1a583e3f482717a40656a18a0f9071ea">00046</a> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#1a583e3f482717a40656a18a0f9071ea">ConjugateGradient::ConjugateGradient</a>(<span class="keywordtype">void</span>) : <a class="code" href="class_open_n_n_1_1_training_algorithm.html">TrainingAlgorithm</a>()
<a name="l00047"></a>00047 {
<a name="l00048"></a>00048    <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#7098a0ce93b35aed9c735d9e8fdf87b5">set_default</a>();
<a name="l00049"></a>00049 }
<a name="l00050"></a>00050 
<a name="l00051"></a>00051 
<a name="l00052"></a>00052 <span class="comment">// GENERAL CONSTRUCTOR</span>
<a name="l00053"></a>00053 
<a name="l00058"></a>00058 
<a name="l00059"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#cb492edd9546dd27a67de1b91d80906b">00059</a> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#1a583e3f482717a40656a18a0f9071ea">ConjugateGradient::ConjugateGradient</a>(<a class="code" href="class_open_n_n_1_1_performance_functional.html">PerformanceFunctional</a>* new_performance_functional_pointer)
<a name="l00060"></a>00060 : <a class="code" href="class_open_n_n_1_1_training_algorithm.html">TrainingAlgorithm</a>(new_performance_functional_pointer)
<a name="l00061"></a>00061 {
<a name="l00062"></a>00062    training_rate_algorithm.<a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html#b39e6f8f5624543629db1b2566c0ba94">set_performance_functional_pointer</a>(new_performance_functional_pointer);   
<a name="l00063"></a>00063 
<a name="l00064"></a>00064    <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#7098a0ce93b35aed9c735d9e8fdf87b5">set_default</a>();
<a name="l00065"></a>00065 }
<a name="l00066"></a>00066 
<a name="l00067"></a>00067 
<a name="l00068"></a>00068 <span class="comment">// XML CONSTRUCTOR </span>
<a name="l00069"></a>00069 
<a name="l00074"></a>00074 
<a name="l00075"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#017be76bfb8f94567c64c1e6e4869ea7">00075</a> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#1a583e3f482717a40656a18a0f9071ea">ConjugateGradient::ConjugateGradient</a>(TiXmlElement* conjugate_gradient_element) 
<a name="l00076"></a>00076  : <a class="code" href="class_open_n_n_1_1_training_algorithm.html">TrainingAlgorithm</a>(conjugate_gradient_element)
<a name="l00077"></a>00077 {
<a name="l00078"></a>00078    <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#7098a0ce93b35aed9c735d9e8fdf87b5">set_default</a>();
<a name="l00079"></a>00079 
<a name="l00080"></a>00080    <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#0a4d9795e258a93cf261b996eb785018">from_XML</a>(conjugate_gradient_element);
<a name="l00081"></a>00081 }
<a name="l00082"></a>00082 
<a name="l00083"></a>00083 
<a name="l00084"></a>00084 <span class="comment">// DESTRUCTOR</span>
<a name="l00085"></a>00085 
<a name="l00087"></a>00087 
<a name="l00088"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#f2be8db22fb41b35959c4713450e47de">00088</a> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#f2be8db22fb41b35959c4713450e47de" title="Destructor.">ConjugateGradient::~ConjugateGradient</a>(<span class="keywordtype">void</span>)
<a name="l00089"></a>00089 {
<a name="l00090"></a>00090 }
<a name="l00091"></a>00091 
<a name="l00092"></a>00092 
<a name="l00093"></a>00093 <span class="comment">// METHODS</span>
<a name="l00094"></a>00094 
<a name="l00095"></a>00095 <span class="comment">// const TrainingRateAlgorithm&amp; get_training_rate_algorithm(void) const method</span>
<a name="l00096"></a>00096 
<a name="l00098"></a>00098 
<a name="l00099"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#28c02d60f1cb366d2678b563dd69fa94">00099</a> <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html">TrainingRateAlgorithm</a>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#28c02d60f1cb366d2678b563dd69fa94" title="This method returns a constant reference to the training rate algorithm object inside...">ConjugateGradient::get_training_rate_algorithm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00100"></a>00100 <span class="keyword"></span>{
<a name="l00101"></a>00101    <span class="keywordflow">return</span>(training_rate_algorithm);
<a name="l00102"></a>00102 }
<a name="l00103"></a>00103 
<a name="l00104"></a>00104 
<a name="l00105"></a>00105 <span class="comment">// TrainingRateAlgorithm* get_training_rate_algorithm_pointer(void) method</span>
<a name="l00106"></a>00106 
<a name="l00108"></a>00108 
<a name="l00109"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#b85062c33da1b9065b81ac946198a5c9">00109</a> <a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html">TrainingRateAlgorithm</a>* <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#b85062c33da1b9065b81ac946198a5c9" title="This method returns a pointer to the training rate algorithm object inside the conjugate...">ConjugateGradient::get_training_rate_algorithm_pointer</a>(<span class="keywordtype">void</span>)
<a name="l00110"></a>00110 {
<a name="l00111"></a>00111    <span class="keywordflow">return</span>(&amp;training_rate_algorithm);
<a name="l00112"></a>00112 }
<a name="l00113"></a>00113 
<a name="l00114"></a>00114 
<a name="l00115"></a>00115 <span class="comment">// TrainingDirectionMethod get_training_direction_method(void) const method</span>
<a name="l00116"></a>00116 
<a name="l00118"></a>00118 
<a name="l00119"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#43e996a2ada93a8383616f937d256fca">00119</a> <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#6945bdfb0798a692bc45158307a05423" title="Enumeration of the available training operators for obtaining the training direction...">ConjugateGradient::TrainingDirectionMethod</a>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#43e996a2ada93a8383616f937d256fca" title="This method returns the conjugate gradient training direction method used for training...">ConjugateGradient::get_training_direction_method</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00120"></a>00120 <span class="keyword"></span>{
<a name="l00121"></a>00121    <span class="keywordflow">return</span>(training_direction_method);
<a name="l00122"></a>00122 }
<a name="l00123"></a>00123 
<a name="l00124"></a>00124 
<a name="l00125"></a>00125 <span class="comment">// std::string write_training_direction_method(void) const method</span>
<a name="l00126"></a>00126 
<a name="l00128"></a>00128 
<a name="l00129"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#020bd6b55cfcedf04647818eb388b292">00129</a> std::string <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#020bd6b55cfcedf04647818eb388b292" title="This method returns a string with the name of the training direction.">ConjugateGradient::write_training_direction_method</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00130"></a>00130 <span class="keyword"></span>{
<a name="l00131"></a>00131    <span class="keywordflow">switch</span>(training_direction_method)
<a name="l00132"></a>00132    {
<a name="l00133"></a>00133       <span class="keywordflow">case</span> PR:
<a name="l00134"></a>00134       {
<a name="l00135"></a>00135          <span class="keywordflow">return</span>(<span class="stringliteral">"PR"</span>);
<a name="l00136"></a>00136       }
<a name="l00137"></a>00137       <span class="keywordflow">break</span>;
<a name="l00138"></a>00138 
<a name="l00139"></a>00139       <span class="keywordflow">case</span> FR:
<a name="l00140"></a>00140       {
<a name="l00141"></a>00141          <span class="keywordflow">return</span>(<span class="stringliteral">"FR"</span>);
<a name="l00142"></a>00142       }
<a name="l00143"></a>00143       <span class="keywordflow">break</span>;
<a name="l00144"></a>00144 
<a name="l00145"></a>00145       <span class="keywordflow">default</span>:
<a name="l00146"></a>00146       {
<a name="l00147"></a>00147          std::ostringstream buffer;
<a name="l00148"></a>00148 
<a name="l00149"></a>00149          buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: ConjugateGradient class.\n"</span>
<a name="l00150"></a>00150                 &lt;&lt; <span class="stringliteral">"std::string write_training_direction_method(void) const method.\n"</span>
<a name="l00151"></a>00151                 &lt;&lt; <span class="stringliteral">"Unknown training direction method.\n"</span>;
<a name="l00152"></a>00152  
<a name="l00153"></a>00153          <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00154"></a>00154       }
<a name="l00155"></a>00155       <span class="keywordflow">break</span>;
<a name="l00156"></a>00156    }
<a name="l00157"></a>00157 }
<a name="l00158"></a>00158 
<a name="l00159"></a>00159 
<a name="l00160"></a>00160 <span class="comment">// const double&amp; get_warning_parameters_norm(void) const method</span>
<a name="l00161"></a>00161 
<a name="l00163"></a>00163 
<a name="l00164"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#5e8a2b67c6a42a1b14fa97e2b69175f9">00164</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#5e8a2b67c6a42a1b14fa97e2b69175f9" title="This method returns the minimum value for the norm of the parameters vector at wich...">ConjugateGradient::get_warning_parameters_norm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00165"></a>00165 <span class="keyword"></span>{
<a name="l00166"></a>00166    <span class="keywordflow">return</span>(warning_parameters_norm);       
<a name="l00167"></a>00167 }
<a name="l00168"></a>00168 
<a name="l00169"></a>00169 
<a name="l00170"></a>00170 <span class="comment">// const double&amp; get_warning_gradient_norm(void) const method</span>
<a name="l00171"></a>00171 
<a name="l00173"></a>00173 
<a name="l00174"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#d81ca9794bb6f12cb779429e722ec80b">00174</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#d81ca9794bb6f12cb779429e722ec80b" title="This method returns the minimum value for the norm of the gradient vector at wich...">ConjugateGradient::get_warning_gradient_norm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00175"></a>00175 <span class="keyword"></span>{
<a name="l00176"></a>00176    <span class="keywordflow">return</span>(warning_gradient_norm);       
<a name="l00177"></a>00177 }
<a name="l00178"></a>00178 
<a name="l00179"></a>00179 
<a name="l00180"></a>00180 <span class="comment">// const double&amp; get_warning_training_rate(void) const method</span>
<a name="l00181"></a>00181 
<a name="l00183"></a>00183 
<a name="l00184"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#38eb80588a92a1b33b5d55f23619a92e">00184</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#38eb80588a92a1b33b5d55f23619a92e" title="This method returns the training rate value at wich a warning message is written...">ConjugateGradient::get_warning_training_rate</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00185"></a>00185 <span class="keyword"></span>{
<a name="l00186"></a>00186    <span class="keywordflow">return</span>(warning_training_rate);
<a name="l00187"></a>00187 }
<a name="l00188"></a>00188 
<a name="l00189"></a>00189 
<a name="l00190"></a>00190 <span class="comment">// const double&amp; get_error_parameters_norm(void) const method</span>
<a name="l00191"></a>00191 
<a name="l00193"></a>00193 
<a name="l00194"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#2d4b35306a6a32e8db701c4fbf279ab8">00194</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#2d4b35306a6a32e8db701c4fbf279ab8" title="This method returns the value for the norm of the parameters vector at wich an error...">ConjugateGradient::get_error_parameters_norm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00195"></a>00195 <span class="keyword"></span>{
<a name="l00196"></a>00196    <span class="keywordflow">return</span>(error_parameters_norm);
<a name="l00197"></a>00197 }
<a name="l00198"></a>00198 
<a name="l00199"></a>00199 
<a name="l00200"></a>00200 <span class="comment">// const double&amp; get_error_gradient_norm(void) const method</span>
<a name="l00201"></a>00201 
<a name="l00204"></a>00204 
<a name="l00205"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#1ca39aa64b5dc6587e8cd58899b28871">00205</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#1ca39aa64b5dc6587e8cd58899b28871">ConjugateGradient::get_error_gradient_norm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00206"></a>00206 <span class="keyword"></span>{
<a name="l00207"></a>00207    <span class="keywordflow">return</span>(error_gradient_norm);
<a name="l00208"></a>00208 }
<a name="l00209"></a>00209 
<a name="l00210"></a>00210 
<a name="l00211"></a>00211 <span class="comment">// const double&amp; get_error_training_rate(void) const method</span>
<a name="l00212"></a>00212 
<a name="l00215"></a>00215 
<a name="l00216"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#823a4cd208d9d23e78c367b5d40dc973">00216</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#823a4cd208d9d23e78c367b5d40dc973">ConjugateGradient::get_error_training_rate</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00217"></a>00217 <span class="keyword"></span>{
<a name="l00218"></a>00218    <span class="keywordflow">return</span>(error_training_rate);
<a name="l00219"></a>00219 }
<a name="l00220"></a>00220 
<a name="l00221"></a>00221 
<a name="l00222"></a>00222 <span class="comment">// const double&amp; get_minimum_parameters_increment_norm(void) const method</span>
<a name="l00223"></a>00223 
<a name="l00225"></a>00225 
<a name="l00226"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#c51ecb02105e6c6eee62a03bfa4de9b5">00226</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#c51ecb02105e6c6eee62a03bfa4de9b5" title="This method returns the minimum norm of the parameter increment vector used as a...">ConjugateGradient::get_minimum_parameters_increment_norm</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00227"></a>00227 <span class="keyword"></span>{
<a name="l00228"></a>00228    <span class="keywordflow">return</span>(minimum_parameters_increment_norm);
<a name="l00229"></a>00229 }
<a name="l00230"></a>00230 
<a name="l00231"></a>00231 
<a name="l00232"></a>00232 <span class="comment">// const double&amp; get_minimum_performance_increase(void) const method</span>
<a name="l00233"></a>00233 
<a name="l00235"></a>00235 
<a name="l00236"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#438904120da1acb53573676ced73fb3e">00236</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#438904120da1acb53573676ced73fb3e" title="This method returns the minimum performance improvement during training.">ConjugateGradient::get_minimum_performance_increase</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00237"></a>00237 <span class="keyword"></span>{
<a name="l00238"></a>00238    <span class="keywordflow">return</span>(minimum_performance_increase);
<a name="l00239"></a>00239 }
<a name="l00240"></a>00240 
<a name="l00241"></a>00241 
<a name="l00242"></a>00242 <span class="comment">// const double&amp; get_performance_goal(void) const method</span>
<a name="l00243"></a>00243 
<a name="l00246"></a>00246 
<a name="l00247"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#952c454e35e25bd7c425b80bb33e3837">00247</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#952c454e35e25bd7c425b80bb33e3837">ConjugateGradient::get_performance_goal</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00248"></a>00248 <span class="keyword"></span>{
<a name="l00249"></a>00249    <span class="keywordflow">return</span>(performance_goal);
<a name="l00250"></a>00250 }
<a name="l00251"></a>00251 
<a name="l00252"></a>00252 
<a name="l00253"></a>00253 <span class="comment">// const double&amp; get_gradient_norm_goal(void) const method</span>
<a name="l00254"></a>00254 
<a name="l00257"></a>00257 
<a name="l00258"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#94c848cc3c12c07e246ae16d7a648a35">00258</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#94c848cc3c12c07e246ae16d7a648a35">ConjugateGradient::get_gradient_norm_goal</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00259"></a>00259 <span class="keyword"></span>{
<a name="l00260"></a>00260    <span class="keywordflow">return</span>(gradient_norm_goal);
<a name="l00261"></a>00261 }
<a name="l00262"></a>00262 
<a name="l00263"></a>00263 
<a name="l00264"></a>00264 <span class="comment">// const unsigned int&amp; get_maximum_generalization_evaluation_decreases(void) const method</span>
<a name="l00265"></a>00265 
<a name="l00267"></a>00267 
<a name="l00268"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#194897049aeb0163bf0f558ce566a980">00268</a> <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#194897049aeb0163bf0f558ce566a980" title="This method returns the maximum number of generalization failures during the training...">ConjugateGradient::get_maximum_generalization_evaluation_decreases</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00269"></a>00269 <span class="keyword"></span>{
<a name="l00270"></a>00270    <span class="keywordflow">return</span>(maximum_generalization_evaluation_decreases);
<a name="l00271"></a>00271 }
<a name="l00272"></a>00272 
<a name="l00273"></a>00273 
<a name="l00274"></a>00274 <span class="comment">// const unsigned int&amp; get_maximum_epochs_number(void) const method</span>
<a name="l00275"></a>00275 
<a name="l00277"></a>00277 
<a name="l00278"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#9d4a5f1af0823b85434a3da875aa2ad8">00278</a> <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#9d4a5f1af0823b85434a3da875aa2ad8" title="This method returns the maximum number of epochs for training.">ConjugateGradient::get_maximum_epochs_number</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00279"></a>00279 <span class="keyword"></span>{
<a name="l00280"></a>00280    <span class="keywordflow">return</span>(maximum_epochs_number);
<a name="l00281"></a>00281 }
<a name="l00282"></a>00282 
<a name="l00283"></a>00283 
<a name="l00284"></a>00284 <span class="comment">// const double&amp; get_maximum_time(void) const method</span>
<a name="l00285"></a>00285 
<a name="l00287"></a>00287 
<a name="l00288"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#c5624cf775eb3edd705202f2ee984812">00288</a> <span class="keyword">const</span> <span class="keywordtype">double</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#c5624cf775eb3edd705202f2ee984812" title="This method returns the maximum training time.">ConjugateGradient::get_maximum_time</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00289"></a>00289 <span class="keyword"></span>{
<a name="l00290"></a>00290    <span class="keywordflow">return</span>(maximum_time);
<a name="l00291"></a>00291 }
<a name="l00292"></a>00292 
<a name="l00293"></a>00293 
<a name="l00294"></a>00294 <span class="comment">// const bool&amp; get_reserve_parameters_history(void) const method</span>
<a name="l00295"></a>00295 
<a name="l00297"></a>00297 
<a name="l00298"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#32db840eb56ef1a5b63d05b72bfc548d">00298</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#32db840eb56ef1a5b63d05b72bfc548d" title="This method returns true if the parameters history matrix is to be reserved, and...">ConjugateGradient::get_reserve_parameters_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00299"></a>00299 <span class="keyword"></span>{
<a name="l00300"></a>00300    <span class="keywordflow">return</span>(reserve_parameters_history);     
<a name="l00301"></a>00301 }
<a name="l00302"></a>00302 
<a name="l00303"></a>00303 
<a name="l00304"></a>00304 <span class="comment">// const bool&amp; get_reserve_parameters_norm_history(void) const method </span>
<a name="l00305"></a>00305 
<a name="l00307"></a>00307 
<a name="l00308"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#3dcbdaa338cdce965a679c335559596e">00308</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#3dcbdaa338cdce965a679c335559596e" title="This method returns true if the parameters norm history vector is to be reserved...">ConjugateGradient::get_reserve_parameters_norm_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00309"></a>00309 <span class="keyword"></span>{
<a name="l00310"></a>00310    <span class="keywordflow">return</span>(reserve_parameters_norm_history);     
<a name="l00311"></a>00311 }
<a name="l00312"></a>00312 
<a name="l00313"></a>00313 
<a name="l00314"></a>00314 <span class="comment">// const bool&amp; get_reserve_evaluation_history(void) const method</span>
<a name="l00315"></a>00315 
<a name="l00317"></a>00317 
<a name="l00318"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#3c679d7258867ddb0c2444d1ace3e97f">00318</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#3c679d7258867ddb0c2444d1ace3e97f" title="This method returns true if the evaluation history vector is to be reserved, and...">ConjugateGradient::get_reserve_evaluation_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00319"></a>00319 <span class="keyword"></span>{
<a name="l00320"></a>00320    <span class="keywordflow">return</span>(reserve_evaluation_history);     
<a name="l00321"></a>00321 }
<a name="l00322"></a>00322 
<a name="l00323"></a>00323 
<a name="l00324"></a>00324 <span class="comment">// const bool&amp; get_reserve_gradient_history(void) const method</span>
<a name="l00325"></a>00325 
<a name="l00327"></a>00327 
<a name="l00328"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#6e4409d688dea5659d53353046bcbdb0">00328</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#6e4409d688dea5659d53353046bcbdb0" title="This method returns true if the gradient history vector of vectors is to be reserved...">ConjugateGradient::get_reserve_gradient_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00329"></a>00329 <span class="keyword"></span>{
<a name="l00330"></a>00330    <span class="keywordflow">return</span>(reserve_gradient_history);     
<a name="l00331"></a>00331 }
<a name="l00332"></a>00332 
<a name="l00333"></a>00333 
<a name="l00334"></a>00334 <span class="comment">// const bool&amp; get_reserve_gradient_norm_history(void) const method</span>
<a name="l00335"></a>00335 
<a name="l00337"></a>00337 
<a name="l00338"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#3b5923caafc58407ebc36e6cbb7cc12d">00338</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#3b5923caafc58407ebc36e6cbb7cc12d" title="This method returns true if the gradient norm history vector is to be reserved, and...">ConjugateGradient::get_reserve_gradient_norm_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00339"></a>00339 <span class="keyword"></span>{
<a name="l00340"></a>00340    <span class="keywordflow">return</span>(reserve_gradient_norm_history);     
<a name="l00341"></a>00341 }
<a name="l00342"></a>00342 
<a name="l00343"></a>00343 
<a name="l00344"></a>00344 
<a name="l00345"></a>00345 <span class="comment">// const bool&amp; get_reserve_training_direction_history(void) const method</span>
<a name="l00346"></a>00346 
<a name="l00348"></a>00348 
<a name="l00349"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#5e688cf2d60a8619910b3477c69cd15a">00349</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#5e688cf2d60a8619910b3477c69cd15a" title="This method returns true if the training direction history matrix is to be reserved...">ConjugateGradient::get_reserve_training_direction_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00350"></a>00350 <span class="keyword"></span>{
<a name="l00351"></a>00351    <span class="keywordflow">return</span>(reserve_training_direction_history);     
<a name="l00352"></a>00352 }
<a name="l00353"></a>00353 
<a name="l00354"></a>00354 
<a name="l00355"></a>00355 <span class="comment">// const bool&amp; get_reserve_training_rate_history(void) const method</span>
<a name="l00356"></a>00356 
<a name="l00358"></a>00358 
<a name="l00359"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#b8d074f8bbd8a98c6314cf2e557aedc8">00359</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#b8d074f8bbd8a98c6314cf2e557aedc8" title="This method returns true if the training rate history vector is to be reserved, and...">ConjugateGradient::get_reserve_training_rate_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00360"></a>00360 <span class="keyword"></span>{
<a name="l00361"></a>00361    <span class="keywordflow">return</span>(reserve_training_rate_history);     
<a name="l00362"></a>00362 }
<a name="l00363"></a>00363 
<a name="l00364"></a>00364 
<a name="l00365"></a>00365 <span class="comment">// const bool&amp; get_reserve_elapsed_time_history(void) const method</span>
<a name="l00366"></a>00366 
<a name="l00368"></a>00368 
<a name="l00369"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#7aa7d75996822715371d62a40f056223">00369</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#7aa7d75996822715371d62a40f056223" title="This method returns true if the elapsed time history vector is to be reserved, and...">ConjugateGradient::get_reserve_elapsed_time_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00370"></a>00370 <span class="keyword"></span>{
<a name="l00371"></a>00371    <span class="keywordflow">return</span>(reserve_elapsed_time_history);     
<a name="l00372"></a>00372 }
<a name="l00373"></a>00373 
<a name="l00374"></a>00374 
<a name="l00375"></a>00375 <span class="comment">// const bool&amp; get_reserve_generalization_evaluation_history(void) const method</span>
<a name="l00376"></a>00376 
<a name="l00378"></a>00378 
<a name="l00379"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#98ef4a1f946631cf02659db7b32cd6ef">00379</a> <span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#98ef4a1f946631cf02659db7b32cd6ef" title="This method returns true if the Generalization evaluation history vector is to be...">ConjugateGradient::get_reserve_generalization_evaluation_history</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00380"></a>00380 <span class="keyword"></span>{
<a name="l00381"></a>00381    <span class="keywordflow">return</span>(reserve_generalization_evaluation_history);
<a name="l00382"></a>00382 }
<a name="l00383"></a>00383 
<a name="l00384"></a>00384 
<a name="l00385"></a>00385 <span class="comment">// const unsigned int&amp; get_display_period(void) const method</span>
<a name="l00386"></a>00386 
<a name="l00388"></a>00388 
<a name="l00389"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#877bd03a5d267e8d648c9ad1f5c941c8">00389</a> <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#877bd03a5d267e8d648c9ad1f5c941c8" title="This method returns the number of epochs between the training showing progress.">ConjugateGradient::get_display_period</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l00390"></a>00390 <span class="keyword"></span>{
<a name="l00391"></a>00391    <span class="keywordflow">return</span>(display_period);
<a name="l00392"></a>00392 }
<a name="l00393"></a>00393 
<a name="l00394"></a>00394 
<a name="l00395"></a>00395 
<a name="l00396"></a>00396 <span class="comment">// void set_training_direction_method(const TrainingDirectionMethod&amp;) method</span>
<a name="l00397"></a>00397 
<a name="l00401"></a>00401 
<a name="l00402"></a>00402 <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#6ae0ce9c10e2462993eee59883e490ba">ConjugateGradient::set_training_direction_method</a>
<a name="l00403"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#6ae0ce9c10e2462993eee59883e490ba">00403</a> (<span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#6945bdfb0798a692bc45158307a05423" title="Enumeration of the available training operators for obtaining the training direction...">ConjugateGradient::TrainingDirectionMethod</a>&amp; new_training_direction_method)
<a name="l00404"></a>00404 {
<a name="l00405"></a>00405    training_direction_method = new_training_direction_method;   
<a name="l00406"></a>00406 }
<a name="l00407"></a>00407 
<a name="l00408"></a>00408 
<a name="l00409"></a>00409 <span class="comment">// void set_training_direction_method(const std::string&amp;) method</span>
<a name="l00410"></a>00410 
<a name="l00418"></a>00418 
<a name="l00419"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#ee58db9e1adf83e2e1ab8bbba3a58ee1">00419</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#6ae0ce9c10e2462993eee59883e490ba">ConjugateGradient::set_training_direction_method</a>(<span class="keyword">const</span> std::string&amp; new_training_direction_method_name)
<a name="l00420"></a>00420 {
<a name="l00421"></a>00421    <span class="keywordflow">if</span>(new_training_direction_method_name == <span class="stringliteral">"PR"</span>)
<a name="l00422"></a>00422    {
<a name="l00423"></a>00423       training_direction_method = PR;
<a name="l00424"></a>00424    }
<a name="l00425"></a>00425    <span class="keywordflow">else</span> <span class="keywordflow">if</span>(new_training_direction_method_name == <span class="stringliteral">"FR"</span>)
<a name="l00426"></a>00426    {
<a name="l00427"></a>00427       training_direction_method = FR;
<a name="l00428"></a>00428    }
<a name="l00429"></a>00429    <span class="keywordflow">else</span>
<a name="l00430"></a>00430    {
<a name="l00431"></a>00431       std::ostringstream buffer;
<a name="l00432"></a>00432 
<a name="l00433"></a>00433       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: ConjugateGradient class.\n"</span>
<a name="l00434"></a>00434              &lt;&lt; <span class="stringliteral">"void set_training_direction_method(const std::string&amp;) method.\n"</span>
<a name="l00435"></a>00435                          &lt;&lt; <span class="stringliteral">"Unknown training direction method: "</span> &lt;&lt; new_training_direction_method_name &lt;&lt; <span class="stringliteral">".\n"</span>;
<a name="l00436"></a>00436    
<a name="l00437"></a>00437       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00438"></a>00438    }
<a name="l00439"></a>00439 }
<a name="l00440"></a>00440 
<a name="l00441"></a>00441 
<a name="l00442"></a>00442 <span class="comment">// void set_reserve_all_training_history(bool) method</span>
<a name="l00443"></a>00443 
<a name="l00459"></a>00459 
<a name="l00460"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#5b21e8e16b09531d93384dd9339c02f5">00460</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#5b21e8e16b09531d93384dd9339c02f5">ConjugateGradient::set_reserve_all_training_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_all_training_history)
<a name="l00461"></a>00461 {
<a name="l00462"></a>00462 
<a name="l00463"></a>00463    <span class="comment">// Multilayer perceptron</span>
<a name="l00464"></a>00464 
<a name="l00465"></a>00465    reserve_parameters_history = new_reserve_all_training_history;
<a name="l00466"></a>00466    reserve_parameters_norm_history = new_reserve_all_training_history;
<a name="l00467"></a>00467    
<a name="l00468"></a>00468    <span class="comment">// Performance functional</span>
<a name="l00469"></a>00469 
<a name="l00470"></a>00470    reserve_evaluation_history = new_reserve_all_training_history;
<a name="l00471"></a>00471    reserve_gradient_history = new_reserve_all_training_history;
<a name="l00472"></a>00472    reserve_gradient_norm_history = new_reserve_all_training_history;
<a name="l00473"></a>00473 
<a name="l00474"></a>00474    reserve_generalization_evaluation_history = new_reserve_all_training_history;
<a name="l00475"></a>00475 
<a name="l00476"></a>00476    <span class="comment">// Training algorithm</span>
<a name="l00477"></a>00477 
<a name="l00478"></a>00478    reserve_training_direction_history = new_reserve_all_training_history;
<a name="l00479"></a>00479    reserve_training_rate_history = new_reserve_all_training_history;
<a name="l00480"></a>00480    reserve_elapsed_time_history = new_reserve_all_training_history;
<a name="l00481"></a>00481 }
<a name="l00482"></a>00482 
<a name="l00483"></a>00483 
<a name="l00484"></a>00484 <span class="comment">// void set_default(void) method</span>
<a name="l00485"></a>00485 
<a name="l00519"></a>00519 
<a name="l00520"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#7098a0ce93b35aed9c735d9e8fdf87b5">00520</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#7098a0ce93b35aed9c735d9e8fdf87b5">ConjugateGradient::set_default</a>(<span class="keywordtype">void</span>)
<a name="l00521"></a>00521 {
<a name="l00522"></a>00522    <span class="comment">// TRAINING PARAMETERS</span>
<a name="l00523"></a>00523 
<a name="l00524"></a>00524    warning_parameters_norm = 1.0e6;
<a name="l00525"></a>00525    warning_gradient_norm = 1.0e6;   
<a name="l00526"></a>00526    warning_training_rate = 1.0e6;
<a name="l00527"></a>00527 
<a name="l00528"></a>00528    error_parameters_norm = 1.0e9;
<a name="l00529"></a>00529    error_gradient_norm = 1.0e9;
<a name="l00530"></a>00530    error_training_rate = 1.0e9;
<a name="l00531"></a>00531 
<a name="l00532"></a>00532    <span class="comment">// STOPPING CRITERIA</span>
<a name="l00533"></a>00533 
<a name="l00534"></a>00534    minimum_parameters_increment_norm = 0.0;
<a name="l00535"></a>00535 
<a name="l00536"></a>00536    minimum_performance_increase = 0.0;
<a name="l00537"></a>00537    performance_goal = -1.0e99;
<a name="l00538"></a>00538    gradient_norm_goal = 0.0;
<a name="l00539"></a>00539    maximum_generalization_evaluation_decreases = 1000000;
<a name="l00540"></a>00540 
<a name="l00541"></a>00541    maximum_epochs_number = 1000;
<a name="l00542"></a>00542    maximum_time = 1000.0;
<a name="l00543"></a>00543 
<a name="l00544"></a>00544    <span class="comment">// TRAINING HISTORY</span>
<a name="l00545"></a>00545 
<a name="l00546"></a>00546    reserve_parameters_history = <span class="keyword">false</span>;
<a name="l00547"></a>00547    reserve_parameters_norm_history = <span class="keyword">false</span>;
<a name="l00548"></a>00548 
<a name="l00549"></a>00549    reserve_evaluation_history = <span class="keyword">true</span>;
<a name="l00550"></a>00550    reserve_gradient_history = <span class="keyword">false</span>;
<a name="l00551"></a>00551    reserve_gradient_norm_history = <span class="keyword">false</span>;
<a name="l00552"></a>00552    reserve_generalization_evaluation_history = <span class="keyword">false</span>;
<a name="l00553"></a>00553 
<a name="l00554"></a>00554    reserve_training_direction_history = <span class="keyword">false</span>;
<a name="l00555"></a>00555    reserve_training_rate_history = <span class="keyword">false</span>;
<a name="l00556"></a>00556    reserve_elapsed_time_history = <span class="keyword">false</span>;
<a name="l00557"></a>00557 
<a name="l00558"></a>00558    <span class="comment">// UTILITIES</span>
<a name="l00559"></a>00559 
<a name="l00560"></a>00560    <a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a> = <span class="keyword">true</span>;
<a name="l00561"></a>00561    display_period = 100;
<a name="l00562"></a>00562 
<a name="l00563"></a>00563 
<a name="l00564"></a>00564    training_direction_method = PR;
<a name="l00565"></a>00565 }
<a name="l00566"></a>00566 
<a name="l00567"></a>00567 
<a name="l00568"></a>00568 <span class="comment">// void set_warning_parameters_norm(const double&amp;) method</span>
<a name="l00569"></a>00569 
<a name="l00573"></a>00573 
<a name="l00574"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#4b1a110fd305f1986af4cb58c528ac63">00574</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#4b1a110fd305f1986af4cb58c528ac63">ConjugateGradient::set_warning_parameters_norm</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_warning_parameters_norm)
<a name="l00575"></a>00575 {
<a name="l00576"></a>00576    <span class="comment">// Control sentence (if debug)</span>
<a name="l00577"></a>00577 
<a name="l00578"></a>00578 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00579"></a>00579 <span class="preprocessor"></span>
<a name="l00580"></a>00580    <span class="keywordflow">if</span>(new_warning_parameters_norm &lt; 0.0)
<a name="l00581"></a>00581    {
<a name="l00582"></a>00582       std::ostringstream buffer;
<a name="l00583"></a>00583 
<a name="l00584"></a>00584       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00585"></a>00585              &lt;&lt; <span class="stringliteral">"void set_warning_parameters_norm(const double&amp;) method.\n"</span>
<a name="l00586"></a>00586              &lt;&lt; <span class="stringliteral">"Warning parameters norm must be equal or greater than 0.\n"</span>;
<a name="l00587"></a>00587 
<a name="l00588"></a>00588       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00589"></a>00589    }
<a name="l00590"></a>00590 
<a name="l00591"></a>00591 <span class="preprocessor">   #endif</span>
<a name="l00592"></a>00592 <span class="preprocessor"></span>
<a name="l00593"></a>00593    <span class="comment">// Set warning parameters norm</span>
<a name="l00594"></a>00594 
<a name="l00595"></a>00595    warning_parameters_norm = new_warning_parameters_norm;     
<a name="l00596"></a>00596 }
<a name="l00597"></a>00597 
<a name="l00598"></a>00598 
<a name="l00599"></a>00599 <span class="comment">// void set_warning_gradient_norm(const double&amp;) method</span>
<a name="l00600"></a>00600 
<a name="l00604"></a>00604 
<a name="l00605"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#b9d2365e8c662083d1edd946d521f7bf">00605</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#b9d2365e8c662083d1edd946d521f7bf">ConjugateGradient::set_warning_gradient_norm</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_warning_gradient_norm)
<a name="l00606"></a>00606 {
<a name="l00607"></a>00607    <span class="comment">// Control sentence (if debug)</span>
<a name="l00608"></a>00608 
<a name="l00609"></a>00609 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00610"></a>00610 <span class="preprocessor"></span>
<a name="l00611"></a>00611    <span class="keywordflow">if</span>(new_warning_gradient_norm &lt; 0.0)
<a name="l00612"></a>00612    {
<a name="l00613"></a>00613       std::ostringstream buffer;
<a name="l00614"></a>00614 
<a name="l00615"></a>00615       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00616"></a>00616              &lt;&lt; <span class="stringliteral">"void set_warning_gradient_norm(const double&amp;) method.\n"</span>
<a name="l00617"></a>00617              &lt;&lt; <span class="stringliteral">"Warning gradient norm must be equal or greater than 0.\n"</span>;
<a name="l00618"></a>00618 
<a name="l00619"></a>00619       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00620"></a>00620    }
<a name="l00621"></a>00621 
<a name="l00622"></a>00622 <span class="preprocessor">   #endif</span>
<a name="l00623"></a>00623 <span class="preprocessor"></span>
<a name="l00624"></a>00624    <span class="comment">// Set warning gradient norm</span>
<a name="l00625"></a>00625 
<a name="l00626"></a>00626    warning_gradient_norm = new_warning_gradient_norm;     
<a name="l00627"></a>00627 }
<a name="l00628"></a>00628 
<a name="l00629"></a>00629 
<a name="l00630"></a>00630 <span class="comment">// void set_warning_training_rate(const double&amp;) method</span>
<a name="l00631"></a>00631 
<a name="l00635"></a>00635 
<a name="l00636"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#bbae77c17da7d542b4e3c4251e57cb06">00636</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#bbae77c17da7d542b4e3c4251e57cb06">ConjugateGradient::set_warning_training_rate</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_warning_training_rate)
<a name="l00637"></a>00637 {
<a name="l00638"></a>00638    <span class="comment">// Control sentence (if debug)</span>
<a name="l00639"></a>00639 
<a name="l00640"></a>00640 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00641"></a>00641 <span class="preprocessor"></span>
<a name="l00642"></a>00642    <span class="keywordflow">if</span>(new_warning_training_rate &lt; 0.0)
<a name="l00643"></a>00643    {
<a name="l00644"></a>00644       std::ostringstream buffer;
<a name="l00645"></a>00645 
<a name="l00646"></a>00646       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span> 
<a name="l00647"></a>00647              &lt;&lt; <span class="stringliteral">"void set_warning_training_rate(const double&amp;) method.\n"</span>
<a name="l00648"></a>00648              &lt;&lt; <span class="stringliteral">"Warning training rate must be equal or greater than 0.\n"</span>;
<a name="l00649"></a>00649 
<a name="l00650"></a>00650       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00651"></a>00651    }
<a name="l00652"></a>00652 
<a name="l00653"></a>00653 <span class="preprocessor">   #endif</span>
<a name="l00654"></a>00654 <span class="preprocessor"></span>
<a name="l00655"></a>00655    warning_training_rate = new_warning_training_rate;
<a name="l00656"></a>00656 }
<a name="l00657"></a>00657 
<a name="l00658"></a>00658 
<a name="l00659"></a>00659 <span class="comment">// void set_error_parameters_norm(const double&amp;) method</span>
<a name="l00660"></a>00660 
<a name="l00664"></a>00664 
<a name="l00665"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#c213f5dae6f2236e288462abb794a956">00665</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#c213f5dae6f2236e288462abb794a956">ConjugateGradient::set_error_parameters_norm</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_error_parameters_norm)
<a name="l00666"></a>00666 {
<a name="l00667"></a>00667    <span class="comment">// Control sentence (if debug)</span>
<a name="l00668"></a>00668 
<a name="l00669"></a>00669 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00670"></a>00670 <span class="preprocessor"></span>
<a name="l00671"></a>00671    <span class="keywordflow">if</span>(new_error_parameters_norm &lt; 0.0)
<a name="l00672"></a>00672    {
<a name="l00673"></a>00673       std::ostringstream buffer;
<a name="l00674"></a>00674 
<a name="l00675"></a>00675       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00676"></a>00676              &lt;&lt; <span class="stringliteral">"void set_error_parameters_norm(const double&amp;) method.\n"</span>
<a name="l00677"></a>00677              &lt;&lt; <span class="stringliteral">"Error parameters norm must be equal or greater than 0.\n"</span>;
<a name="l00678"></a>00678 
<a name="l00679"></a>00679       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00680"></a>00680    }
<a name="l00681"></a>00681 
<a name="l00682"></a>00682 <span class="preprocessor">   #endif</span>
<a name="l00683"></a>00683 <span class="preprocessor"></span>
<a name="l00684"></a>00684    <span class="comment">// Set error parameters norm</span>
<a name="l00685"></a>00685 
<a name="l00686"></a>00686    error_parameters_norm = new_error_parameters_norm;
<a name="l00687"></a>00687 }
<a name="l00688"></a>00688 
<a name="l00689"></a>00689 
<a name="l00690"></a>00690 <span class="comment">// void set_error_gradient_norm(const double&amp;) method</span>
<a name="l00691"></a>00691 
<a name="l00695"></a>00695 
<a name="l00696"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#ac8d36d1372f47c9cf03a3b73c1df440">00696</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#ac8d36d1372f47c9cf03a3b73c1df440">ConjugateGradient::set_error_gradient_norm</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_error_gradient_norm)
<a name="l00697"></a>00697 {
<a name="l00698"></a>00698    <span class="comment">// Control sentence (if debug)</span>
<a name="l00699"></a>00699 
<a name="l00700"></a>00700 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00701"></a>00701 <span class="preprocessor"></span>
<a name="l00702"></a>00702    <span class="keywordflow">if</span>(new_error_gradient_norm &lt; 0.0)
<a name="l00703"></a>00703    {
<a name="l00704"></a>00704       std::ostringstream buffer;
<a name="l00705"></a>00705 
<a name="l00706"></a>00706       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00707"></a>00707              &lt;&lt; <span class="stringliteral">"void set_error_gradient_norm(const double&amp;) method.\n"</span>
<a name="l00708"></a>00708              &lt;&lt; <span class="stringliteral">"Error gradient norm must be equal or greater than 0.\n"</span>;
<a name="l00709"></a>00709 
<a name="l00710"></a>00710       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00711"></a>00711    }
<a name="l00712"></a>00712 
<a name="l00713"></a>00713 <span class="preprocessor">   #endif</span>
<a name="l00714"></a>00714 <span class="preprocessor"></span>
<a name="l00715"></a>00715    <span class="comment">// Set error gradient norm</span>
<a name="l00716"></a>00716 
<a name="l00717"></a>00717    error_gradient_norm = new_error_gradient_norm;
<a name="l00718"></a>00718 }
<a name="l00719"></a>00719 
<a name="l00720"></a>00720 
<a name="l00721"></a>00721 <span class="comment">// void set_error_training_rate(const double&amp;) method</span>
<a name="l00722"></a>00722 
<a name="l00726"></a>00726 
<a name="l00727"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#165eaf088bb453f5a8ffc231d61d3d5f">00727</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#165eaf088bb453f5a8ffc231d61d3d5f">ConjugateGradient::set_error_training_rate</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_error_training_rate)
<a name="l00728"></a>00728 {
<a name="l00729"></a>00729    <span class="comment">// Control sentence (if debug)</span>
<a name="l00730"></a>00730 
<a name="l00731"></a>00731 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00732"></a>00732 <span class="preprocessor"></span>
<a name="l00733"></a>00733    <span class="keywordflow">if</span>(new_error_training_rate &lt; 0.0)
<a name="l00734"></a>00734    {
<a name="l00735"></a>00735       std::ostringstream buffer;
<a name="l00736"></a>00736 
<a name="l00737"></a>00737       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00738"></a>00738              &lt;&lt; <span class="stringliteral">"void set_error_training_rate(const double&amp;) method.\n"</span>
<a name="l00739"></a>00739              &lt;&lt; <span class="stringliteral">"Error training rate must be equal or greater than 0.\n"</span>;
<a name="l00740"></a>00740 
<a name="l00741"></a>00741       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00742"></a>00742    }
<a name="l00743"></a>00743 
<a name="l00744"></a>00744 <span class="preprocessor">   #endif</span>
<a name="l00745"></a>00745 <span class="preprocessor"></span>
<a name="l00746"></a>00746    <span class="comment">// Set error training rate</span>
<a name="l00747"></a>00747 
<a name="l00748"></a>00748    error_training_rate = new_error_training_rate;
<a name="l00749"></a>00749 }
<a name="l00750"></a>00750 
<a name="l00751"></a>00751 
<a name="l00752"></a>00752 <span class="comment">// void set_minimum_parameters_increment_norm(const double&amp;) method</span>
<a name="l00753"></a>00753 
<a name="l00756"></a>00756 
<a name="l00757"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#bcf2b6f18bb3ea67c66b8f274e05c170">00757</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#bcf2b6f18bb3ea67c66b8f274e05c170">ConjugateGradient::set_minimum_parameters_increment_norm</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_minimum_parameters_increment_norm)
<a name="l00758"></a>00758 {
<a name="l00759"></a>00759    <span class="comment">// Control sentence (if debug)</span>
<a name="l00760"></a>00760 
<a name="l00761"></a>00761 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00762"></a>00762 <span class="preprocessor"></span>
<a name="l00763"></a>00763    <span class="keywordflow">if</span>(new_minimum_parameters_increment_norm &lt; 0.0)
<a name="l00764"></a>00764    {
<a name="l00765"></a>00765       std::ostringstream buffer;
<a name="l00766"></a>00766 
<a name="l00767"></a>00767       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00768"></a>00768              &lt;&lt; <span class="stringliteral">"void new_minimum_parameters_increment_norm(const double&amp;) method.\n"</span>
<a name="l00769"></a>00769              &lt;&lt; <span class="stringliteral">"Minimum parameters increment norm must be equal or greater than 0.\n"</span>;
<a name="l00770"></a>00770 
<a name="l00771"></a>00771       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00772"></a>00772    }
<a name="l00773"></a>00773 
<a name="l00774"></a>00774 <span class="preprocessor">   #endif</span>
<a name="l00775"></a>00775 <span class="preprocessor"></span>
<a name="l00776"></a>00776    <span class="comment">// Set error training rate</span>
<a name="l00777"></a>00777 
<a name="l00778"></a>00778    minimum_parameters_increment_norm = new_minimum_parameters_increment_norm;
<a name="l00779"></a>00779 }
<a name="l00780"></a>00780 
<a name="l00781"></a>00781 
<a name="l00782"></a>00782 <span class="comment">// void set_minimum_performance_increase(const double&amp;) method</span>
<a name="l00783"></a>00783 
<a name="l00786"></a>00786 
<a name="l00787"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#bfc1ed807a9f18532c879a82180aebd6">00787</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#bfc1ed807a9f18532c879a82180aebd6">ConjugateGradient::set_minimum_performance_increase</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_minimum_performance_increase)
<a name="l00788"></a>00788 {
<a name="l00789"></a>00789    <span class="comment">// Control sentence (if debug)</span>
<a name="l00790"></a>00790 
<a name="l00791"></a>00791 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00792"></a>00792 <span class="preprocessor"></span>
<a name="l00793"></a>00793    <span class="keywordflow">if</span>(new_minimum_performance_increase &lt; 0.0)
<a name="l00794"></a>00794    {
<a name="l00795"></a>00795       std::ostringstream buffer;
<a name="l00796"></a>00796 
<a name="l00797"></a>00797       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00798"></a>00798              &lt;&lt; <span class="stringliteral">"void set_minimum_performance_increase(const double&amp;) method.\n"</span>
<a name="l00799"></a>00799              &lt;&lt; <span class="stringliteral">"Minimum performance improvement must be equal or greater than 0.\n"</span>;
<a name="l00800"></a>00800 
<a name="l00801"></a>00801       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00802"></a>00802    }
<a name="l00803"></a>00803 
<a name="l00804"></a>00804 <span class="preprocessor">   #endif</span>
<a name="l00805"></a>00805 <span class="preprocessor"></span>
<a name="l00806"></a>00806    <span class="comment">// Set minimum performance improvement</span>
<a name="l00807"></a>00807 
<a name="l00808"></a>00808    minimum_performance_increase = new_minimum_performance_increase;
<a name="l00809"></a>00809 }
<a name="l00810"></a>00810 
<a name="l00811"></a>00811 
<a name="l00812"></a>00812 <span class="comment">// void set_performance_goal(const double&amp;) method</span>
<a name="l00813"></a>00813 
<a name="l00817"></a>00817 
<a name="l00818"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#1e5f16b50b764b468183186422703313">00818</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#1e5f16b50b764b468183186422703313">ConjugateGradient::set_performance_goal</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_performance_goal)
<a name="l00819"></a>00819 {
<a name="l00820"></a>00820    performance_goal = new_performance_goal;
<a name="l00821"></a>00821 }
<a name="l00822"></a>00822 
<a name="l00823"></a>00823 
<a name="l00824"></a>00824 <span class="comment">// void set_gradient_norm_goal(const double&amp;) method</span>
<a name="l00825"></a>00825 
<a name="l00829"></a>00829 
<a name="l00830"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#06331ca7f99debc62d1afadf26bff850">00830</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#06331ca7f99debc62d1afadf26bff850">ConjugateGradient::set_gradient_norm_goal</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_gradient_norm_goal)
<a name="l00831"></a>00831 {
<a name="l00832"></a>00832    <span class="comment">// Control sentence (if debug)</span>
<a name="l00833"></a>00833 
<a name="l00834"></a>00834 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00835"></a>00835 <span class="preprocessor"></span>
<a name="l00836"></a>00836    <span class="keywordflow">if</span>(new_gradient_norm_goal &lt; 0.0)
<a name="l00837"></a>00837    {
<a name="l00838"></a>00838       std::ostringstream buffer;
<a name="l00839"></a>00839 
<a name="l00840"></a>00840       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00841"></a>00841              &lt;&lt; <span class="stringliteral">"void set_gradient_norm_goal(const double&amp;) method.\n"</span>
<a name="l00842"></a>00842              &lt;&lt; <span class="stringliteral">"Gradient norm goal must be equal or greater than 0.\n"</span>;
<a name="l00843"></a>00843 
<a name="l00844"></a>00844       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00845"></a>00845    }
<a name="l00846"></a>00846 
<a name="l00847"></a>00847 <span class="preprocessor">   #endif</span>
<a name="l00848"></a>00848 <span class="preprocessor"></span>
<a name="l00849"></a>00849    <span class="comment">// Set gradient norm goal</span>
<a name="l00850"></a>00850 
<a name="l00851"></a>00851    gradient_norm_goal = new_gradient_norm_goal;
<a name="l00852"></a>00852 }
<a name="l00853"></a>00853 
<a name="l00854"></a>00854 
<a name="l00855"></a>00855 <span class="comment">// void set_maximum_generalization_evaluation_decreases(const unsigned int&amp;) method</span>
<a name="l00856"></a>00856 
<a name="l00859"></a>00859 
<a name="l00860"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#ac4f86ed5087ff33770e3a92b3df4fdb">00860</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#ac4f86ed5087ff33770e3a92b3df4fdb">ConjugateGradient::set_maximum_generalization_evaluation_decreases</a>(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; new_maximum_generalization_evaluation_decreases)
<a name="l00861"></a>00861 {
<a name="l00862"></a>00862    <span class="comment">// Control sentence (if debug)</span>
<a name="l00863"></a>00863 
<a name="l00864"></a>00864 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00865"></a>00865 <span class="preprocessor"></span>
<a name="l00866"></a>00866    <span class="keywordflow">if</span>(new_maximum_generalization_evaluation_decreases &lt; 0)
<a name="l00867"></a>00867    {
<a name="l00868"></a>00868       std::ostringstream buffer;
<a name="l00869"></a>00869 
<a name="l00870"></a>00870       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00871"></a>00871              &lt;&lt; <span class="stringliteral">"void set_maximum_generalization_evaluation_decreases(const unsigned int&amp;) method.\n"</span>
<a name="l00872"></a>00872              &lt;&lt; <span class="stringliteral">"Number of generalization performance decreases must be equal or greater than 0.\n"</span>;
<a name="l00873"></a>00873 
<a name="l00874"></a>00874       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00875"></a>00875    }
<a name="l00876"></a>00876 
<a name="l00877"></a>00877 <span class="preprocessor">   #endif</span>
<a name="l00878"></a>00878 <span class="preprocessor"></span>
<a name="l00879"></a>00879    <span class="comment">// Set maximum generalization performace decrases</span>
<a name="l00880"></a>00880 
<a name="l00881"></a>00881    maximum_generalization_evaluation_decreases = new_maximum_generalization_evaluation_decreases;
<a name="l00882"></a>00882 }
<a name="l00883"></a>00883 
<a name="l00884"></a>00884 
<a name="l00885"></a>00885 <span class="comment">// void set_maximum_epochs_number(unsigned int) method</span>
<a name="l00886"></a>00886 
<a name="l00889"></a>00889 
<a name="l00890"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#aebcfb56d10a7846d587db6d66caea63">00890</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#aebcfb56d10a7846d587db6d66caea63">ConjugateGradient::set_maximum_epochs_number</a>(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; new_maximum_epochs_number)
<a name="l00891"></a>00891 {
<a name="l00892"></a>00892    <span class="comment">// Control sentence (if debug)</span>
<a name="l00893"></a>00893 
<a name="l00894"></a>00894 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00895"></a>00895 <span class="preprocessor"></span>
<a name="l00896"></a>00896    <span class="keywordflow">if</span>(new_maximum_epochs_number &lt; 0)
<a name="l00897"></a>00897    {
<a name="l00898"></a>00898       std::ostringstream buffer;
<a name="l00899"></a>00899 
<a name="l00900"></a>00900       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00901"></a>00901              &lt;&lt; <span class="stringliteral">"void set_maximum_epochs_number(unsigned int) method.\n"</span>
<a name="l00902"></a>00902              &lt;&lt; <span class="stringliteral">"Number of epochs must be equal or greater than 0.\n"</span>;
<a name="l00903"></a>00903 
<a name="l00904"></a>00904       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00905"></a>00905    }
<a name="l00906"></a>00906 
<a name="l00907"></a>00907 <span class="preprocessor">   #endif</span>
<a name="l00908"></a>00908 <span class="preprocessor"></span>
<a name="l00909"></a>00909    <span class="comment">// Set maximum epochs number</span>
<a name="l00910"></a>00910 
<a name="l00911"></a>00911    maximum_epochs_number = new_maximum_epochs_number;
<a name="l00912"></a>00912 }
<a name="l00913"></a>00913 
<a name="l00914"></a>00914 
<a name="l00915"></a>00915 <span class="comment">// void set_maximum_time(const double&amp;) method</span>
<a name="l00916"></a>00916 
<a name="l00919"></a>00919 
<a name="l00920"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#7227ff99b52a9fa1a3a5905c6cc66b76">00920</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#7227ff99b52a9fa1a3a5905c6cc66b76">ConjugateGradient::set_maximum_time</a>(<span class="keyword">const</span> <span class="keywordtype">double</span>&amp; new_maximum_time)
<a name="l00921"></a>00921 {
<a name="l00922"></a>00922    <span class="comment">// Control sentence (if debug)</span>
<a name="l00923"></a>00923 
<a name="l00924"></a>00924 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l00925"></a>00925 <span class="preprocessor"></span>
<a name="l00926"></a>00926    <span class="keywordflow">if</span>(new_maximum_time &lt; 0.0)
<a name="l00927"></a>00927    {
<a name="l00928"></a>00928       std::ostringstream buffer;
<a name="l00929"></a>00929 
<a name="l00930"></a>00930       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l00931"></a>00931              &lt;&lt; <span class="stringliteral">"void set_maximum_time(const double&amp;) method.\n"</span>
<a name="l00932"></a>00932              &lt;&lt; <span class="stringliteral">"Maximum time must be equal or greater than 0.\n"</span>;
<a name="l00933"></a>00933 
<a name="l00934"></a>00934       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l00935"></a>00935    }
<a name="l00936"></a>00936    
<a name="l00937"></a>00937 <span class="preprocessor">   #endif</span>
<a name="l00938"></a>00938 <span class="preprocessor"></span>
<a name="l00939"></a>00939    <span class="comment">// Set maximum time</span>
<a name="l00940"></a>00940 
<a name="l00941"></a>00941    maximum_time = new_maximum_time;
<a name="l00942"></a>00942 }
<a name="l00943"></a>00943 
<a name="l00944"></a>00944 
<a name="l00945"></a>00945 <span class="comment">// void set_reserve_parameters_history(bool) method</span>
<a name="l00946"></a>00946 
<a name="l00949"></a>00949 
<a name="l00950"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#9de9b218ff51f2393ea38006019f7ffa">00950</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#9de9b218ff51f2393ea38006019f7ffa">ConjugateGradient::set_reserve_parameters_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_parameters_history)
<a name="l00951"></a>00951 {
<a name="l00952"></a>00952    reserve_parameters_history = new_reserve_parameters_history;     
<a name="l00953"></a>00953 }
<a name="l00954"></a>00954 
<a name="l00955"></a>00955 
<a name="l00956"></a>00956 <span class="comment">// void set_reserve_parameters_norm_history(bool) method</span>
<a name="l00957"></a>00957 
<a name="l00960"></a>00960 
<a name="l00961"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#274786cd036399313295dff4a097130c">00961</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#274786cd036399313295dff4a097130c">ConjugateGradient::set_reserve_parameters_norm_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_parameters_norm_history)
<a name="l00962"></a>00962 {
<a name="l00963"></a>00963    reserve_parameters_norm_history = new_reserve_parameters_norm_history;     
<a name="l00964"></a>00964 }
<a name="l00965"></a>00965 
<a name="l00966"></a>00966 
<a name="l00967"></a>00967 <span class="comment">// void set_reserve_evaluation_history(bool) method</span>
<a name="l00968"></a>00968 
<a name="l00971"></a>00971 
<a name="l00972"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#eb7604ee6718eeda10dbff12695ca215">00972</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#eb7604ee6718eeda10dbff12695ca215">ConjugateGradient::set_reserve_evaluation_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_evaluation_history)
<a name="l00973"></a>00973 {
<a name="l00974"></a>00974    reserve_evaluation_history = new_reserve_evaluation_history;     
<a name="l00975"></a>00975 }
<a name="l00976"></a>00976 
<a name="l00977"></a>00977 
<a name="l00978"></a>00978 <span class="comment">// void set_reserve_gradient_history(bool) method</span>
<a name="l00979"></a>00979 
<a name="l00982"></a>00982 
<a name="l00983"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#faef3aac943440a248a9f2e9851eb9c0">00983</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#faef3aac943440a248a9f2e9851eb9c0">ConjugateGradient::set_reserve_gradient_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_gradient_history)
<a name="l00984"></a>00984 {
<a name="l00985"></a>00985    reserve_gradient_history = new_reserve_gradient_history;    
<a name="l00986"></a>00986 }
<a name="l00987"></a>00987 
<a name="l00988"></a>00988 
<a name="l00989"></a>00989 <span class="comment">// void set_reserve_gradient_norm_history(bool) method</span>
<a name="l00990"></a>00990 
<a name="l00994"></a>00994 
<a name="l00995"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#0c59f4b561c506844b01718945db2ea8">00995</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#0c59f4b561c506844b01718945db2ea8">ConjugateGradient::set_reserve_gradient_norm_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_gradient_norm_history)
<a name="l00996"></a>00996 {
<a name="l00997"></a>00997    reserve_gradient_norm_history = new_reserve_gradient_norm_history;     
<a name="l00998"></a>00998 }
<a name="l00999"></a>00999 
<a name="l01000"></a>01000 
<a name="l01001"></a>01001 <span class="comment">// void set_reserve_training_direction_history(bool) method</span>
<a name="l01002"></a>01002 
<a name="l01006"></a>01006 
<a name="l01007"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#379aa30482f6ce0a1ad2071a4e6f6b61">01007</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#379aa30482f6ce0a1ad2071a4e6f6b61">ConjugateGradient::set_reserve_training_direction_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_training_direction_history)
<a name="l01008"></a>01008 {
<a name="l01009"></a>01009    reserve_training_direction_history = new_reserve_training_direction_history;          
<a name="l01010"></a>01010 }
<a name="l01011"></a>01011 
<a name="l01012"></a>01012 
<a name="l01013"></a>01013 <span class="comment">// void set_reserve_training_rate_history(bool) method</span>
<a name="l01014"></a>01014 
<a name="l01018"></a>01018 
<a name="l01019"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#e46832a168326e64d3e19ef769335771">01019</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#e46832a168326e64d3e19ef769335771">ConjugateGradient::set_reserve_training_rate_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_training_rate_history)
<a name="l01020"></a>01020 {
<a name="l01021"></a>01021    reserve_training_rate_history = new_reserve_training_rate_history;          
<a name="l01022"></a>01022 }
<a name="l01023"></a>01023 
<a name="l01024"></a>01024 
<a name="l01025"></a>01025 <span class="comment">// void set_reserve_elapsed_time_history(bool) method</span>
<a name="l01026"></a>01026 
<a name="l01030"></a>01030 
<a name="l01031"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#98b68a0401e023dc61692d4b7377a4ac">01031</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#98b68a0401e023dc61692d4b7377a4ac">ConjugateGradient::set_reserve_elapsed_time_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_elapsed_time_history)
<a name="l01032"></a>01032 {
<a name="l01033"></a>01033    reserve_elapsed_time_history = new_reserve_elapsed_time_history;     
<a name="l01034"></a>01034 }
<a name="l01035"></a>01035 
<a name="l01036"></a>01036 
<a name="l01037"></a>01037 <span class="comment">// void set_reserve_generalization_evaluation_history(bool) method</span>
<a name="l01038"></a>01038 
<a name="l01042"></a>01042 
<a name="l01043"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#97eaf29840f91536f1a55878bc077882">01043</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#97eaf29840f91536f1a55878bc077882">ConjugateGradient::set_reserve_generalization_evaluation_history</a>(<span class="keyword">const</span> <span class="keywordtype">bool</span>&amp; new_reserve_generalization_evaluation_history)  
<a name="l01044"></a>01044 {
<a name="l01045"></a>01045    reserve_generalization_evaluation_history = new_reserve_generalization_evaluation_history;
<a name="l01046"></a>01046 }
<a name="l01047"></a>01047 
<a name="l01048"></a>01048 
<a name="l01049"></a>01049 <span class="comment">// void set_display_period(unsigned int) method</span>
<a name="l01050"></a>01050 
<a name="l01054"></a>01054 
<a name="l01055"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#a38aa187051cc3605a507a35d36b63be">01055</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#a38aa187051cc3605a507a35d36b63be">ConjugateGradient::set_display_period</a>(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; new_display_period)
<a name="l01056"></a>01056 {
<a name="l01057"></a>01057    <span class="comment">// Control sentence (if debug)</span>
<a name="l01058"></a>01058 
<a name="l01059"></a>01059 <span class="preprocessor">   #ifdef _DEBUG </span>
<a name="l01060"></a>01060 <span class="preprocessor"></span>     
<a name="l01061"></a>01061    <span class="keywordflow">if</span>(new_display_period &lt;= 0)
<a name="l01062"></a>01062    {
<a name="l01063"></a>01063       std::ostringstream buffer;
<a name="l01064"></a>01064 
<a name="l01065"></a>01065       buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: TrainingAlgorithm class.\n"</span>
<a name="l01066"></a>01066              &lt;&lt; <span class="stringliteral">"void set_display_period(const double&amp;) method.\n"</span>
<a name="l01067"></a>01067              &lt;&lt; <span class="stringliteral">"First training rate must be greater than 0.\n"</span>;
<a name="l01068"></a>01068 
<a name="l01069"></a>01069       <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());       
<a name="l01070"></a>01070    }
<a name="l01071"></a>01071 
<a name="l01072"></a>01072 <span class="preprocessor">   #endif</span>
<a name="l01073"></a>01073 <span class="preprocessor"></span>
<a name="l01074"></a>01074    display_period = new_display_period;
<a name="l01075"></a>01075 }
<a name="l01076"></a>01076 
<a name="l01077"></a>01077 
<a name="l01078"></a>01078 
<a name="l01079"></a>01079 <span class="comment">// double calculate_FR_parameter(const Vector&lt;double&gt;&amp;, const Vector&lt;double&gt;&amp;) const method</span>
<a name="l01080"></a>01080 
<a name="l01085"></a>01085 
<a name="l01086"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#eef9bc0433ecff10db98d87945b553ae">01086</a> <span class="keywordtype">double</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#eef9bc0433ecff10db98d87945b553ae">ConjugateGradient::calculate_FR_parameter</a>(<span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; old_gradient, <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; gradient)<span class="keyword"> const</span>
<a name="l01087"></a>01087 <span class="keyword"></span>{
<a name="l01088"></a>01088    <span class="keywordtype">double</span> FR_parameter = 0.0;
<a name="l01089"></a>01089 
<a name="l01090"></a>01090    <span class="keyword">const</span> <span class="keywordtype">double</span> numerator = gradient.<a class="code" href="class_open_n_n_1_1_vector.html#0af2b14634f3dc1cea8ce692f77eb94e">dot</a>(gradient);
<a name="l01091"></a>01091    <span class="keyword">const</span> <span class="keywordtype">double</span> denominator = old_gradient.<a class="code" href="class_open_n_n_1_1_vector.html#0af2b14634f3dc1cea8ce692f77eb94e">dot</a>(old_gradient);
<a name="l01092"></a>01092 
<a name="l01093"></a>01093    <span class="comment">// Prevent a possible division by 0</span>
<a name="l01094"></a>01094 
<a name="l01095"></a>01095    <span class="keywordflow">if</span>(denominator == 0.0)
<a name="l01096"></a>01096    {
<a name="l01097"></a>01097       FR_parameter = 0.0;
<a name="l01098"></a>01098    }
<a name="l01099"></a>01099    <span class="keywordflow">else</span>
<a name="l01100"></a>01100    {
<a name="l01101"></a>01101       FR_parameter = numerator/denominator;
<a name="l01102"></a>01102    }
<a name="l01103"></a>01103 
<a name="l01104"></a>01104    <span class="comment">// Bound the Fletcher-Reeves parameter between 0 and 1</span>
<a name="l01105"></a>01105 
<a name="l01106"></a>01106    <span class="keywordflow">if</span>(FR_parameter &lt; 0.0)
<a name="l01107"></a>01107       FR_parameter = 0.0;
<a name="l01108"></a>01108 
<a name="l01109"></a>01109    <span class="keywordflow">if</span>(FR_parameter &gt; 1.0)
<a name="l01110"></a>01110       FR_parameter = 1.0;
<a name="l01111"></a>01111 
<a name="l01112"></a>01112    <span class="keywordflow">return</span>(FR_parameter);
<a name="l01113"></a>01113 }
<a name="l01114"></a>01114 
<a name="l01115"></a>01115 
<a name="l01116"></a>01116 <span class="comment">// double calculate_PR_parameter(const Vector&lt;double&gt;&amp;, const Vector&lt;double&gt;&amp;) const method</span>
<a name="l01117"></a>01117 
<a name="l01121"></a>01121 
<a name="l01122"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#f73bcb65142c7608cca02496f6bc484b">01122</a> <span class="keywordtype">double</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#f73bcb65142c7608cca02496f6bc484b">ConjugateGradient::calculate_PR_parameter</a>(<span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; old_gradient, <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; gradient)<span class="keyword"> const</span>
<a name="l01123"></a>01123 <span class="keyword"></span>{
<a name="l01124"></a>01124    <span class="keywordtype">double</span> PR_parameter = 0.0;
<a name="l01125"></a>01125 
<a name="l01126"></a>01126    <span class="keyword">const</span> <span class="keywordtype">double</span> numerator = (gradient-old_gradient).dot(gradient);
<a name="l01127"></a>01127    <span class="keyword">const</span> <span class="keywordtype">double</span> denominator = old_gradient.<a class="code" href="class_open_n_n_1_1_vector.html#0af2b14634f3dc1cea8ce692f77eb94e">dot</a>(old_gradient);
<a name="l01128"></a>01128 
<a name="l01129"></a>01129    <span class="comment">// Prevent a possible division by 0</span>
<a name="l01130"></a>01130 
<a name="l01131"></a>01131    <span class="keywordflow">if</span>(denominator == 0.0)
<a name="l01132"></a>01132    {
<a name="l01133"></a>01133       PR_parameter = 0.0;
<a name="l01134"></a>01134    }
<a name="l01135"></a>01135    <span class="keywordflow">else</span>
<a name="l01136"></a>01136    {
<a name="l01137"></a>01137       PR_parameter = numerator/denominator;
<a name="l01138"></a>01138    }
<a name="l01139"></a>01139 
<a name="l01140"></a>01140    <span class="comment">// Bound the Polak-Ribiere parameter between 0 and 1</span>
<a name="l01141"></a>01141 
<a name="l01142"></a>01142    <span class="keywordflow">if</span>(PR_parameter &lt; 0.0)
<a name="l01143"></a>01143       PR_parameter = 0.0;
<a name="l01144"></a>01144 
<a name="l01145"></a>01145    <span class="keywordflow">if</span>(PR_parameter &gt; 1.0)
<a name="l01146"></a>01146       PR_parameter = 1.0;
<a name="l01147"></a>01147 
<a name="l01148"></a>01148    <span class="keywordflow">return</span>(PR_parameter);
<a name="l01149"></a>01149 }
<a name="l01150"></a>01150 
<a name="l01151"></a>01151 
<a name="l01152"></a>01152 <span class="comment">// Vector&lt;double&gt; calculate_PR_training_direction(const Vector&lt;double&gt;&amp;, const Vector&lt;double&gt;&amp;, const Vector&lt;double&gt;&amp;) const method</span>
<a name="l01153"></a>01153 
<a name="l01158"></a>01158 
<a name="l01159"></a>01159 <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#349d877b87f39e34f2041ca12578493f">ConjugateGradient::calculate_PR_training_direction</a>
<a name="l01160"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#349d877b87f39e34f2041ca12578493f">01160</a> (<span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; old_gradient, <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; gradient, <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; old_training_direction) <span class="keyword">const</span>
<a name="l01161"></a>01161 {
<a name="l01162"></a>01162    <span class="keyword">const</span> <span class="keywordtype">double</span> PR_parameter = calculate_PR_parameter(old_gradient, gradient);
<a name="l01163"></a>01163 
<a name="l01164"></a>01164    <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> gradient_descent_term = calculate_gradient_descent_training_direction(gradient);
<a name="l01165"></a>01165    <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> conjugate_direction_term = old_training_direction*PR_parameter;
<a name="l01166"></a>01166 
<a name="l01167"></a>01167    <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> PR_training_direction = gradient_descent_term + conjugate_direction_term;
<a name="l01168"></a>01168 
<a name="l01169"></a>01169    <span class="keyword">const</span> <span class="keywordtype">double</span> PR_training_direction_norm = PR_training_direction.<a class="code" href="class_open_n_n_1_1_vector.html#17c63b7346189c7337602b0aac5f529f" title="This element returns the vector norm.">calculate_norm</a>();   
<a name="l01170"></a>01170 
<a name="l01171"></a>01171    <span class="keywordflow">return</span>(PR_training_direction/PR_training_direction_norm);
<a name="l01172"></a>01172 }
<a name="l01173"></a>01173 
<a name="l01174"></a>01174 
<a name="l01175"></a>01175 <span class="comment">// Vector&lt;double&gt; calculate_FR_training_direction(const Vector&lt;double&gt;&amp;, const Vector&lt;double&gt;&amp;, const Vector&lt;double&gt;&amp;) const method</span>
<a name="l01176"></a>01176 
<a name="l01181"></a>01181 
<a name="l01182"></a>01182 <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#eb3e308b272d1bffa46c96cbb1160be7">ConjugateGradient::calculate_FR_training_direction</a>
<a name="l01183"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#eb3e308b272d1bffa46c96cbb1160be7">01183</a> (<span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; old_gradient, <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; gradient, <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; old_training_direction) <span class="keyword">const</span>
<a name="l01184"></a>01184 {
<a name="l01185"></a>01185    <span class="keyword">const</span> <span class="keywordtype">double</span> FR_parameter = calculate_FR_parameter(old_gradient, gradient);
<a name="l01186"></a>01186 
<a name="l01187"></a>01187    <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> gradient_descent_term = calculate_gradient_descent_training_direction(gradient);
<a name="l01188"></a>01188    <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> conjugate_direction_term = old_training_direction*FR_parameter;
<a name="l01189"></a>01189 
<a name="l01190"></a>01190    <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> FR_training_direction = gradient_descent_term + conjugate_direction_term;
<a name="l01191"></a>01191 
<a name="l01192"></a>01192    <span class="keyword">const</span> <span class="keywordtype">double</span> FR_training_direction_norm = FR_training_direction.<a class="code" href="class_open_n_n_1_1_vector.html#17c63b7346189c7337602b0aac5f529f" title="This element returns the vector norm.">calculate_norm</a>();   
<a name="l01193"></a>01193 
<a name="l01194"></a>01194    <span class="keywordflow">return</span>(FR_training_direction/FR_training_direction_norm);
<a name="l01195"></a>01195 }
<a name="l01196"></a>01196 
<a name="l01197"></a>01197 
<a name="l01198"></a>01198 <span class="comment">// Vector&lt;double&gt; calculate_training_direction(const Vector&lt;double&gt;&amp;, const Vector&lt;double&gt;&amp;, const Vector&lt;double&gt;&amp;) const method</span>
<a name="l01199"></a>01199 
<a name="l01204"></a>01204 
<a name="l01205"></a>01205 <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#24e0e461557d5f855c97dbeedf43d0cb">ConjugateGradient::calculate_training_direction</a>
<a name="l01206"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#24e0e461557d5f855c97dbeedf43d0cb">01206</a> (<span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; old_gradient, <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; gradient, <span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; old_training_direction) <span class="keyword">const</span>
<a name="l01207"></a>01207 {
<a name="l01208"></a>01208    <span class="keywordflow">switch</span>(training_direction_method)
<a name="l01209"></a>01209    {
<a name="l01210"></a>01210       <span class="keywordflow">case</span> FR:
<a name="l01211"></a>01211       {
<a name="l01212"></a>01212          <span class="keywordflow">return</span>(calculate_FR_training_direction(old_gradient, gradient, old_training_direction));
<a name="l01213"></a>01213       }    
<a name="l01214"></a>01214       <span class="keywordflow">break</span>;
<a name="l01215"></a>01215 
<a name="l01216"></a>01216       <span class="keywordflow">case</span> PR:
<a name="l01217"></a>01217       {
<a name="l01218"></a>01218          <span class="keywordflow">return</span>(calculate_PR_training_direction(old_gradient, gradient, old_training_direction));
<a name="l01219"></a>01219       }            
<a name="l01220"></a>01220       <span class="keywordflow">break</span>;
<a name="l01221"></a>01221 
<a name="l01222"></a>01222           <span class="keywordflow">default</span>:
<a name="l01223"></a>01223       {
<a name="l01224"></a>01224          std::ostringstream buffer;
<a name="l01225"></a>01225 
<a name="l01226"></a>01226          buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: ConjugateGradient class.\n"</span>
<a name="l01227"></a>01227                 &lt;&lt; <span class="stringliteral">"Vector&lt;double&gt; calculate_training_direction(const Vector&lt;double&gt;&amp;, const Vector&lt;double&gt;&amp;, const Vector&lt;double&gt;&amp;) const method.\n"</span>
<a name="l01228"></a>01228                                 &lt;&lt; <span class="stringliteral">"Unknown training direction method: "</span> &lt;&lt; training_direction_method &lt;&lt; <span class="stringliteral">".\n"</span>;
<a name="l01229"></a>01229    
<a name="l01230"></a>01230          <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());           
<a name="l01231"></a>01231           }
<a name="l01232"></a>01232           <span class="keywordflow">break</span>;
<a name="l01233"></a>01233    }
<a name="l01234"></a>01234 }
<a name="l01235"></a>01235 
<a name="l01236"></a>01236 
<a name="l01237"></a>01237 <span class="comment">// Vector&lt;double&gt; calculate_gradient_descent_training_direction(const Vector&lt;double&gt;&amp;) const method</span>
<a name="l01238"></a>01238 
<a name="l01241"></a>01241 
<a name="l01242"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#42e7f404e32cceabb10a25d2b66f6ca4">01242</a> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#42e7f404e32cceabb10a25d2b66f6ca4">ConjugateGradient::calculate_gradient_descent_training_direction</a>(<span class="keyword">const</span> <a class="code" href="class_open_n_n_1_1_vector.html">Vector&lt;double&gt;</a>&amp; gradient)<span class="keyword"> const</span>
<a name="l01243"></a>01243 <span class="keyword"></span>{
<a name="l01244"></a>01244    <span class="keywordtype">double</span> gradient_norm = gradient.<a class="code" href="class_open_n_n_1_1_vector.html#17c63b7346189c7337602b0aac5f529f" title="This element returns the vector norm.">calculate_norm</a>(); 
<a name="l01245"></a>01245 
<a name="l01246"></a>01246    <span class="keywordflow">return</span>(gradient*(-1.0)/gradient_norm);   
<a name="l01247"></a>01247 }
<a name="l01248"></a>01248 
<a name="l01249"></a>01249 
<a name="l01250"></a>01250 <span class="comment">// void resize_training_history(const unsigned int&amp;) method</span>
<a name="l01251"></a>01251 
<a name="l01254"></a>01254 
<a name="l01255"></a><a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#65d91c3d495fa7b28200c2c8acf421d1">01255</a> <span class="keywordtype">void</span> <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#65d91c3d495fa7b28200c2c8acf421d1">ConjugateGradient::ConjugateGradientResults::resize_training_history</a>(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; new_size)
<a name="l01256"></a>01256 {
<a name="l01257"></a>01257    <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#9fda92c8023c879c9e82ef173c158115" title="History of the neural network parameters over the training epochs.">parameters_history</a>.resize(new_size);
<a name="l01258"></a>01258    <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#62e07dd5df528efedcaab021348398d4" title="History of the parameters norm over the training epochs.">parameters_norm_history</a>.resize(new_size);
<a name="l01259"></a>01259 
<a name="l01260"></a>01260    <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#9db463e6016b05d38694c15566249db7" title="History of the performance function evaluation over the training epochs.">evaluation_history</a>.resize(new_size);
<a name="l01261"></a>01261    <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#66e2af942d34f021162b7c432fb03168" title="History of the generalization evaluation over the training epochs.">generalization_evaluation_history</a>.resize(new_size);
<a name="l01262"></a>01262    <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#f5d8e446d06ef90422f08f7041ba743e" title="History of the performance function gradient over the training epochs.">gradient_history</a>.resize(new_size);
<a name="l01263"></a>01263    <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#7d0e932edbf2d1ee6f4ffd7262c34b13" title="History of the gradient norm over the training epochs.">gradient_norm_history</a>.resize(new_size);
<a name="l01264"></a>01264    
<a name="l01265"></a>01265    <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#8758ef21a627afacc592aee18a6c8099" title="History of the conjugate gradient training direction over the training epochs.">training_direction_history</a>.resize(new_size);
<a name="l01266"></a>01266    <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#37665e71844dad0e7e5a1fca4c597ed9" title="History of the training rate over the training epochs.">training_rate_history</a>.resize(new_size);
<a name="l01267"></a>01267    <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#fbd4287ee5d7c3fbac4bee003a7cda27" title="History of the elapsed time over the training epochs.">elapsed_time_history</a>.resize(new_size);
<a name="l01268"></a>01268 }
<a name="l01269"></a>01269 
<a name="l01270"></a>01270 
<a name="l01271"></a>01271 <span class="comment">// std::string to_string(void) const method</span>
<a name="l01272"></a>01272 
<a name="l01273"></a><a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#9a4fed1efd1dc88439d04136ec4c9efe">01273</a> std::string <a class="code" href="class_open_n_n_1_1_training_algorithm.html#5caf204457d2e1898c9e6782c11a6dc8" title="This method returns a default string representation of a training algorithm.">ConjugateGradient::ConjugateGradientResults::to_string</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l01274"></a>01274 <span class="keyword"></span>{
<a name="l01275"></a>01275    std::ostringstream buffer;
<a name="l01276"></a>01276 
<a name="l01277"></a>01277    <span class="comment">// Parameters history</span>
<a name="l01278"></a>01278 
<a name="l01279"></a>01279    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#9fda92c8023c879c9e82ef173c158115" title="History of the neural network parameters over the training epochs.">parameters_history</a>.empty())
<a name="l01280"></a>01280    {
<a name="l01281"></a>01281            <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#9fda92c8023c879c9e82ef173c158115" title="History of the neural network parameters over the training epochs.">parameters_history</a>[0].empty())
<a name="l01282"></a>01282            {
<a name="l01283"></a>01283           buffer &lt;&lt; <span class="stringliteral">"% Parameters history:\n"</span>
<a name="l01284"></a>01284                  &lt;&lt; <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#9fda92c8023c879c9e82ef173c158115" title="History of the neural network parameters over the training epochs.">parameters_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01285"></a>01285            }
<a name="l01286"></a>01286    }
<a name="l01287"></a>01287 
<a name="l01288"></a>01288    <span class="comment">// Parameters norm history</span>
<a name="l01289"></a>01289 
<a name="l01290"></a>01290    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#62e07dd5df528efedcaab021348398d4" title="History of the parameters norm over the training epochs.">parameters_norm_history</a>.empty())
<a name="l01291"></a>01291    {
<a name="l01292"></a>01292        buffer &lt;&lt; <span class="stringliteral">"% Parameters norm history:\n"</span>
<a name="l01293"></a>01293               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#62e07dd5df528efedcaab021348398d4" title="History of the parameters norm over the training epochs.">parameters_norm_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01294"></a>01294    }
<a name="l01295"></a>01295 
<a name="l01296"></a>01296    <span class="comment">// Evaluation history</span>
<a name="l01297"></a>01297 
<a name="l01298"></a>01298    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#9db463e6016b05d38694c15566249db7" title="History of the performance function evaluation over the training epochs.">evaluation_history</a>.empty())
<a name="l01299"></a>01299    {
<a name="l01300"></a>01300        buffer &lt;&lt; <span class="stringliteral">"% Evaluation history:\n"</span>
<a name="l01301"></a>01301               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#9db463e6016b05d38694c15566249db7" title="History of the performance function evaluation over the training epochs.">evaluation_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01302"></a>01302    }
<a name="l01303"></a>01303 
<a name="l01304"></a>01304    <span class="comment">// Generalization evaluation history</span>
<a name="l01305"></a>01305 
<a name="l01306"></a>01306    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#66e2af942d34f021162b7c432fb03168" title="History of the generalization evaluation over the training epochs.">generalization_evaluation_history</a>.empty())
<a name="l01307"></a>01307    {
<a name="l01308"></a>01308        buffer &lt;&lt; <span class="stringliteral">"% Generalization evaluation history:\n"</span>
<a name="l01309"></a>01309               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#66e2af942d34f021162b7c432fb03168" title="History of the generalization evaluation over the training epochs.">generalization_evaluation_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01310"></a>01310    }
<a name="l01311"></a>01311 
<a name="l01312"></a>01312    <span class="comment">// Gradient history</span>
<a name="l01313"></a>01313 
<a name="l01314"></a>01314    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#f5d8e446d06ef90422f08f7041ba743e" title="History of the performance function gradient over the training epochs.">gradient_history</a>.empty())
<a name="l01315"></a>01315    {
<a name="l01316"></a>01316            <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#f5d8e446d06ef90422f08f7041ba743e" title="History of the performance function gradient over the training epochs.">gradient_history</a>[0].empty())
<a name="l01317"></a>01317            {
<a name="l01318"></a>01318           buffer &lt;&lt; <span class="stringliteral">"% Gradient history:\n"</span>
<a name="l01319"></a>01319                  &lt;&lt; <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#f5d8e446d06ef90422f08f7041ba743e" title="History of the performance function gradient over the training epochs.">gradient_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01320"></a>01320            }
<a name="l01321"></a>01321    }
<a name="l01322"></a>01322 
<a name="l01323"></a>01323    <span class="comment">// Gradient norm history</span>
<a name="l01324"></a>01324 
<a name="l01325"></a>01325    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#7d0e932edbf2d1ee6f4ffd7262c34b13" title="History of the gradient norm over the training epochs.">gradient_norm_history</a>.empty())
<a name="l01326"></a>01326    {
<a name="l01327"></a>01327        buffer &lt;&lt; <span class="stringliteral">"% Gradient norm history:\n"</span>
<a name="l01328"></a>01328               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#7d0e932edbf2d1ee6f4ffd7262c34b13" title="History of the gradient norm over the training epochs.">gradient_norm_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01329"></a>01329    }
<a name="l01330"></a>01330 
<a name="l01331"></a>01331    <span class="comment">// Training direction history</span>
<a name="l01332"></a>01332 
<a name="l01333"></a>01333    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#8758ef21a627afacc592aee18a6c8099" title="History of the conjugate gradient training direction over the training epochs.">training_direction_history</a>.empty())
<a name="l01334"></a>01334    {
<a name="l01335"></a>01335            <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#8758ef21a627afacc592aee18a6c8099" title="History of the conjugate gradient training direction over the training epochs.">training_direction_history</a>[0].empty())
<a name="l01336"></a>01336            {
<a name="l01337"></a>01337           buffer &lt;&lt; <span class="stringliteral">"% Training direction history:\n"</span>
<a name="l01338"></a>01338                  &lt;&lt; <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#8758ef21a627afacc592aee18a6c8099" title="History of the conjugate gradient training direction over the training epochs.">training_direction_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01339"></a>01339            }
<a name="l01340"></a>01340    }
<a name="l01341"></a>01341 
<a name="l01342"></a>01342    <span class="comment">// Training rate history</span>
<a name="l01343"></a>01343 
<a name="l01344"></a>01344    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#37665e71844dad0e7e5a1fca4c597ed9" title="History of the training rate over the training epochs.">training_rate_history</a>.empty())
<a name="l01345"></a>01345    {
<a name="l01346"></a>01346        buffer &lt;&lt; <span class="stringliteral">"% Training rate history:\n"</span>
<a name="l01347"></a>01347               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#37665e71844dad0e7e5a1fca4c597ed9" title="History of the training rate over the training epochs.">training_rate_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01348"></a>01348    }
<a name="l01349"></a>01349 
<a name="l01350"></a>01350    <span class="comment">// Elapsed time history</span>
<a name="l01351"></a>01351 
<a name="l01352"></a>01352    <span class="keywordflow">if</span>(!<a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#fbd4287ee5d7c3fbac4bee003a7cda27" title="History of the elapsed time over the training epochs.">elapsed_time_history</a>.empty())
<a name="l01353"></a>01353    {
<a name="l01354"></a>01354        buffer &lt;&lt; <span class="stringliteral">"% Elapsed time history:\n"</span>
<a name="l01355"></a>01355               &lt;&lt; <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html#fbd4287ee5d7c3fbac4bee003a7cda27" title="History of the elapsed time over the training epochs.">elapsed_time_history</a> &lt;&lt; <span class="stringliteral">"\n"</span>; 
<a name="l01356"></a>01356    }
<a name="l01357"></a>01357 
<a name="l01358"></a>01358    <span class="keywordflow">return</span>(buffer.str());
<a name="l01359"></a>01359 }
<a name="l01360"></a>01360 
<a name="l01361"></a>01361 
<a name="l01362"></a>01362 <span class="comment">// ConjugateGradientResults* perform_training(void) method</span>
<a name="l01363"></a>01363 
<a name="l01366"></a>01366 
<a name="l01367"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#21c8810233ae2c5da6f6e5b923759d4f">01367</a> <a class="code" href="struct_open_n_n_1_1_conjugate_gradient_1_1_conjugate_gradient_results.html">ConjugateGradient::ConjugateGradientResults</a>* <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#21c8810233ae2c5da6f6e5b923759d4f">ConjugateGradient::perform_training</a>(<span class="keywordtype">void</span>)
<a name="l01368"></a>01368 {
<a name="l01369"></a>01369    std::ostringstream buffer;
<a name="l01370"></a>01370 
<a name="l01371"></a>01371    buffer &lt;&lt; <span class="stringliteral">"OpenNN Exception: ConjugateGradient class.\n"</span>
<a name="l01372"></a>01372           &lt;&lt; <span class="stringliteral">"ConjugateGradientResults* perform_training(void) method.\n"</span>
<a name="l01373"></a>01373           &lt;&lt; <span class="stringliteral">"This method is under development.\n"</span>;
<a name="l01374"></a>01374 
<a name="l01375"></a>01375    <span class="keywordflow">throw</span> std::logic_error(buffer.str().c_str());          
<a name="l01376"></a>01376 
<a name="l01377"></a>01377 <span class="comment">/*</span>
<a name="l01378"></a>01378 <span class="comment">   // Control sentence (if debug)</span>
<a name="l01379"></a>01379 <span class="comment"></span>
<a name="l01380"></a>01380 <span class="comment">   #ifdef _DEBUG </span>
<a name="l01381"></a>01381 <span class="comment"></span>
<a name="l01382"></a>01382 <span class="comment">   check();</span>
<a name="l01383"></a>01383 <span class="comment"></span>
<a name="l01384"></a>01384 <span class="comment">   #endif</span>
<a name="l01385"></a>01385 <span class="comment"></span>
<a name="l01386"></a>01386 <span class="comment">   // Start training</span>
<a name="l01387"></a>01387 <span class="comment"></span>
<a name="l01388"></a>01388 <span class="comment">   if(display)</span>
<a name="l01389"></a>01389 <span class="comment">   {</span>
<a name="l01390"></a>01390 <span class="comment">      std::cout &lt;&lt; "Training with conjugate gradient...\n";</span>
<a name="l01391"></a>01391 <span class="comment">   }</span>
<a name="l01392"></a>01392 <span class="comment"></span>
<a name="l01393"></a>01393 <span class="comment">   ConjugateGradientResults* conjugate_gradient_training_results_pointer = new ConjugateGradientResults;</span>
<a name="l01394"></a>01394 <span class="comment"></span>
<a name="l01395"></a>01395 <span class="comment">   // Elapsed time</span>
<a name="l01396"></a>01396 <span class="comment"></span>
<a name="l01397"></a>01397 <span class="comment">   time_t beginning_time, current_time;</span>
<a name="l01398"></a>01398 <span class="comment">   time(&amp;beginning_time);</span>
<a name="l01399"></a>01399 <span class="comment">   double elapsed_time;</span>
<a name="l01400"></a>01400 <span class="comment"></span>
<a name="l01401"></a>01401 <span class="comment">   // Neural network stuff</span>
<a name="l01402"></a>01402 <span class="comment"></span>
<a name="l01403"></a>01403 <span class="comment">   NeuralNetwork* neural_network_pointer = performance_functional_pointer-&gt;get_neural_network_pointer();</span>
<a name="l01404"></a>01404 <span class="comment"></span>
<a name="l01405"></a>01405 <span class="comment">   const unsigned int parameters_number = neural_network_pointer-&gt;count_parameters_number();</span>
<a name="l01406"></a>01406 <span class="comment"></span>
<a name="l01407"></a>01407 <span class="comment">   Vector&lt;double&gt; parameters = neural_network_pointer-&gt;arrange_parameters();</span>
<a name="l01408"></a>01408 <span class="comment">   double parameters_norm;</span>
<a name="l01409"></a>01409 <span class="comment"></span>
<a name="l01410"></a>01410 <span class="comment">   // Performance functional stuff</span>
<a name="l01411"></a>01411 <span class="comment"></span>
<a name="l01412"></a>01412 <span class="comment">   double performance = 0.0;</span>
<a name="l01413"></a>01413 <span class="comment">   double old_performance = 0.0;</span>
<a name="l01414"></a>01414 <span class="comment">   double performance_increase = 0.0;</span>
<a name="l01415"></a>01415 <span class="comment">      </span>
<a name="l01416"></a>01416 <span class="comment">   Vector&lt;double&gt; gradient(parameters_number);</span>
<a name="l01417"></a>01417 <span class="comment">   double gradient_norm;</span>
<a name="l01418"></a>01418 <span class="comment"></span>
<a name="l01419"></a>01419 <span class="comment">   double generalization_evaluation = 0.0; </span>
<a name="l01420"></a>01420 <span class="comment">   double old_generalization_evaluation = 0.0;</span>
<a name="l01421"></a>01421 <span class="comment"></span>
<a name="l01422"></a>01422 <span class="comment">   std::string information;</span>
<a name="l01423"></a>01423 <span class="comment"></span>
<a name="l01424"></a>01424 <span class="comment">   // Training algorithm stuff </span>
<a name="l01425"></a>01425 <span class="comment"></span>
<a name="l01426"></a>01426 <span class="comment">   const double&amp; first_training_rate = training_rate_algorithm.get_first_training_rate();</span>
<a name="l01427"></a>01427 <span class="comment"></span>
<a name="l01428"></a>01428 <span class="comment">   Vector&lt;double&gt; parameters_increment(parameters_number);</span>
<a name="l01429"></a>01429 <span class="comment">   double parameters_increment_norm;</span>
<a name="l01430"></a>01430 <span class="comment"></span>
<a name="l01431"></a>01431 <span class="comment">   Vector&lt;double&gt; old_gradient(parameters_number);</span>
<a name="l01432"></a>01432 <span class="comment">   Vector&lt;double&gt; training_direction(parameters_number);</span>
<a name="l01433"></a>01433 <span class="comment">   Vector&lt;double&gt; old_training_direction(parameters_number);</span>
<a name="l01434"></a>01434 <span class="comment"></span>
<a name="l01435"></a>01435 <span class="comment">   double training_slope;</span>
<a name="l01436"></a>01436 <span class="comment"></span>
<a name="l01437"></a>01437 <span class="comment">   double initial_training_rate = 0.0;</span>
<a name="l01438"></a>01438 <span class="comment">   double training_rate = 0.0;</span>
<a name="l01439"></a>01439 <span class="comment">   double old_training_rate = 0.0;</span>
<a name="l01440"></a>01440 <span class="comment"></span>
<a name="l01441"></a>01441 <span class="comment">   Vector&lt;double&gt; directional_point(2);</span>
<a name="l01442"></a>01442 <span class="comment">   directional_point[1] = 0.0;</span>
<a name="l01443"></a>01443 <span class="comment">   directional_point[0] = 0.0;</span>
<a name="l01444"></a>01444 <span class="comment"></span>
<a name="l01445"></a>01445 <span class="comment">   bool stop_training = false;</span>
<a name="l01446"></a>01446 <span class="comment"></span>
<a name="l01447"></a>01447 <span class="comment">   unsigned int generalization_evaluation_decreases_count = 0;</span>
<a name="l01448"></a>01448 <span class="comment">   </span>
<a name="l01449"></a>01449 <span class="comment">   // Main loop    </span>
<a name="l01450"></a>01450 <span class="comment">   </span>
<a name="l01451"></a>01451 <span class="comment">   for(unsigned int epoch = 0; epoch &lt;= maximum_epochs_number; epoch++)</span>
<a name="l01452"></a>01452 <span class="comment">   {</span>
<a name="l01453"></a>01453 <span class="comment">      // Multilayer perceptron </span>
<a name="l01454"></a>01454 <span class="comment"></span>
<a name="l01455"></a>01455 <span class="comment">      parameters = neural_network_pointer-&gt;arrange_parameters();</span>
<a name="l01456"></a>01456 <span class="comment"></span>
<a name="l01457"></a>01457 <span class="comment">      parameters_norm = parameters.calculate_norm();</span>
<a name="l01458"></a>01458 <span class="comment"></span>
<a name="l01459"></a>01459 <span class="comment">      if(parameters_norm &gt;= error_parameters_norm)</span>
<a name="l01460"></a>01460 <span class="comment">      {</span>
<a name="l01461"></a>01461 <span class="comment">         std::ostringstream buffer;</span>
<a name="l01462"></a>01462 <span class="comment"></span>
<a name="l01463"></a>01463 <span class="comment">         buffer &lt;&lt; "OpenNN Exception: ConjugateGradient class.\n"</span>
<a name="l01464"></a>01464 <span class="comment">                &lt;&lt; "ConjugateGradientResults* perform_training(void) method.\n"</span>
<a name="l01465"></a>01465 <span class="comment">                &lt;&lt; "Parameters norm is greater than error parameters norm.\n";</span>
<a name="l01466"></a>01466 <span class="comment"> </span>
<a name="l01467"></a>01467 <span class="comment">         throw std::logic_error(buffer.str().c_str());           </span>
<a name="l01468"></a>01468 <span class="comment">      }</span>
<a name="l01469"></a>01469 <span class="comment">      else if(display &amp;&amp; parameters_norm &gt;= warning_parameters_norm)</span>
<a name="l01470"></a>01470 <span class="comment">      {</span>
<a name="l01471"></a>01471 <span class="comment">         std::cout &lt;&lt; "OpenNN Warning: Parameters norm is " &lt;&lt; parameters_norm &lt;&lt; ".\n";          </span>
<a name="l01472"></a>01472 <span class="comment">      }</span>
<a name="l01473"></a>01473 <span class="comment"></span>
<a name="l01474"></a>01474 <span class="comment">      // Performance functional stuff</span>
<a name="l01475"></a>01475 <span class="comment">    </span>
<a name="l01476"></a>01476 <span class="comment">      if(epoch == 0)</span>
<a name="l01477"></a>01477 <span class="comment">      {      </span>
<a name="l01478"></a>01478 <span class="comment">         performance = performance_functional_pointer-&gt;calculate_evaluation();</span>
<a name="l01479"></a>01479 <span class="comment">         performance_increase = 0.0; </span>
<a name="l01480"></a>01480 <span class="comment">      }</span>
<a name="l01481"></a>01481 <span class="comment">      else</span>
<a name="l01482"></a>01482 <span class="comment">      {</span>
<a name="l01483"></a>01483 <span class="comment">         performance = directional_point[1];</span>
<a name="l01484"></a>01484 <span class="comment">         performance_increase = old_performance - performance; </span>
<a name="l01485"></a>01485 <span class="comment">      }</span>
<a name="l01486"></a>01486 <span class="comment"></span>
<a name="l01487"></a>01487 <span class="comment">      gradient = performance_functional_pointer-&gt;calculate_gradient();</span>
<a name="l01488"></a>01488 <span class="comment"></span>
<a name="l01489"></a>01489 <span class="comment">      gradient_norm = gradient.calculate_norm();</span>
<a name="l01490"></a>01490 <span class="comment"></span>
<a name="l01491"></a>01491 <span class="comment">      if(display &amp;&amp; gradient_norm &gt;= warning_gradient_norm)</span>
<a name="l01492"></a>01492 <span class="comment">      {</span>
<a name="l01493"></a>01493 <span class="comment">         std::cout &lt;&lt; "OpenNN Warning: Gradient norm is " &lt;&lt; gradient_norm &lt;&lt; ".\n";          </span>
<a name="l01494"></a>01494 <span class="comment">      }</span>
<a name="l01495"></a>01495 <span class="comment"></span>
<a name="l01496"></a>01496 <span class="comment">      generalization_evaluation = performance_functional_pointer-&gt;calculate_generalization_evaluation();</span>
<a name="l01497"></a>01497 <span class="comment"></span>
<a name="l01498"></a>01498 <span class="comment">          if(epoch != 0 &amp;&amp; generalization_evaluation &gt; old_generalization_evaluation)</span>
<a name="l01499"></a>01499 <span class="comment">          {</span>
<a name="l01500"></a>01500 <span class="comment">             generalization_evaluation_decreases_count++;         </span>
<a name="l01501"></a>01501 <span class="comment">          }</span>
<a name="l01502"></a>01502 <span class="comment"></span>
<a name="l01503"></a>01503 <span class="comment">      // Training algorithm </span>
<a name="l01504"></a>01504 <span class="comment"></span>
<a name="l01505"></a>01505 <span class="comment">      if(epoch == 0 || epoch % parameters_number == 0)</span>
<a name="l01506"></a>01506 <span class="comment">      {</span>
<a name="l01507"></a>01507 <span class="comment">         // Gradient descent training direction</span>
<a name="l01508"></a>01508 <span class="comment"></span>
<a name="l01509"></a>01509 <span class="comment">         training_direction = calculate_gradient_descent_training_direction(gradient);</span>
<a name="l01510"></a>01510 <span class="comment">      }</span>
<a name="l01511"></a>01511 <span class="comment">      else if(fabs(old_gradient.dot(gradient)) &gt;= 0.2*gradient.dot(gradient)) // Powell-Bealle restarts</span>
<a name="l01512"></a>01512 <span class="comment">      {</span>
<a name="l01513"></a>01513 <span class="comment">         // Gradient descent training direction</span>
<a name="l01514"></a>01514 <span class="comment"></span>
<a name="l01515"></a>01515 <span class="comment">         training_direction = calculate_gradient_descent_training_direction(gradient);    </span>
<a name="l01516"></a>01516 <span class="comment">      }</span>
<a name="l01517"></a>01517 <span class="comment">      else</span>
<a name="l01518"></a>01518 <span class="comment">      {</span>
<a name="l01519"></a>01519 <span class="comment">         // Conjugate gradient training direction</span>
<a name="l01520"></a>01520 <span class="comment"></span>
<a name="l01521"></a>01521 <span class="comment">         training_direction = calculate_training_direction(old_gradient, gradient, old_training_direction);</span>
<a name="l01522"></a>01522 <span class="comment">      }</span>
<a name="l01523"></a>01523 <span class="comment"></span>
<a name="l01524"></a>01524 <span class="comment">      // Calculate performance training_slope</span>
<a name="l01525"></a>01525 <span class="comment"></span>
<a name="l01526"></a>01526 <span class="comment">      training_slope = (gradient/gradient_norm).dot(training_direction);</span>
<a name="l01527"></a>01527 <span class="comment"></span>
<a name="l01528"></a>01528 <span class="comment">      // Check for a descent direction </span>
<a name="l01529"></a>01529 <span class="comment"></span>
<a name="l01530"></a>01530 <span class="comment">      if(training_slope &gt;= 0.0)</span>
<a name="l01531"></a>01531 <span class="comment">      {</span>
<a name="l01532"></a>01532 <span class="comment">         // Reset training direction</span>
<a name="l01533"></a>01533 <span class="comment"></span>
<a name="l01534"></a>01534 <span class="comment">         training_direction = calculate_gradient_descent_training_direction(gradient);</span>
<a name="l01535"></a>01535 <span class="comment">      }</span>
<a name="l01536"></a>01536 <span class="comment"></span>
<a name="l01537"></a>01537 <span class="comment">      // Get initial training rate</span>
<a name="l01538"></a>01538 <span class="comment"></span>
<a name="l01539"></a>01539 <span class="comment">      if(epoch == 0)</span>
<a name="l01540"></a>01540 <span class="comment">      {</span>
<a name="l01541"></a>01541 <span class="comment">         initial_training_rate = first_training_rate;</span>
<a name="l01542"></a>01542 <span class="comment">      }</span>
<a name="l01543"></a>01543 <span class="comment">      else</span>
<a name="l01544"></a>01544 <span class="comment">      {</span>
<a name="l01545"></a>01545 <span class="comment">         initial_training_rate = old_training_rate;</span>
<a name="l01546"></a>01546 <span class="comment">      }</span>
<a name="l01547"></a>01547 <span class="comment"></span>
<a name="l01548"></a>01548 <span class="comment">      directional_point = training_rate_algorithm.calculate_directional_point(performance, training_direction, initial_training_rate);</span>
<a name="l01549"></a>01549 <span class="comment"></span>
<a name="l01550"></a>01550 <span class="comment">          training_rate = directional_point[0];</span>
<a name="l01551"></a>01551 <span class="comment"></span>
<a name="l01552"></a>01552 <span class="comment">      if(epoch != 0 &amp;&amp; training_rate &lt; 1.0e-99)</span>
<a name="l01553"></a>01553 <span class="comment">      {</span>
<a name="l01554"></a>01554 <span class="comment">         // Reset training direction</span>
<a name="l01555"></a>01555 <span class="comment"></span>
<a name="l01556"></a>01556 <span class="comment">         training_direction = calculate_gradient_descent_training_direction(gradient);         </span>
<a name="l01557"></a>01557 <span class="comment"></span>
<a name="l01558"></a>01558 <span class="comment">         directional_point = training_rate_algorithm.calculate_directional_point(performance, training_direction, first_training_rate);</span>
<a name="l01559"></a>01559 <span class="comment"></span>
<a name="l01560"></a>01560 <span class="comment">                 training_rate = directional_point[0];</span>
<a name="l01561"></a>01561 <span class="comment">      }</span>
<a name="l01562"></a>01562 <span class="comment"></span>
<a name="l01563"></a>01563 <span class="comment">      parameters_increment = training_direction*training_rate;</span>
<a name="l01564"></a>01564 <span class="comment">      parameters_increment_norm = parameters_increment.calculate_norm();</span>
<a name="l01565"></a>01565 <span class="comment">      </span>
<a name="l01566"></a>01566 <span class="comment">      // Elapsed time</span>
<a name="l01567"></a>01567 <span class="comment"></span>
<a name="l01568"></a>01568 <span class="comment">      time(&amp;current_time);</span>
<a name="l01569"></a>01569 <span class="comment">      elapsed_time = difftime(current_time, beginning_time);</span>
<a name="l01570"></a>01570 <span class="comment"></span>
<a name="l01571"></a>01571 <span class="comment">      // Training history multilayer perceptron </span>
<a name="l01572"></a>01572 <span class="comment"></span>
<a name="l01573"></a>01573 <span class="comment">      if(reserve_parameters_history)</span>
<a name="l01574"></a>01574 <span class="comment">      {</span>
<a name="l01575"></a>01575 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;parameters_history[epoch] = parameters;                                </span>
<a name="l01576"></a>01576 <span class="comment">      }</span>
<a name="l01577"></a>01577 <span class="comment"></span>
<a name="l01578"></a>01578 <span class="comment">      if(reserve_parameters_norm_history)</span>
<a name="l01579"></a>01579 <span class="comment">      {</span>
<a name="l01580"></a>01580 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;parameters_norm_history[epoch] = parameters_norm; </span>
<a name="l01581"></a>01581 <span class="comment">      }</span>
<a name="l01582"></a>01582 <span class="comment"></span>
<a name="l01583"></a>01583 <span class="comment">      // Training history performance functional</span>
<a name="l01584"></a>01584 <span class="comment"></span>
<a name="l01585"></a>01585 <span class="comment">      if(reserve_evaluation_history)</span>
<a name="l01586"></a>01586 <span class="comment">      {</span>
<a name="l01587"></a>01587 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;evaluation_history[epoch] = performance;</span>
<a name="l01588"></a>01588 <span class="comment">      }</span>
<a name="l01589"></a>01589 <span class="comment"></span>
<a name="l01590"></a>01590 <span class="comment">      if(reserve_generalization_evaluation_history)</span>
<a name="l01591"></a>01591 <span class="comment">      {</span>
<a name="l01592"></a>01592 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;generalization_evaluation_history[epoch] = generalization_evaluation;</span>
<a name="l01593"></a>01593 <span class="comment">      }</span>
<a name="l01594"></a>01594 <span class="comment"></span>
<a name="l01595"></a>01595 <span class="comment">      if(reserve_gradient_history)</span>
<a name="l01596"></a>01596 <span class="comment">      {</span>
<a name="l01597"></a>01597 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;gradient_history[epoch] = gradient;                                </span>
<a name="l01598"></a>01598 <span class="comment">      }</span>
<a name="l01599"></a>01599 <span class="comment"></span>
<a name="l01600"></a>01600 <span class="comment">      if(reserve_gradient_norm_history)</span>
<a name="l01601"></a>01601 <span class="comment">      {</span>
<a name="l01602"></a>01602 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;gradient_norm_history[epoch] = gradient_norm;</span>
<a name="l01603"></a>01603 <span class="comment">      }</span>
<a name="l01604"></a>01604 <span class="comment"></span>
<a name="l01605"></a>01605 <span class="comment">      // Training history training algorithm</span>
<a name="l01606"></a>01606 <span class="comment"></span>
<a name="l01607"></a>01607 <span class="comment">      if(reserve_training_direction_history)</span>
<a name="l01608"></a>01608 <span class="comment">      {</span>
<a name="l01609"></a>01609 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;training_direction_history[epoch] = training_direction;                                </span>
<a name="l01610"></a>01610 <span class="comment">      }</span>
<a name="l01611"></a>01611 <span class="comment"></span>
<a name="l01612"></a>01612 <span class="comment">      if(reserve_training_rate_history)</span>
<a name="l01613"></a>01613 <span class="comment">      {</span>
<a name="l01614"></a>01614 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;training_rate_history[epoch] = training_rate;</span>
<a name="l01615"></a>01615 <span class="comment">      }</span>
<a name="l01616"></a>01616 <span class="comment"></span>
<a name="l01617"></a>01617 <span class="comment">      if(reserve_elapsed_time_history)</span>
<a name="l01618"></a>01618 <span class="comment">      {</span>
<a name="l01619"></a>01619 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;elapsed_time_history[epoch] = elapsed_time;</span>
<a name="l01620"></a>01620 <span class="comment">      }</span>
<a name="l01621"></a>01621 <span class="comment"></span>
<a name="l01622"></a>01622 <span class="comment">      // Stopping Criteria</span>
<a name="l01623"></a>01623 <span class="comment"></span>
<a name="l01624"></a>01624 <span class="comment">      if(parameters_increment_norm &lt;= minimum_parameters_increment_norm)</span>
<a name="l01625"></a>01625 <span class="comment">      {</span>
<a name="l01626"></a>01626 <span class="comment">         if(display)</span>
<a name="l01627"></a>01627 <span class="comment">         {</span>
<a name="l01628"></a>01628 <span class="comment">            std::cout &lt;&lt; "Epoch " &lt;&lt; epoch &lt;&lt; ": Minimum parameters increment norm reached.\n";</span>
<a name="l01629"></a>01629 <span class="comment">            std::cout &lt;&lt; "Parameters increment norm: " &lt;&lt; parameters_increment_norm &lt;&lt; std::endl;</span>
<a name="l01630"></a>01630 <span class="comment">         }</span>
<a name="l01631"></a>01631 <span class="comment"></span>
<a name="l01632"></a>01632 <span class="comment">         stop_training = true;</span>
<a name="l01633"></a>01633 <span class="comment">      }</span>
<a name="l01634"></a>01634 <span class="comment"></span>
<a name="l01635"></a>01635 <span class="comment">      else if(epoch != 0 &amp;&amp; performance_increase &lt;= minimum_performance_increase)</span>
<a name="l01636"></a>01636 <span class="comment">      {</span>
<a name="l01637"></a>01637 <span class="comment">         if(display)</span>
<a name="l01638"></a>01638 <span class="comment">         {</span>
<a name="l01639"></a>01639 <span class="comment">            std::cout &lt;&lt; "Epoch " &lt;&lt; epoch &lt;&lt; ": Minimum performance increase reached.\n";</span>
<a name="l01640"></a>01640 <span class="comment">            std::cout &lt;&lt; "Performance increase: " &lt;&lt; performance_increase &lt;&lt; std::endl;</span>
<a name="l01641"></a>01641 <span class="comment">         }</span>
<a name="l01642"></a>01642 <span class="comment"></span>
<a name="l01643"></a>01643 <span class="comment">         stop_training = true;</span>
<a name="l01644"></a>01644 <span class="comment">      }</span>
<a name="l01645"></a>01645 <span class="comment"></span>
<a name="l01646"></a>01646 <span class="comment">      else if(performance &lt;= performance_goal)</span>
<a name="l01647"></a>01647 <span class="comment">      {</span>
<a name="l01648"></a>01648 <span class="comment">         if(display)</span>
<a name="l01649"></a>01649 <span class="comment">         {</span>
<a name="l01650"></a>01650 <span class="comment">            std::cout &lt;&lt; "Epoch " &lt;&lt; epoch &lt;&lt; ": Performance goal reached.\n";</span>
<a name="l01651"></a>01651 <span class="comment">         }</span>
<a name="l01652"></a>01652 <span class="comment"></span>
<a name="l01653"></a>01653 <span class="comment">         stop_training = true;</span>
<a name="l01654"></a>01654 <span class="comment">      }</span>
<a name="l01655"></a>01655 <span class="comment"></span>
<a name="l01656"></a>01656 <span class="comment">      else if(gradient_norm &lt;= gradient_norm_goal)</span>
<a name="l01657"></a>01657 <span class="comment">      {</span>
<a name="l01658"></a>01658 <span class="comment">         if(display)</span>
<a name="l01659"></a>01659 <span class="comment">         {</span>
<a name="l01660"></a>01660 <span class="comment">            std::cout &lt;&lt; "Epoch " &lt;&lt; epoch &lt;&lt; ": Gradient norm goal reached.\n";  </span>
<a name="l01661"></a>01661 <span class="comment">         }</span>
<a name="l01662"></a>01662 <span class="comment"></span>
<a name="l01663"></a>01663 <span class="comment">         stop_training = true;</span>
<a name="l01664"></a>01664 <span class="comment">      }</span>
<a name="l01665"></a>01665 <span class="comment"></span>
<a name="l01666"></a>01666 <span class="comment">      else if(generalization_evaluation_decreases_count &gt; maximum_generalization_evaluation_decreases)</span>
<a name="l01667"></a>01667 <span class="comment">      {</span>
<a name="l01668"></a>01668 <span class="comment">         if(display)</span>
<a name="l01669"></a>01669 <span class="comment">         {</span>
<a name="l01670"></a>01670 <span class="comment">            std::cout &lt;&lt; "Epoch " &lt;&lt; epoch &lt;&lt; ": Maximum generalization performance decreases reached.\n";</span>
<a name="l01671"></a>01671 <span class="comment">            std::cout &lt;&lt; "Generalization performance decreases: "&lt;&lt; generalization_evaluation_decreases_count &lt;&lt; std::endl;</span>
<a name="l01672"></a>01672 <span class="comment">         }</span>
<a name="l01673"></a>01673 <span class="comment"></span>
<a name="l01674"></a>01674 <span class="comment">         stop_training = true;</span>
<a name="l01675"></a>01675 <span class="comment">      }</span>
<a name="l01676"></a>01676 <span class="comment"></span>
<a name="l01677"></a>01677 <span class="comment">      else if(epoch == maximum_epochs_number)</span>
<a name="l01678"></a>01678 <span class="comment">      {</span>
<a name="l01679"></a>01679 <span class="comment">         if(display)</span>
<a name="l01680"></a>01680 <span class="comment">         {</span>
<a name="l01681"></a>01681 <span class="comment">            std::cout &lt;&lt; "Epoch " &lt;&lt; epoch &lt;&lt; ": Maximum number of epochs reached.\n";</span>
<a name="l01682"></a>01682 <span class="comment">         }</span>
<a name="l01683"></a>01683 <span class="comment"></span>
<a name="l01684"></a>01684 <span class="comment">         stop_training = true;</span>
<a name="l01685"></a>01685 <span class="comment">      }</span>
<a name="l01686"></a>01686 <span class="comment"></span>
<a name="l01687"></a>01687 <span class="comment">      else if(elapsed_time &gt;= maximum_time)</span>
<a name="l01688"></a>01688 <span class="comment">      {</span>
<a name="l01689"></a>01689 <span class="comment">         if(display)</span>
<a name="l01690"></a>01690 <span class="comment">         {</span>
<a name="l01691"></a>01691 <span class="comment">            std::cout &lt;&lt; "Epoch " &lt;&lt; epoch &lt;&lt; ": Maximum training time reached.\n";</span>
<a name="l01692"></a>01692 <span class="comment">         }</span>
<a name="l01693"></a>01693 <span class="comment"></span>
<a name="l01694"></a>01694 <span class="comment">         stop_training = true;</span>
<a name="l01695"></a>01695 <span class="comment">      }</span>
<a name="l01696"></a>01696 <span class="comment"></span>
<a name="l01697"></a>01697 <span class="comment">      if(stop_training)</span>
<a name="l01698"></a>01698 <span class="comment">      {</span>
<a name="l01699"></a>01699 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;final_parameters = parameters;</span>
<a name="l01700"></a>01700 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;final_parameters_norm = parameters_norm;</span>
<a name="l01701"></a>01701 <span class="comment"></span>
<a name="l01702"></a>01702 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;final_evaluation = performance;</span>
<a name="l01703"></a>01703 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;final_generalization_evaluation = generalization_evaluation;</span>
<a name="l01704"></a>01704 <span class="comment"></span>
<a name="l01705"></a>01705 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;final_gradient = gradient;</span>
<a name="l01706"></a>01706 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;final_gradient_norm = gradient_norm;</span>
<a name="l01707"></a>01707 <span class="comment">   </span>
<a name="l01708"></a>01708 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;final_training_direction = training_direction;</span>
<a name="l01709"></a>01709 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;final_training_rate = training_rate;</span>
<a name="l01710"></a>01710 <span class="comment">         conjugate_gradient_training_results_pointer-&gt;elapsed_time = elapsed_time;</span>
<a name="l01711"></a>01711 <span class="comment"></span>
<a name="l01712"></a>01712 <span class="comment">         if(display)</span>
<a name="l01713"></a>01713 <span class="comment">                 {                        </span>
<a name="l01714"></a>01714 <span class="comment">            information = performance_functional_pointer-&gt;write_information();</span>
<a name="l01715"></a>01715 <span class="comment">   </span>
<a name="l01716"></a>01716 <span class="comment">            std::cout &lt;&lt; "Parameters norm: " &lt;&lt; parameters_norm &lt;&lt; "\n"</span>
<a name="l01717"></a>01717 <span class="comment">                      &lt;&lt; "Performance: " &lt;&lt; performance &lt;&lt; "\n"</span>
<a name="l01718"></a>01718 <span class="comment">                      &lt;&lt; "Gradient norm: " &lt;&lt; gradient_norm &lt;&lt; "\n"</span>
<a name="l01719"></a>01719 <span class="comment">                                          &lt;&lt; information </span>
<a name="l01720"></a>01720 <span class="comment">                      &lt;&lt; "Training rate: " &lt;&lt; training_rate &lt;&lt; "\n"</span>
<a name="l01721"></a>01721 <span class="comment">                      &lt;&lt; "Elapsed time: " &lt;&lt; elapsed_time &lt;&lt; std::endl; </span>
<a name="l01722"></a>01722 <span class="comment"></span>
<a name="l01723"></a>01723 <span class="comment">            if(generalization_evaluation != 0)</span>
<a name="l01724"></a>01724 <span class="comment">            {</span>
<a name="l01725"></a>01725 <span class="comment">               std::cout &lt;&lt; "Generalization performance: " &lt;&lt; generalization_evaluation &lt;&lt; std::endl;</span>
<a name="l01726"></a>01726 <span class="comment">            }</span>
<a name="l01727"></a>01727 <span class="comment">                 }</span>
<a name="l01728"></a>01728 <span class="comment">  </span>
<a name="l01729"></a>01729 <span class="comment">         break;</span>
<a name="l01730"></a>01730 <span class="comment">      }</span>
<a name="l01731"></a>01731 <span class="comment"></span>
<a name="l01732"></a>01732 <span class="comment">      else if(display &amp;&amp; epoch % display_period == 0)</span>
<a name="l01733"></a>01733 <span class="comment">      {</span>
<a name="l01734"></a>01734 <span class="comment">         information = performance_functional_pointer-&gt;write_information();</span>
<a name="l01735"></a>01735 <span class="comment"></span>
<a name="l01736"></a>01736 <span class="comment">         std::cout &lt;&lt; "Epoch " &lt;&lt; epoch &lt;&lt; ";\n"</span>
<a name="l01737"></a>01737 <span class="comment">                   &lt;&lt; "Parameters norm: " &lt;&lt; parameters_norm &lt;&lt; "\n"</span>
<a name="l01738"></a>01738 <span class="comment">                   &lt;&lt; "Performance: " &lt;&lt; performance &lt;&lt; "\n"</span>
<a name="l01739"></a>01739 <span class="comment">                   &lt;&lt; "Gradient norm: " &lt;&lt; gradient_norm &lt;&lt; "\n"</span>
<a name="l01740"></a>01740 <span class="comment">                   &lt;&lt; information </span>
<a name="l01741"></a>01741 <span class="comment">                   &lt;&lt; "Training rate: " &lt;&lt; training_rate &lt;&lt; "\n"</span>
<a name="l01742"></a>01742 <span class="comment">                   &lt;&lt; "Elapsed time: " &lt;&lt; elapsed_time &lt;&lt; std::endl; </span>
<a name="l01743"></a>01743 <span class="comment"></span>
<a name="l01744"></a>01744 <span class="comment">         if(generalization_evaluation != 0)</span>
<a name="l01745"></a>01745 <span class="comment">         {</span>
<a name="l01746"></a>01746 <span class="comment">            std::cout &lt;&lt; "Generalization performance: " &lt;&lt; generalization_evaluation &lt;&lt; std::endl;</span>
<a name="l01747"></a>01747 <span class="comment">         }</span>
<a name="l01748"></a>01748 <span class="comment">      }</span>
<a name="l01749"></a>01749 <span class="comment"></span>
<a name="l01750"></a>01750 <span class="comment">      // Set new parameters</span>
<a name="l01751"></a>01751 <span class="comment"></span>
<a name="l01752"></a>01752 <span class="comment">      parameters += parameters_increment;</span>
<a name="l01753"></a>01753 <span class="comment"></span>
<a name="l01754"></a>01754 <span class="comment">      neural_network_pointer-&gt;set_parameters(parameters);</span>
<a name="l01755"></a>01755 <span class="comment"></span>
<a name="l01756"></a>01756 <span class="comment">      // Update stuff</span>
<a name="l01757"></a>01757 <span class="comment"></span>
<a name="l01758"></a>01758 <span class="comment">      old_performance = performance;</span>
<a name="l01759"></a>01759 <span class="comment">      old_gradient = gradient;</span>
<a name="l01760"></a>01760 <span class="comment">      old_generalization_evaluation = generalization_evaluation;</span>
<a name="l01761"></a>01761 <span class="comment"></span>
<a name="l01762"></a>01762 <span class="comment">      old_training_direction = training_direction;   </span>
<a name="l01763"></a>01763 <span class="comment">      old_training_rate = training_rate;</span>
<a name="l01764"></a>01764 <span class="comment">   } </span>
<a name="l01765"></a>01765 <span class="comment"></span>
<a name="l01766"></a>01766 <span class="comment">   return(conjugate_gradient_training_results_pointer);</span>
<a name="l01767"></a>01767 <span class="comment">*/</span>
<a name="l01768"></a>01768 }
<a name="l01769"></a>01769 
<a name="l01770"></a>01770 
<a name="l01771"></a>01771 <span class="comment">// std::string write_training_algorithm_type(void) const method</span>
<a name="l01772"></a>01772 
<a name="l01773"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#da7b8d4a49f4e401f94a08a287d5359e">01773</a> std::string <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#da7b8d4a49f4e401f94a08a287d5359e" title="This method writes a string with the type of training algoritm.">ConjugateGradient::write_training_algorithm_type</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l01774"></a>01774 <span class="keyword"></span>{
<a name="l01775"></a>01775    <span class="keywordflow">return</span>(<span class="stringliteral">"CONJUGATE_GRADIENT"</span>);
<a name="l01776"></a>01776 }
<a name="l01777"></a>01777 
<a name="l01778"></a>01778 
<a name="l01779"></a>01779 <span class="comment">// TiXmlElement* to_XML(void) const method</span>
<a name="l01780"></a>01780 
<a name="l01783"></a>01783 
<a name="l01784"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#6c49f6f5185b73909d10decaed887b2a">01784</a> TiXmlElement* <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#6c49f6f5185b73909d10decaed887b2a">ConjugateGradient::to_XML</a>(<span class="keywordtype">void</span>)<span class="keyword"> const</span>
<a name="l01785"></a>01785 <span class="keyword"></span>{
<a name="l01786"></a>01786    std::ostringstream buffer;
<a name="l01787"></a>01787 
<a name="l01788"></a>01788    <span class="comment">// Conjugate gradient</span>
<a name="l01789"></a>01789 
<a name="l01790"></a>01790    TiXmlElement* conjugate_gradient_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"GonjugateGradient"</span>);
<a name="l01791"></a>01791    conjugate_gradient_element-&gt;SetAttribute(<span class="stringliteral">"Version"</span>, 4); 
<a name="l01792"></a>01792 
<a name="l01793"></a>01793    <span class="comment">// Training direction method</span>
<a name="l01794"></a>01794    {
<a name="l01795"></a>01795       TiXmlElement* training_direction_method_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"TrainingDirectionMethod"</span>);
<a name="l01796"></a>01796       conjugate_gradient_element-&gt;LinkEndChild(training_direction_method_element);
<a name="l01797"></a>01797 
<a name="l01798"></a>01798       TiXmlText* training_direction_method_text = <span class="keyword">new</span> TiXmlText(<a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#020bd6b55cfcedf04647818eb388b292" title="This method returns a string with the name of the training direction.">write_training_direction_method</a>().c_str());
<a name="l01799"></a>01799       training_direction_method_element-&gt;LinkEndChild(training_direction_method_text);
<a name="l01800"></a>01800    }
<a name="l01801"></a>01801 
<a name="l01802"></a>01802    <span class="comment">// Training rate algorithm</span>
<a name="l01803"></a>01803    {
<a name="l01804"></a>01804       TiXmlElement* training_rate_algorithm_element = training_rate_algorithm.<a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html#b3856d91ebd1912224be846c50b889df">to_XML</a>();
<a name="l01805"></a>01805       conjugate_gradient_element-&gt;LinkEndChild(training_rate_algorithm_element);
<a name="l01806"></a>01806    }
<a name="l01807"></a>01807 
<a name="l01808"></a>01808    <span class="comment">// Warning parameters norm</span>
<a name="l01809"></a>01809    {
<a name="l01810"></a>01810       TiXmlElement* warning_parameters_norm_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"WarningParametersNorm"</span>);
<a name="l01811"></a>01811       conjugate_gradient_element-&gt;LinkEndChild(warning_parameters_norm_element);
<a name="l01812"></a>01812 
<a name="l01813"></a>01813       buffer.str(<span class="stringliteral">""</span>);
<a name="l01814"></a>01814       buffer &lt;&lt; warning_parameters_norm;
<a name="l01815"></a>01815 
<a name="l01816"></a>01816       TiXmlText* warning_parameters_norm_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01817"></a>01817       warning_parameters_norm_element-&gt;LinkEndChild(warning_parameters_norm_text);
<a name="l01818"></a>01818    }
<a name="l01819"></a>01819 
<a name="l01820"></a>01820    <span class="comment">// Warning gradient norm </span>
<a name="l01821"></a>01821    {
<a name="l01822"></a>01822       TiXmlElement* warning_gradient_norm_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"WarningGradientNorm"</span>);
<a name="l01823"></a>01823       conjugate_gradient_element-&gt;LinkEndChild(warning_gradient_norm_element);
<a name="l01824"></a>01824 
<a name="l01825"></a>01825       buffer.str(<span class="stringliteral">""</span>);
<a name="l01826"></a>01826       buffer &lt;&lt; warning_gradient_norm;
<a name="l01827"></a>01827 
<a name="l01828"></a>01828       TiXmlText* warning_gradient_norm_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01829"></a>01829       warning_gradient_norm_element-&gt;LinkEndChild(warning_gradient_norm_text);
<a name="l01830"></a>01830    }
<a name="l01831"></a>01831 
<a name="l01832"></a>01832    <span class="comment">// Warning training rate </span>
<a name="l01833"></a>01833    {
<a name="l01834"></a>01834       TiXmlElement* warning_training_rate_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"WarningTrainingRate"</span>);
<a name="l01835"></a>01835       conjugate_gradient_element-&gt;LinkEndChild(warning_training_rate_element);
<a name="l01836"></a>01836 
<a name="l01837"></a>01837       buffer.str(<span class="stringliteral">""</span>);
<a name="l01838"></a>01838       buffer &lt;&lt; warning_training_rate;
<a name="l01839"></a>01839 
<a name="l01840"></a>01840       TiXmlText* warning_training_rate_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01841"></a>01841       warning_training_rate_element-&gt;LinkEndChild(warning_training_rate_text);
<a name="l01842"></a>01842    }
<a name="l01843"></a>01843 
<a name="l01844"></a>01844    <span class="comment">// Error parameters norm</span>
<a name="l01845"></a>01845    {
<a name="l01846"></a>01846       TiXmlElement* error_parameters_norm_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ErrorParametersNorm"</span>);
<a name="l01847"></a>01847       conjugate_gradient_element-&gt;LinkEndChild(error_parameters_norm_element);
<a name="l01848"></a>01848 
<a name="l01849"></a>01849       buffer.str(<span class="stringliteral">""</span>);
<a name="l01850"></a>01850       buffer &lt;&lt; error_parameters_norm;
<a name="l01851"></a>01851 
<a name="l01852"></a>01852       TiXmlText* error_parameters_norm_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01853"></a>01853       error_parameters_norm_element-&gt;LinkEndChild(error_parameters_norm_text);
<a name="l01854"></a>01854    }
<a name="l01855"></a>01855 
<a name="l01856"></a>01856    <span class="comment">// Error gradient norm </span>
<a name="l01857"></a>01857    {
<a name="l01858"></a>01858       TiXmlElement* error_gradient_norm_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ErrorGradientNorm"</span>);
<a name="l01859"></a>01859       conjugate_gradient_element-&gt;LinkEndChild(error_gradient_norm_element);
<a name="l01860"></a>01860 
<a name="l01861"></a>01861       buffer.str(<span class="stringliteral">""</span>);
<a name="l01862"></a>01862       buffer &lt;&lt; error_gradient_norm;
<a name="l01863"></a>01863 
<a name="l01864"></a>01864       TiXmlText* error_gradient_norm_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01865"></a>01865       error_gradient_norm_element-&gt;LinkEndChild(error_gradient_norm_text);
<a name="l01866"></a>01866    }
<a name="l01867"></a>01867 
<a name="l01868"></a>01868    <span class="comment">// Error training rate</span>
<a name="l01869"></a>01869    {
<a name="l01870"></a>01870       TiXmlElement* error_training_rate_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ErrorTrainingRate"</span>);
<a name="l01871"></a>01871       conjugate_gradient_element-&gt;LinkEndChild(error_training_rate_element);
<a name="l01872"></a>01872 
<a name="l01873"></a>01873       buffer.str(<span class="stringliteral">""</span>);
<a name="l01874"></a>01874       buffer &lt;&lt; error_training_rate;
<a name="l01875"></a>01875 
<a name="l01876"></a>01876       TiXmlText* error_training_rate_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01877"></a>01877       error_training_rate_element-&gt;LinkEndChild(error_training_rate_text);
<a name="l01878"></a>01878    }
<a name="l01879"></a>01879 
<a name="l01880"></a>01880    <span class="comment">// Minimum parameters increment norm</span>
<a name="l01881"></a>01881    {
<a name="l01882"></a>01882       TiXmlElement* minimum_parameters_increment_norm_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"MinimumParametersIncrement"</span>);
<a name="l01883"></a>01883       conjugate_gradient_element-&gt;LinkEndChild(minimum_parameters_increment_norm_element);
<a name="l01884"></a>01884 
<a name="l01885"></a>01885       buffer.str(<span class="stringliteral">""</span>);
<a name="l01886"></a>01886       buffer &lt;&lt; minimum_parameters_increment_norm;
<a name="l01887"></a>01887 
<a name="l01888"></a>01888       TiXmlText* minimum_parameters_increment_norm_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01889"></a>01889       minimum_parameters_increment_norm_element-&gt;LinkEndChild(minimum_parameters_increment_norm_text);
<a name="l01890"></a>01890    }
<a name="l01891"></a>01891 
<a name="l01892"></a>01892    <span class="comment">// Minimum performance increase </span>
<a name="l01893"></a>01893    {
<a name="l01894"></a>01894       TiXmlElement* minimum_performance_increase_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"MinimumPerformanceIncrease"</span>);
<a name="l01895"></a>01895       conjugate_gradient_element-&gt;LinkEndChild(minimum_performance_increase_element);
<a name="l01896"></a>01896 
<a name="l01897"></a>01897       buffer.str(<span class="stringliteral">""</span>);
<a name="l01898"></a>01898       buffer &lt;&lt; minimum_performance_increase;
<a name="l01899"></a>01899 
<a name="l01900"></a>01900       TiXmlText* minimum_performance_increase_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01901"></a>01901       minimum_performance_increase_element-&gt;LinkEndChild(minimum_performance_increase_text);
<a name="l01902"></a>01902    }
<a name="l01903"></a>01903 
<a name="l01904"></a>01904    <span class="comment">// Performance goal </span>
<a name="l01905"></a>01905    {
<a name="l01906"></a>01906       TiXmlElement* performance_goal_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"PerformanceGoal"</span>);
<a name="l01907"></a>01907       conjugate_gradient_element-&gt;LinkEndChild(performance_goal_element);
<a name="l01908"></a>01908 
<a name="l01909"></a>01909       buffer.str(<span class="stringliteral">""</span>);
<a name="l01910"></a>01910       buffer &lt;&lt; performance_goal;
<a name="l01911"></a>01911 
<a name="l01912"></a>01912       TiXmlText* performance_goal_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01913"></a>01913       performance_goal_element-&gt;LinkEndChild(performance_goal_text);
<a name="l01914"></a>01914    }
<a name="l01915"></a>01915 
<a name="l01916"></a>01916    <span class="comment">// Gradient norm goal </span>
<a name="l01917"></a>01917    {
<a name="l01918"></a>01918       TiXmlElement* gradient_norm_goal_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"GradientNormGoal"</span>);
<a name="l01919"></a>01919       conjugate_gradient_element-&gt;LinkEndChild(gradient_norm_goal_element);
<a name="l01920"></a>01920 
<a name="l01921"></a>01921       buffer.str(<span class="stringliteral">""</span>);
<a name="l01922"></a>01922       buffer &lt;&lt; gradient_norm_goal;
<a name="l01923"></a>01923 
<a name="l01924"></a>01924       TiXmlText* gradient_norm_goal_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01925"></a>01925       gradient_norm_goal_element-&gt;LinkEndChild(gradient_norm_goal_text);
<a name="l01926"></a>01926    }
<a name="l01927"></a>01927 
<a name="l01928"></a>01928    <span class="comment">// Maximum generalization performance decreases</span>
<a name="l01929"></a>01929    {
<a name="l01930"></a>01930       TiXmlElement* maximum_generalization_evaluation_decreases_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"MaximumGeneralizationEvaluationDecreases"</span>);
<a name="l01931"></a>01931       conjugate_gradient_element-&gt;LinkEndChild(maximum_generalization_evaluation_decreases_element);
<a name="l01932"></a>01932 
<a name="l01933"></a>01933       buffer.str(<span class="stringliteral">""</span>);
<a name="l01934"></a>01934       buffer &lt;&lt; maximum_generalization_evaluation_decreases;
<a name="l01935"></a>01935 
<a name="l01936"></a>01936       TiXmlText* maximum_generalization_evaluation_decreases_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01937"></a>01937       maximum_generalization_evaluation_decreases_element-&gt;LinkEndChild(maximum_generalization_evaluation_decreases_text);
<a name="l01938"></a>01938    }
<a name="l01939"></a>01939 
<a name="l01940"></a>01940    <span class="comment">// Maximum epochs number </span>
<a name="l01941"></a>01941    {
<a name="l01942"></a>01942       TiXmlElement* maximum_epochs_number_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"MaximumEpochsNumber"</span>);
<a name="l01943"></a>01943       conjugate_gradient_element-&gt;LinkEndChild(maximum_epochs_number_element);
<a name="l01944"></a>01944 
<a name="l01945"></a>01945       buffer.str(<span class="stringliteral">""</span>);
<a name="l01946"></a>01946       buffer &lt;&lt; maximum_epochs_number;
<a name="l01947"></a>01947 
<a name="l01948"></a>01948       TiXmlText* maximum_epochs_number_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01949"></a>01949       maximum_epochs_number_element-&gt;LinkEndChild(maximum_epochs_number_text);
<a name="l01950"></a>01950    }
<a name="l01951"></a>01951 
<a name="l01952"></a>01952    <span class="comment">// Maximum time </span>
<a name="l01953"></a>01953    {
<a name="l01954"></a>01954       TiXmlElement* maximum_time_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"MaximumTime"</span>);
<a name="l01955"></a>01955       conjugate_gradient_element-&gt;LinkEndChild(maximum_time_element);
<a name="l01956"></a>01956 
<a name="l01957"></a>01957       buffer.str(<span class="stringliteral">""</span>);
<a name="l01958"></a>01958       buffer &lt;&lt; maximum_time;
<a name="l01959"></a>01959 
<a name="l01960"></a>01960       TiXmlText* maximum_time_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01961"></a>01961       maximum_time_element-&gt;LinkEndChild(maximum_time_text);
<a name="l01962"></a>01962    }
<a name="l01963"></a>01963 
<a name="l01964"></a>01964    <span class="comment">// Reserve parameters history </span>
<a name="l01965"></a>01965    {
<a name="l01966"></a>01966       TiXmlElement* reserve_parameters_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveParametersHistory"</span>);
<a name="l01967"></a>01967       conjugate_gradient_element-&gt;LinkEndChild(reserve_parameters_history_element);
<a name="l01968"></a>01968 
<a name="l01969"></a>01969       buffer.str(<span class="stringliteral">""</span>);
<a name="l01970"></a>01970       buffer &lt;&lt; reserve_parameters_history;
<a name="l01971"></a>01971 
<a name="l01972"></a>01972       TiXmlText* reserve_parameters_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01973"></a>01973       reserve_parameters_history_element-&gt;LinkEndChild(reserve_parameters_history_text);
<a name="l01974"></a>01974    }
<a name="l01975"></a>01975 
<a name="l01976"></a>01976    <span class="comment">// Reserve parameters norm history </span>
<a name="l01977"></a>01977    {
<a name="l01978"></a>01978       TiXmlElement* reserve_parameters_norm_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveParametersNormHistory"</span>);
<a name="l01979"></a>01979       conjugate_gradient_element-&gt;LinkEndChild(reserve_parameters_norm_history_element);
<a name="l01980"></a>01980 
<a name="l01981"></a>01981       buffer.str(<span class="stringliteral">""</span>);
<a name="l01982"></a>01982       buffer &lt;&lt; reserve_parameters_norm_history;
<a name="l01983"></a>01983 
<a name="l01984"></a>01984       TiXmlText* reserve_parameters_norm_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01985"></a>01985       reserve_parameters_norm_history_element-&gt;LinkEndChild(reserve_parameters_norm_history_text);
<a name="l01986"></a>01986    }
<a name="l01987"></a>01987 
<a name="l01988"></a>01988    <span class="comment">// Reserve evaluation history </span>
<a name="l01989"></a>01989    {
<a name="l01990"></a>01990       TiXmlElement* reserve_evaluation_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReservePerformanceHistory"</span>);
<a name="l01991"></a>01991       conjugate_gradient_element-&gt;LinkEndChild(reserve_evaluation_history_element);
<a name="l01992"></a>01992 
<a name="l01993"></a>01993       buffer.str(<span class="stringliteral">""</span>);
<a name="l01994"></a>01994       buffer &lt;&lt; reserve_evaluation_history;
<a name="l01995"></a>01995 
<a name="l01996"></a>01996       TiXmlText* reserve_evaluation_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l01997"></a>01997       reserve_evaluation_history_element-&gt;LinkEndChild(reserve_evaluation_history_text);
<a name="l01998"></a>01998    }
<a name="l01999"></a>01999 
<a name="l02000"></a>02000    <span class="comment">// Reserve gradient history </span>
<a name="l02001"></a>02001    {
<a name="l02002"></a>02002       TiXmlElement* reserve_gradient_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveGradientHistory"</span>);
<a name="l02003"></a>02003       conjugate_gradient_element-&gt;LinkEndChild(reserve_gradient_history_element);
<a name="l02004"></a>02004 
<a name="l02005"></a>02005       buffer.str(<span class="stringliteral">""</span>);
<a name="l02006"></a>02006       buffer &lt;&lt; reserve_gradient_history;
<a name="l02007"></a>02007 
<a name="l02008"></a>02008       TiXmlText* reserve_gradient_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l02009"></a>02009       reserve_gradient_history_element-&gt;LinkEndChild(reserve_gradient_history_text);
<a name="l02010"></a>02010    }
<a name="l02011"></a>02011 
<a name="l02012"></a>02012    <span class="comment">// Reserve gradient norm history </span>
<a name="l02013"></a>02013    {
<a name="l02014"></a>02014       TiXmlElement* reserve_gradient_norm_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveGradientNormHistory"</span>);
<a name="l02015"></a>02015       conjugate_gradient_element-&gt;LinkEndChild(reserve_gradient_norm_history_element);
<a name="l02016"></a>02016 
<a name="l02017"></a>02017       buffer.str(<span class="stringliteral">""</span>);
<a name="l02018"></a>02018       buffer &lt;&lt; reserve_gradient_norm_history;
<a name="l02019"></a>02019 
<a name="l02020"></a>02020       TiXmlText* reserve_gradient_norm_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l02021"></a>02021       reserve_gradient_norm_history_element-&gt;LinkEndChild(reserve_gradient_norm_history_text);
<a name="l02022"></a>02022    }
<a name="l02023"></a>02023 
<a name="l02024"></a>02024    <span class="comment">// Reserve training direction history </span>
<a name="l02025"></a>02025    {
<a name="l02026"></a>02026       TiXmlElement* reserve_training_direction_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveTrainingDirectionHistory"</span>);
<a name="l02027"></a>02027       conjugate_gradient_element-&gt;LinkEndChild(reserve_training_direction_history_element);
<a name="l02028"></a>02028 
<a name="l02029"></a>02029       buffer.str(<span class="stringliteral">""</span>);
<a name="l02030"></a>02030       buffer &lt;&lt; reserve_training_direction_history;
<a name="l02031"></a>02031 
<a name="l02032"></a>02032       TiXmlText* reserve_training_direction_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l02033"></a>02033       reserve_training_direction_history_element-&gt;LinkEndChild(reserve_training_direction_history_text);
<a name="l02034"></a>02034    }
<a name="l02035"></a>02035 
<a name="l02036"></a>02036    <span class="comment">// Reserve training rate history </span>
<a name="l02037"></a>02037    {
<a name="l02038"></a>02038       TiXmlElement* reserve_training_rate_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveTrainingRateHistory"</span>);
<a name="l02039"></a>02039       conjugate_gradient_element-&gt;LinkEndChild(reserve_training_rate_history_element);
<a name="l02040"></a>02040 
<a name="l02041"></a>02041       buffer.str(<span class="stringliteral">""</span>);
<a name="l02042"></a>02042       buffer &lt;&lt; reserve_training_rate_history;
<a name="l02043"></a>02043 
<a name="l02044"></a>02044       TiXmlText* reserve_training_rate_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l02045"></a>02045       reserve_training_rate_history_element-&gt;LinkEndChild(reserve_training_rate_history_text);
<a name="l02046"></a>02046    }
<a name="l02047"></a>02047 
<a name="l02048"></a>02048    <span class="comment">// Reserve elapsed time history </span>
<a name="l02049"></a>02049    {
<a name="l02050"></a>02050       TiXmlElement* reserve_elapsed_time_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveElapsedTimeHistory"</span>);
<a name="l02051"></a>02051       conjugate_gradient_element-&gt;LinkEndChild(reserve_elapsed_time_history_element);
<a name="l02052"></a>02052 
<a name="l02053"></a>02053       buffer.str(<span class="stringliteral">""</span>);
<a name="l02054"></a>02054       buffer &lt;&lt; reserve_elapsed_time_history;
<a name="l02055"></a>02055 
<a name="l02056"></a>02056       TiXmlText* reserve_elapsed_time_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l02057"></a>02057       reserve_elapsed_time_history_element-&gt;LinkEndChild(reserve_elapsed_time_history_text);
<a name="l02058"></a>02058    }
<a name="l02059"></a>02059 
<a name="l02060"></a>02060    <span class="comment">// Reserve generalization evaluation history </span>
<a name="l02061"></a>02061    {
<a name="l02062"></a>02062       TiXmlElement* reserve_generalization_evaluation_history_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"ReserveGeneralizationPerformanceHistory"</span>);
<a name="l02063"></a>02063       conjugate_gradient_element-&gt;LinkEndChild(reserve_generalization_evaluation_history_element);
<a name="l02064"></a>02064 
<a name="l02065"></a>02065       buffer.str(<span class="stringliteral">""</span>);
<a name="l02066"></a>02066       buffer &lt;&lt; reserve_generalization_evaluation_history;
<a name="l02067"></a>02067 
<a name="l02068"></a>02068       TiXmlText* reserve_generalization_evaluation_history_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l02069"></a>02069       reserve_generalization_evaluation_history_element-&gt;LinkEndChild(reserve_generalization_evaluation_history_text);
<a name="l02070"></a>02070    }
<a name="l02071"></a>02071 
<a name="l02072"></a>02072    <span class="comment">// Display period</span>
<a name="l02073"></a>02073    {
<a name="l02074"></a>02074       TiXmlElement* display_period_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"DisplayPeriod"</span>);
<a name="l02075"></a>02075       conjugate_gradient_element-&gt;LinkEndChild(display_period_element);
<a name="l02076"></a>02076 
<a name="l02077"></a>02077       buffer.str(<span class="stringliteral">""</span>);
<a name="l02078"></a>02078       buffer &lt;&lt; display_period;
<a name="l02079"></a>02079 
<a name="l02080"></a>02080       TiXmlText* display_period_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l02081"></a>02081       display_period_element-&gt;LinkEndChild(display_period_text);
<a name="l02082"></a>02082    }
<a name="l02083"></a>02083 
<a name="l02084"></a>02084    <span class="comment">// Display</span>
<a name="l02085"></a>02085    {
<a name="l02086"></a>02086       TiXmlElement* display_element = <span class="keyword">new</span> TiXmlElement(<span class="stringliteral">"Display"</span>);
<a name="l02087"></a>02087       conjugate_gradient_element-&gt;LinkEndChild(display_element);
<a name="l02088"></a>02088 
<a name="l02089"></a>02089       buffer.str(<span class="stringliteral">""</span>);
<a name="l02090"></a>02090       buffer &lt;&lt; <a class="code" href="class_open_n_n_1_1_training_algorithm.html#13bf46cc2670a9852b9336b66b7897f3" title="Display messages to screen.">display</a>;
<a name="l02091"></a>02091 
<a name="l02092"></a>02092       TiXmlText* display_text = <span class="keyword">new</span> TiXmlText(buffer.str().c_str());
<a name="l02093"></a>02093       display_element-&gt;LinkEndChild(display_text);
<a name="l02094"></a>02094    }
<a name="l02095"></a>02095 
<a name="l02096"></a>02096    <span class="keywordflow">return</span>(conjugate_gradient_element);
<a name="l02097"></a>02097 }
<a name="l02098"></a>02098 
<a name="l02099"></a>02099 
<a name="l02100"></a>02100 <span class="comment">// void from_XML(TiXmlElement*) method</span>
<a name="l02101"></a>02101 
<a name="l02105"></a>02105 
<a name="l02106"></a><a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#0a4d9795e258a93cf261b996eb785018">02106</a> <span class="keywordtype">void</span> <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#0a4d9795e258a93cf261b996eb785018">ConjugateGradient::from_XML</a>(TiXmlElement* conjugate_gradient_element)
<a name="l02107"></a>02107 {
<a name="l02108"></a>02108    <span class="keywordflow">if</span>(conjugate_gradient_element)
<a name="l02109"></a>02109    {
<a name="l02110"></a>02110       <span class="comment">// Training direction method</span>
<a name="l02111"></a>02111       {
<a name="l02112"></a>02112          TiXmlElement* training_direction_method_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"TrainingDirectionMethod"</span>);
<a name="l02113"></a>02113 
<a name="l02114"></a>02114          <span class="keywordflow">if</span>(training_direction_method_element)
<a name="l02115"></a>02115          {
<a name="l02116"></a>02116             <span class="keyword">const</span> std::string new_training_direction_method = training_direction_method_element-&gt;GetText(); 
<a name="l02117"></a>02117 
<a name="l02118"></a>02118             <span class="keywordflow">try</span>
<a name="l02119"></a>02119             {
<a name="l02120"></a>02120                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#6ae0ce9c10e2462993eee59883e490ba">set_training_direction_method</a>(new_training_direction_method);
<a name="l02121"></a>02121             }
<a name="l02122"></a>02122             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02123"></a>02123             {
<a name="l02124"></a>02124                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02125"></a>02125             }
<a name="l02126"></a>02126          }
<a name="l02127"></a>02127       }
<a name="l02128"></a>02128 
<a name="l02129"></a>02129       <span class="comment">// Training rate algorithm</span>
<a name="l02130"></a>02130       {
<a name="l02131"></a>02131          TiXmlElement* training_rate_algorithm_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"TrainingRateAlgorithm"</span>);
<a name="l02132"></a>02132    
<a name="l02133"></a>02133          <span class="keywordflow">if</span>(training_rate_algorithm_element)
<a name="l02134"></a>02134          {
<a name="l02135"></a>02135             <span class="keywordflow">try</span>
<a name="l02136"></a>02136             {
<a name="l02137"></a>02137                training_rate_algorithm.<a class="code" href="class_open_n_n_1_1_training_rate_algorithm.html#368026ed823890e2267c985d611112ce">from_XML</a>(training_rate_algorithm_element);
<a name="l02138"></a>02138             }
<a name="l02139"></a>02139             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02140"></a>02140             {
<a name="l02141"></a>02141                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02142"></a>02142             }
<a name="l02143"></a>02143          }
<a name="l02144"></a>02144       }
<a name="l02145"></a>02145 
<a name="l02146"></a>02146       <span class="comment">// Warning parameters norm</span>
<a name="l02147"></a>02147       {
<a name="l02148"></a>02148          TiXmlElement* warning_parameters_norm_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"WarningParametersNorm"</span>);
<a name="l02149"></a>02149 
<a name="l02150"></a>02150          <span class="keywordflow">if</span>(warning_parameters_norm_element)
<a name="l02151"></a>02151          {
<a name="l02152"></a>02152             <span class="keyword">const</span> <span class="keywordtype">double</span> new_warning_parameters_norm = atof(warning_parameters_norm_element-&gt;GetText()); 
<a name="l02153"></a>02153 
<a name="l02154"></a>02154             <span class="keywordflow">try</span>
<a name="l02155"></a>02155             {
<a name="l02156"></a>02156                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#4b1a110fd305f1986af4cb58c528ac63">set_warning_parameters_norm</a>(new_warning_parameters_norm);
<a name="l02157"></a>02157             }
<a name="l02158"></a>02158             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02159"></a>02159             {
<a name="l02160"></a>02160                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02161"></a>02161             }
<a name="l02162"></a>02162          }
<a name="l02163"></a>02163       }
<a name="l02164"></a>02164 
<a name="l02165"></a>02165       <span class="comment">// Warning gradient norm </span>
<a name="l02166"></a>02166       {
<a name="l02167"></a>02167          TiXmlElement* warning_gradient_norm_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"WarningGradientNorm"</span>);
<a name="l02168"></a>02168 
<a name="l02169"></a>02169          <span class="keywordflow">if</span>(warning_gradient_norm_element)
<a name="l02170"></a>02170          {
<a name="l02171"></a>02171             <span class="keyword">const</span> <span class="keywordtype">double</span> new_warning_gradient_norm = atof(warning_gradient_norm_element-&gt;GetText()); 
<a name="l02172"></a>02172 
<a name="l02173"></a>02173             <span class="keywordflow">try</span>
<a name="l02174"></a>02174             {
<a name="l02175"></a>02175                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#b9d2365e8c662083d1edd946d521f7bf">set_warning_gradient_norm</a>(new_warning_gradient_norm);
<a name="l02176"></a>02176             }
<a name="l02177"></a>02177             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02178"></a>02178             {
<a name="l02179"></a>02179                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02180"></a>02180             }
<a name="l02181"></a>02181          }
<a name="l02182"></a>02182       }
<a name="l02183"></a>02183 
<a name="l02184"></a>02184       <span class="comment">// Warning training rate </span>
<a name="l02185"></a>02185       {
<a name="l02186"></a>02186          TiXmlElement* warning_training_rate_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"WarningTrainingRate"</span>);
<a name="l02187"></a>02187 
<a name="l02188"></a>02188          <span class="keywordflow">if</span>(warning_training_rate_element)
<a name="l02189"></a>02189          {
<a name="l02190"></a>02190             <span class="keywordtype">double</span> new_warning_training_rate = atof(warning_training_rate_element-&gt;GetText()); 
<a name="l02191"></a>02191 
<a name="l02192"></a>02192             <span class="keywordflow">try</span>
<a name="l02193"></a>02193             {
<a name="l02194"></a>02194                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#bbae77c17da7d542b4e3c4251e57cb06">set_warning_training_rate</a>(new_warning_training_rate);
<a name="l02195"></a>02195             }
<a name="l02196"></a>02196             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02197"></a>02197             {
<a name="l02198"></a>02198                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02199"></a>02199             }
<a name="l02200"></a>02200          }
<a name="l02201"></a>02201       }
<a name="l02202"></a>02202 
<a name="l02203"></a>02203       <span class="comment">// Error parameters norm</span>
<a name="l02204"></a>02204       {
<a name="l02205"></a>02205          TiXmlElement* error_parameters_norm_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ErrorParametersNorm"</span>);
<a name="l02206"></a>02206 
<a name="l02207"></a>02207          <span class="keywordflow">if</span>(error_parameters_norm_element)
<a name="l02208"></a>02208          {
<a name="l02209"></a>02209             <span class="keywordtype">double</span> new_error_parameters_norm = atof(error_parameters_norm_element-&gt;GetText()); 
<a name="l02210"></a>02210 
<a name="l02211"></a>02211             <span class="keywordflow">try</span>
<a name="l02212"></a>02212             {
<a name="l02213"></a>02213                 <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#c213f5dae6f2236e288462abb794a956">set_error_parameters_norm</a>(new_error_parameters_norm);
<a name="l02214"></a>02214             }
<a name="l02215"></a>02215             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02216"></a>02216             {
<a name="l02217"></a>02217                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02218"></a>02218             }
<a name="l02219"></a>02219          }
<a name="l02220"></a>02220       }
<a name="l02221"></a>02221 
<a name="l02222"></a>02222       <span class="comment">// Error gradient norm </span>
<a name="l02223"></a>02223       {
<a name="l02224"></a>02224          TiXmlElement* error_gradient_norm_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ErrorGradientNorm"</span>);
<a name="l02225"></a>02225 
<a name="l02226"></a>02226          <span class="keywordflow">if</span>(error_gradient_norm_element)
<a name="l02227"></a>02227          {
<a name="l02228"></a>02228             <span class="keywordtype">double</span> new_error_gradient_norm = atof(error_gradient_norm_element-&gt;GetText()); 
<a name="l02229"></a>02229 
<a name="l02230"></a>02230             <span class="keywordflow">try</span>
<a name="l02231"></a>02231             {
<a name="l02232"></a>02232                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#ac8d36d1372f47c9cf03a3b73c1df440">set_error_gradient_norm</a>(new_error_gradient_norm);
<a name="l02233"></a>02233             }
<a name="l02234"></a>02234             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02235"></a>02235             {
<a name="l02236"></a>02236                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02237"></a>02237             }
<a name="l02238"></a>02238          }
<a name="l02239"></a>02239       }
<a name="l02240"></a>02240 
<a name="l02241"></a>02241       <span class="comment">// Error training rate</span>
<a name="l02242"></a>02242       {
<a name="l02243"></a>02243          TiXmlElement* error_training_rate_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ErrorTrainingRate"</span>);
<a name="l02244"></a>02244 
<a name="l02245"></a>02245          <span class="keywordflow">if</span>(error_training_rate_element)
<a name="l02246"></a>02246          {
<a name="l02247"></a>02247             <span class="keywordtype">double</span> new_error_training_rate = atof(error_training_rate_element-&gt;GetText()); 
<a name="l02248"></a>02248 
<a name="l02249"></a>02249             <span class="keywordflow">try</span>
<a name="l02250"></a>02250             {
<a name="l02251"></a>02251                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#165eaf088bb453f5a8ffc231d61d3d5f">set_error_training_rate</a>(new_error_training_rate);
<a name="l02252"></a>02252             }
<a name="l02253"></a>02253             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02254"></a>02254             {
<a name="l02255"></a>02255                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02256"></a>02256             }
<a name="l02257"></a>02257          }
<a name="l02258"></a>02258       }
<a name="l02259"></a>02259 
<a name="l02260"></a>02260       <span class="comment">// Minimum parameters increment norm</span>
<a name="l02261"></a>02261       {
<a name="l02262"></a>02262          TiXmlElement* minimum_parameters_increment_norm_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"MinimumParametersIncrementNorm"</span>);
<a name="l02263"></a>02263 
<a name="l02264"></a>02264          <span class="keywordflow">if</span>(minimum_parameters_increment_norm_element)
<a name="l02265"></a>02265          {
<a name="l02266"></a>02266             <span class="keywordtype">double</span> new_minimum_parameters_increment_norm = atof(minimum_parameters_increment_norm_element-&gt;GetText()); 
<a name="l02267"></a>02267 
<a name="l02268"></a>02268             <span class="keywordflow">try</span>
<a name="l02269"></a>02269             {
<a name="l02270"></a>02270                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#bcf2b6f18bb3ea67c66b8f274e05c170">set_minimum_parameters_increment_norm</a>(new_minimum_parameters_increment_norm);
<a name="l02271"></a>02271             }
<a name="l02272"></a>02272             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02273"></a>02273             {
<a name="l02274"></a>02274                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02275"></a>02275             }
<a name="l02276"></a>02276          }
<a name="l02277"></a>02277       }
<a name="l02278"></a>02278 
<a name="l02279"></a>02279       <span class="comment">// Minimum performance increase </span>
<a name="l02280"></a>02280       {
<a name="l02281"></a>02281          TiXmlElement* minimum_performance_increase_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"MinimumPerformanceIncrease"</span>);
<a name="l02282"></a>02282 
<a name="l02283"></a>02283          <span class="keywordflow">if</span>(minimum_performance_increase_element)
<a name="l02284"></a>02284          {
<a name="l02285"></a>02285             <span class="keywordtype">double</span> new_minimum_performance_increase = atof(minimum_performance_increase_element-&gt;GetText()); 
<a name="l02286"></a>02286 
<a name="l02287"></a>02287             <span class="keywordflow">try</span>
<a name="l02288"></a>02288             {
<a name="l02289"></a>02289                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#bfc1ed807a9f18532c879a82180aebd6">set_minimum_performance_increase</a>(new_minimum_performance_increase);
<a name="l02290"></a>02290             }
<a name="l02291"></a>02291             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02292"></a>02292             {
<a name="l02293"></a>02293                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02294"></a>02294             }
<a name="l02295"></a>02295          }
<a name="l02296"></a>02296       }
<a name="l02297"></a>02297 
<a name="l02298"></a>02298       <span class="comment">// Performance goal </span>
<a name="l02299"></a>02299       {
<a name="l02300"></a>02300          TiXmlElement* performance_goal_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"PerformanceGoal"</span>);
<a name="l02301"></a>02301 
<a name="l02302"></a>02302          <span class="keywordflow">if</span>(performance_goal_element)
<a name="l02303"></a>02303          {
<a name="l02304"></a>02304             <span class="keywordtype">double</span> new_performance_goal = atof(performance_goal_element-&gt;GetText()); 
<a name="l02305"></a>02305 
<a name="l02306"></a>02306             <span class="keywordflow">try</span>
<a name="l02307"></a>02307             {
<a name="l02308"></a>02308                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#1e5f16b50b764b468183186422703313">set_performance_goal</a>(new_performance_goal);
<a name="l02309"></a>02309             }
<a name="l02310"></a>02310             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02311"></a>02311             {
<a name="l02312"></a>02312                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02313"></a>02313             }
<a name="l02314"></a>02314          }
<a name="l02315"></a>02315       }
<a name="l02316"></a>02316 
<a name="l02317"></a>02317       <span class="comment">// Gradient norm goal </span>
<a name="l02318"></a>02318       {
<a name="l02319"></a>02319          TiXmlElement* gradient_norm_goal_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"GradientNormGoal"</span>);
<a name="l02320"></a>02320 
<a name="l02321"></a>02321          <span class="keywordflow">if</span>(gradient_norm_goal_element)
<a name="l02322"></a>02322          {
<a name="l02323"></a>02323             <span class="keywordtype">double</span> new_gradient_norm_goal = atof(gradient_norm_goal_element-&gt;GetText()); 
<a name="l02324"></a>02324 
<a name="l02325"></a>02325             <span class="keywordflow">try</span>
<a name="l02326"></a>02326             {
<a name="l02327"></a>02327                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#06331ca7f99debc62d1afadf26bff850">set_gradient_norm_goal</a>(new_gradient_norm_goal);
<a name="l02328"></a>02328             }
<a name="l02329"></a>02329             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02330"></a>02330             {
<a name="l02331"></a>02331                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02332"></a>02332             }
<a name="l02333"></a>02333          }
<a name="l02334"></a>02334       }
<a name="l02335"></a>02335 
<a name="l02336"></a>02336       <span class="comment">// Maximum generalization performance decreases</span>
<a name="l02337"></a>02337       {
<a name="l02338"></a>02338          TiXmlElement* maximum_generalization_evaluation_decreases_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"MaximumGeneralizationEvaluationDecreases"</span>);
<a name="l02339"></a>02339 
<a name="l02340"></a>02340          <span class="keywordflow">if</span>(maximum_generalization_evaluation_decreases_element)
<a name="l02341"></a>02341          {
<a name="l02342"></a>02342             <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> new_maximum_generalization_evaluation_decreases = atoi(maximum_generalization_evaluation_decreases_element-&gt;GetText()); 
<a name="l02343"></a>02343 
<a name="l02344"></a>02344             <span class="keywordflow">try</span>
<a name="l02345"></a>02345             {
<a name="l02346"></a>02346                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#ac4f86ed5087ff33770e3a92b3df4fdb">set_maximum_generalization_evaluation_decreases</a>(new_maximum_generalization_evaluation_decreases);
<a name="l02347"></a>02347             }
<a name="l02348"></a>02348             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02349"></a>02349             {
<a name="l02350"></a>02350                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02351"></a>02351             }
<a name="l02352"></a>02352          }
<a name="l02353"></a>02353       }
<a name="l02354"></a>02354 
<a name="l02355"></a>02355       <span class="comment">// Maximum epochs number </span>
<a name="l02356"></a>02356       {
<a name="l02357"></a>02357          TiXmlElement* maximum_epochs_number_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"MaximumEpochsNumber"</span>);
<a name="l02358"></a>02358 
<a name="l02359"></a>02359          <span class="keywordflow">if</span>(maximum_epochs_number_element)
<a name="l02360"></a>02360          {
<a name="l02361"></a>02361              <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> new_maximum_epochs_number = atoi(maximum_epochs_number_element-&gt;GetText()); 
<a name="l02362"></a>02362 
<a name="l02363"></a>02363             <span class="keywordflow">try</span>
<a name="l02364"></a>02364             {
<a name="l02365"></a>02365                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#aebcfb56d10a7846d587db6d66caea63">set_maximum_epochs_number</a>(new_maximum_epochs_number);
<a name="l02366"></a>02366             }
<a name="l02367"></a>02367             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02368"></a>02368             {
<a name="l02369"></a>02369                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02370"></a>02370             }
<a name="l02371"></a>02371          }
<a name="l02372"></a>02372       }
<a name="l02373"></a>02373 
<a name="l02374"></a>02374       <span class="comment">// Maximum time </span>
<a name="l02375"></a>02375       {
<a name="l02376"></a>02376          TiXmlElement* maximum_time_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"MaximumTime"</span>);
<a name="l02377"></a>02377 
<a name="l02378"></a>02378          <span class="keywordflow">if</span>(maximum_time_element)
<a name="l02379"></a>02379          {
<a name="l02380"></a>02380             <span class="keywordtype">double</span> new_maximum_time = atof(maximum_time_element-&gt;GetText()); 
<a name="l02381"></a>02381 
<a name="l02382"></a>02382             <span class="keywordflow">try</span>
<a name="l02383"></a>02383             {
<a name="l02384"></a>02384                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#7227ff99b52a9fa1a3a5905c6cc66b76">set_maximum_time</a>(new_maximum_time);
<a name="l02385"></a>02385             }
<a name="l02386"></a>02386             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02387"></a>02387             {
<a name="l02388"></a>02388                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02389"></a>02389             }
<a name="l02390"></a>02390          }
<a name="l02391"></a>02391       }
<a name="l02392"></a>02392 
<a name="l02393"></a>02393       <span class="comment">// Reserve parameters history </span>
<a name="l02394"></a>02394       {
<a name="l02395"></a>02395          TiXmlElement* reserve_parameters_history_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveParametersHistory"</span>);
<a name="l02396"></a>02396 
<a name="l02397"></a>02397          <span class="keywordflow">if</span>(reserve_parameters_history_element)
<a name="l02398"></a>02398          {
<a name="l02399"></a>02399             std::string new_reserve_parameters_history = reserve_parameters_history_element-&gt;GetText(); 
<a name="l02400"></a>02400 
<a name="l02401"></a>02401             <span class="keywordflow">try</span>
<a name="l02402"></a>02402             {
<a name="l02403"></a>02403                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#9de9b218ff51f2393ea38006019f7ffa">set_reserve_parameters_history</a>(new_reserve_parameters_history != <span class="stringliteral">"0"</span>);
<a name="l02404"></a>02404             }
<a name="l02405"></a>02405             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02406"></a>02406             {
<a name="l02407"></a>02407                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02408"></a>02408             }
<a name="l02409"></a>02409          }
<a name="l02410"></a>02410       }
<a name="l02411"></a>02411 
<a name="l02412"></a>02412       <span class="comment">// Reserve parameters norm history </span>
<a name="l02413"></a>02413       {
<a name="l02414"></a>02414          TiXmlElement* reserve_parameters_norm_history_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveParametersNormHistory"</span>);
<a name="l02415"></a>02415 
<a name="l02416"></a>02416          <span class="keywordflow">if</span>(reserve_parameters_norm_history_element)
<a name="l02417"></a>02417          {
<a name="l02418"></a>02418             std::string new_reserve_parameters_norm_history = reserve_parameters_norm_history_element-&gt;GetText(); 
<a name="l02419"></a>02419 
<a name="l02420"></a>02420             <span class="keywordflow">try</span>
<a name="l02421"></a>02421             {
<a name="l02422"></a>02422                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#274786cd036399313295dff4a097130c">set_reserve_parameters_norm_history</a>(new_reserve_parameters_norm_history != <span class="stringliteral">"0"</span>);
<a name="l02423"></a>02423             }
<a name="l02424"></a>02424             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02425"></a>02425             {
<a name="l02426"></a>02426                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02427"></a>02427             }
<a name="l02428"></a>02428          }
<a name="l02429"></a>02429       }
<a name="l02430"></a>02430 
<a name="l02431"></a>02431       <span class="comment">// Reserve evaluation history </span>
<a name="l02432"></a>02432       {
<a name="l02433"></a>02433          TiXmlElement* reserve_evaluation_history_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ReservePerformanceHistory"</span>);
<a name="l02434"></a>02434 
<a name="l02435"></a>02435          <span class="keywordflow">if</span>(reserve_evaluation_history_element)
<a name="l02436"></a>02436          {
<a name="l02437"></a>02437             std::string new_reserve_evaluation_history = reserve_evaluation_history_element-&gt;GetText(); 
<a name="l02438"></a>02438 
<a name="l02439"></a>02439             <span class="keywordflow">try</span>
<a name="l02440"></a>02440             {
<a name="l02441"></a>02441                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#eb7604ee6718eeda10dbff12695ca215">set_reserve_evaluation_history</a>(new_reserve_evaluation_history != <span class="stringliteral">"0"</span>);
<a name="l02442"></a>02442             }
<a name="l02443"></a>02443             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02444"></a>02444             {
<a name="l02445"></a>02445                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02446"></a>02446             }
<a name="l02447"></a>02447          }
<a name="l02448"></a>02448       }
<a name="l02449"></a>02449 
<a name="l02450"></a>02450       <span class="comment">// Reserve gradient history </span>
<a name="l02451"></a>02451       {
<a name="l02452"></a>02452          TiXmlElement* reserve_gradient_history_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveGradientHistory"</span>);
<a name="l02453"></a>02453 
<a name="l02454"></a>02454          <span class="keywordflow">if</span>(reserve_gradient_history_element)
<a name="l02455"></a>02455          {
<a name="l02456"></a>02456             std::string new_reserve_gradient_history = reserve_gradient_history_element-&gt;GetText(); 
<a name="l02457"></a>02457 
<a name="l02458"></a>02458             <span class="keywordflow">try</span>
<a name="l02459"></a>02459             {
<a name="l02460"></a>02460                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#faef3aac943440a248a9f2e9851eb9c0">set_reserve_gradient_history</a>(new_reserve_gradient_history != <span class="stringliteral">"0"</span>);
<a name="l02461"></a>02461             }
<a name="l02462"></a>02462             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02463"></a>02463             {
<a name="l02464"></a>02464                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02465"></a>02465             }
<a name="l02466"></a>02466          }
<a name="l02467"></a>02467 
<a name="l02468"></a>02468       <span class="comment">// Reserve gradient norm history </span>
<a name="l02469"></a>02469       {
<a name="l02470"></a>02470          TiXmlElement* reserve_gradient_norm_history_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveGradientNormHistory"</span>);
<a name="l02471"></a>02471 
<a name="l02472"></a>02472          <span class="keywordflow">if</span>(reserve_gradient_norm_history_element)
<a name="l02473"></a>02473          {
<a name="l02474"></a>02474             std::string new_reserve_gradient_norm_history = reserve_gradient_norm_history_element-&gt;GetText(); 
<a name="l02475"></a>02475 
<a name="l02476"></a>02476             <span class="keywordflow">try</span>
<a name="l02477"></a>02477             {
<a name="l02478"></a>02478                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#0c59f4b561c506844b01718945db2ea8">set_reserve_gradient_norm_history</a>(new_reserve_gradient_norm_history != <span class="stringliteral">"0"</span>);
<a name="l02479"></a>02479             }
<a name="l02480"></a>02480             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02481"></a>02481             {
<a name="l02482"></a>02482                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02483"></a>02483             }
<a name="l02484"></a>02484          }
<a name="l02485"></a>02485       }
<a name="l02486"></a>02486 
<a name="l02487"></a>02487       <span class="comment">// Reserve training direction history </span>
<a name="l02488"></a>02488       {
<a name="l02489"></a>02489          TiXmlElement* reserve_training_direction_history_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveTrainingDirectionHistory"</span>);
<a name="l02490"></a>02490 
<a name="l02491"></a>02491          <span class="keywordflow">if</span>(reserve_training_direction_history_element)
<a name="l02492"></a>02492          {
<a name="l02493"></a>02493             std::string new_reserve_training_direction_history = reserve_training_direction_history_element-&gt;GetText(); 
<a name="l02494"></a>02494 
<a name="l02495"></a>02495             <span class="keywordflow">try</span>
<a name="l02496"></a>02496             {
<a name="l02497"></a>02497                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#379aa30482f6ce0a1ad2071a4e6f6b61">set_reserve_training_direction_history</a>(new_reserve_training_direction_history != <span class="stringliteral">"0"</span>);
<a name="l02498"></a>02498             }
<a name="l02499"></a>02499             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02500"></a>02500             {
<a name="l02501"></a>02501                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02502"></a>02502             }
<a name="l02503"></a>02503          }
<a name="l02504"></a>02504       }
<a name="l02505"></a>02505 
<a name="l02506"></a>02506       <span class="comment">// Reserve training rate history </span>
<a name="l02507"></a>02507       {
<a name="l02508"></a>02508          TiXmlElement* reserve_training_rate_history_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveTrainingRateHistory"</span>);
<a name="l02509"></a>02509 
<a name="l02510"></a>02510          <span class="keywordflow">if</span>(reserve_training_rate_history_element)
<a name="l02511"></a>02511          {
<a name="l02512"></a>02512             std::string new_reserve_training_rate_history = reserve_training_rate_history_element-&gt;GetText(); 
<a name="l02513"></a>02513 
<a name="l02514"></a>02514             <span class="keywordflow">try</span>
<a name="l02515"></a>02515             {
<a name="l02516"></a>02516                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#e46832a168326e64d3e19ef769335771">set_reserve_training_rate_history</a>(new_reserve_training_rate_history != <span class="stringliteral">"0"</span>);
<a name="l02517"></a>02517             }
<a name="l02518"></a>02518             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02519"></a>02519             {
<a name="l02520"></a>02520                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02521"></a>02521             }
<a name="l02522"></a>02522          }
<a name="l02523"></a>02523       }
<a name="l02524"></a>02524 
<a name="l02525"></a>02525       <span class="comment">// Reserve elapsed time history </span>
<a name="l02526"></a>02526       {
<a name="l02527"></a>02527          TiXmlElement* reserve_elapsed_time_history_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveElapsedTimeHistory"</span>);
<a name="l02528"></a>02528 
<a name="l02529"></a>02529          <span class="keywordflow">if</span>(reserve_elapsed_time_history_element)
<a name="l02530"></a>02530          {
<a name="l02531"></a>02531             std::string new_reserve_elapsed_time_history = reserve_elapsed_time_history_element-&gt;GetText(); 
<a name="l02532"></a>02532 
<a name="l02533"></a>02533             <span class="keywordflow">try</span>
<a name="l02534"></a>02534             {
<a name="l02535"></a>02535                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#98b68a0401e023dc61692d4b7377a4ac">set_reserve_elapsed_time_history</a>(new_reserve_elapsed_time_history != <span class="stringliteral">"0"</span>);
<a name="l02536"></a>02536             }
<a name="l02537"></a>02537             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02538"></a>02538             {
<a name="l02539"></a>02539                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02540"></a>02540             }
<a name="l02541"></a>02541          }
<a name="l02542"></a>02542       }
<a name="l02543"></a>02543 
<a name="l02544"></a>02544       <span class="comment">// Reserve generalization evaluation history </span>
<a name="l02545"></a>02545       {
<a name="l02546"></a>02546          TiXmlElement* reserve_generalization_evaluation_history_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"ReserveGeneralizationPerformanceHistory"</span>);
<a name="l02547"></a>02547 
<a name="l02548"></a>02548          <span class="keywordflow">if</span>(reserve_generalization_evaluation_history_element)
<a name="l02549"></a>02549          {
<a name="l02550"></a>02550             std::string new_reserve_generalization_evaluation_history = reserve_generalization_evaluation_history_element-&gt;GetText(); 
<a name="l02551"></a>02551 
<a name="l02552"></a>02552             <span class="keywordflow">try</span>
<a name="l02553"></a>02553             {
<a name="l02554"></a>02554                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#97eaf29840f91536f1a55878bc077882">set_reserve_generalization_evaluation_history</a>(new_reserve_generalization_evaluation_history != <span class="stringliteral">"0"</span>);
<a name="l02555"></a>02555             }
<a name="l02556"></a>02556             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02557"></a>02557             {
<a name="l02558"></a>02558                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02559"></a>02559             }
<a name="l02560"></a>02560          }
<a name="l02561"></a>02561       }
<a name="l02562"></a>02562 
<a name="l02563"></a>02563       <span class="comment">// Display period</span>
<a name="l02564"></a>02564       {
<a name="l02565"></a>02565          TiXmlElement* display_period_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"DisplayPeriod"</span>);
<a name="l02566"></a>02566 
<a name="l02567"></a>02567          <span class="keywordflow">if</span>(display_period_element)
<a name="l02568"></a>02568          {
<a name="l02569"></a>02569             <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> new_display_period = atoi(display_period_element-&gt;GetText()); 
<a name="l02570"></a>02570 
<a name="l02571"></a>02571             <span class="keywordflow">try</span>
<a name="l02572"></a>02572             {
<a name="l02573"></a>02573                <a class="code" href="class_open_n_n_1_1_conjugate_gradient.html#a38aa187051cc3605a507a35d36b63be">set_display_period</a>(new_display_period);
<a name="l02574"></a>02574             }
<a name="l02575"></a>02575             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02576"></a>02576             {
<a name="l02577"></a>02577                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02578"></a>02578             }
<a name="l02579"></a>02579          }
<a name="l02580"></a>02580       }
<a name="l02581"></a>02581 
<a name="l02582"></a>02582       <span class="comment">// Display</span>
<a name="l02583"></a>02583       {
<a name="l02584"></a>02584          TiXmlElement* display_element = conjugate_gradient_element-&gt;FirstChildElement(<span class="stringliteral">"Display"</span>);
<a name="l02585"></a>02585 
<a name="l02586"></a>02586          <span class="keywordflow">if</span>(display_element)
<a name="l02587"></a>02587          {
<a name="l02588"></a>02588             std::string new_display = display_element-&gt;GetText(); 
<a name="l02589"></a>02589 
<a name="l02590"></a>02590             <span class="keywordflow">try</span>
<a name="l02591"></a>02591             {
<a name="l02592"></a>02592                <a class="code" href="class_open_n_n_1_1_training_algorithm.html#8c9546bea276fe018a2e4269a28d0665">set_display</a>(new_display != <span class="stringliteral">"0"</span>);
<a name="l02593"></a>02593             }
<a name="l02594"></a>02594             <span class="keywordflow">catch</span>(std::exception&amp; e)
<a name="l02595"></a>02595             {
<a name="l02596"></a>02596                std::cout &lt;&lt; e.what() &lt;&lt; std::endl;               
<a name="l02597"></a>02597             }
<a name="l02598"></a>02598          }
<a name="l02599"></a>02599       }
<a name="l02600"></a>02600    }
<a name="l02601"></a>02601 }
<a name="l02602"></a>02602 }
<a name="l02603"></a>02603 }
<a name="l02604"></a>02604 
<a name="l02605"></a>02605 <span class="comment">// OpenNN: Open Neural Networks Library.</span>
<a name="l02606"></a>02606 <span class="comment">// Copyright (C) 2005-2012 Roberto Lopez </span>
<a name="l02607"></a>02607 <span class="comment">//</span>
<a name="l02608"></a>02608 <span class="comment">// This library is free software; you can redistribute it and/or</span>
<a name="l02609"></a>02609 <span class="comment">// modify it under the terms of the GNU Lesser General Public</span>
<a name="l02610"></a>02610 <span class="comment">// License as published by the Free Software Foundation; either</span>
<a name="l02611"></a>02611 <span class="comment">// version 2.1 of the License, or any later version.</span>
<a name="l02612"></a>02612 <span class="comment">//</span>
<a name="l02613"></a>02613 <span class="comment">// This library is distributed in the hope that it will be useful,</span>
<a name="l02614"></a>02614 <span class="comment">// but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<a name="l02615"></a>02615 <span class="comment">// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU</span>
<a name="l02616"></a>02616 <span class="comment">// Lesser General Public License for more details.</span>
<a name="l02617"></a>02617 
<a name="l02618"></a>02618 <span class="comment">// You should have received a copy of the GNU Lesser General Public</span>
<a name="l02619"></a>02619 <span class="comment">// License along with this library; if not, write to the Free Software</span>
<a name="l02620"></a>02620 <span class="comment">// Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA</span>
</pre></div></div>
<hr size="1"><address style="text-align: right;"><small>Generated on Sun Aug 26 11:58:09 2012 for OpenNN by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.5.9 </small></address>
</body>
</html>
